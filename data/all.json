[
  {
    "model_name": "GPT-4",
    "fields": {
      "isImportant": {
        "value": "TRUE",
        "source": "custom",
        "citation": ""
      },
      "flops": {
        "value": "2.10E+25",
        "source": "epoch",
        "citation": "90% CI: 8.2E+24 to 4.4E+25NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing"
      },
      "numTokens": {
        "value": "4900000000000",
        "source": "epoch",
        "citation": "Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget."
      },
      "trainingTimeDays": {
        "value": "2280",
        "source": "epoch",
        "citation": "(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%."
      },
      "gpuCount": {
        "value": "25000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.34",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 3",
    "fields": {
      "isImportant": {
        "value": "TRUE",
        "source": "custom",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sparse digit recognition SVM",
    "fields": {
      "releaseDate": {
        "value": "2008-11-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Karakuri LM",
    "fields": {
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DOT(S)-RNN",
    "fields": {
      "numParams": {
        "value": "6160000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KEPLER",
    "fields": {
      "flops": {
        "value": "1.24E+20",
        "source": "epoch",
        "citation": "From author communication\"About 128 GPU-days using Nvidia V100 (16GB). \"precision: float16V100 GPU for float16: 28000000000000 (2.8E+13)0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h= 1.24E+20"
      },
      "numParams": {
        "value": "110000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3300000000",
        "source": "epoch",
        "citation": "For BookCorpus + English Wikipedia: 800M + 2500MFor Wikidata5M: 20614279See table 1. Contains \"entities\", \"relations\", and \"triplets\""
      },
      "costDollars": {
        "value": "437.9671447",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-11-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DOC + Finetune\u2217 + Partial Shuffle (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2019-03-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MV-RNN",
    "fields": {
      "numParams": {
        "value": "3510255",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-07-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "n-gram LM",
    "fields": {
      "releaseDate": {
        "value": "1997-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao - Wen Yuan",
    "fields": {
      "flops": {
        "value": "6.50281E+20",
        "source": "epoch",
        "citation": "64 Nvidia V100 GPUs for two weeks64 GPUs * 2.8e13 FLOP/s /GPU * 14*24*60*60s * 0.3 [utilization rate]"
      },
      "numParams": {
        "value": "2600000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "iGPT-L",
    "fields": {
      "flops": {
        "value": "8.91E+21",
        "source": "epoch",
        "citation": "We have that \"iGPT-L was trained for roughly 2500 V100-days\" [1]I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.[1] https://openai.com/blog/image-gpt/[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf[3] https://openai.com/blog/ai-and-compute/"
      },
      "numParams": {
        "value": "1362000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9600000",
        "source": "epoch",
        "citation": "\"We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.\"https://image-net.org/challenges/LSVRC/2012/\"The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.\""
      },
      "costDollars": {
        "value": "32482.56323",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VD-LSTM+REAL Large",
    "fields": {
      "flops": {
        "value": "2.13E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "51000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-11-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepLab (2017)",
    "fields": {
      "releaseDate": {
        "value": "2017-04-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NEC LLM 13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fuyu-Heavy",
    "fields": {
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deeply-recursive ConvNet",
    "fields": {
      "releaseDate": {
        "value": "2016-11-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GCNN-14",
    "fields": {
      "releaseDate": {
        "value": "2016-12-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ContextNet",
    "fields": {
      "numParams": {
        "value": "112700000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude",
    "fields": {
      "releaseDate": {
        "value": "2023-03-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Kohonen network",
    "fields": {
      "numParams": {
        "value": "4096",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1981-07-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNet-110 (CIFAR-10)",
    "fields": {
      "numParams": {
        "value": "1700000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-12-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT",
    "fields": {
      "flops": {
        "value": "1.75781E+19",
        "source": "epoch",
        "citation": "COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE\"We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\""
      },
      "numParams": {
        "value": "117000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000000",
        "source": "epoch",
        "citation": "\"BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).\"https://paperswithcode.com/dataset/bookcorpusBookCorpus seems to have about 5000MB of contentsource: https://huggingface.co/datasets/bookcorpusopenAssuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens"
      },
      "costDollars": {
        "value": "68.71974737",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": "\"1 month on 8 GPUs.\" from the reference link"
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Quadro P600",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlexNet + coordinating filters",
    "fields": {
      "flops": {
        "value": "1.55714E+18",
        "source": "epoch",
        "citation": "The paper reimplements ResNet-20, AlexNet and GoogLeNet. The largest from these models is AlexNet. (data for these models taken form this db)training of AlexNet taken 470000000000000000.00 FLOPstraining of GoogLeNet taken 1557140125176000000.00 FLOPsmax(1557140125176000000, 470000000000000000) = 1557140125176000000source \"The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. \""
      },
      "numParams": {
        "value": "60000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000",
        "source": "epoch",
        "citation": "size of ImageNet"
      },
      "releaseDate": {
        "value": "2017-03-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepSpeech2 (English)",
    "fields": {
      "flops": {
        "value": "2.6E+19",
        "source": "epoch",
        "citation": "1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop = 26,000 PF = 0.30 pfs-daysSee also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute"
      },
      "numParams": {
        "value": "38000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "163339200",
        "source": "epoch",
        "citation": "\"Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.\"11,940 * 13,680 = 163339200"
      },
      "costDollars": {
        "value": "150.7784796",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GTX Titan X",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.45",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OneLLM",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM(large)+Sememe+cell",
    "fields": {
      "flops": {
        "value": "2.4E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "48000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adaptive Inputs + LayerDrop",
    "fields": {
      "numParams": {
        "value": "423000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Japanese-LM-3.6B",
    "fields": {
      "numParams": {
        "value": "3600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "72000000000",
        "source": "epoch",
        "citation": "650GB per huggingfaceour guide says 111M Japanese words per GB, which would be ~72B wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Neural cache model (size=2000) (300M)",
    "fields": {
      "numParams": {
        "value": "300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-SW3",
    "fields": {
      "flops": {
        "value": "1.30E+21",
        "source": "epoch",
        "citation": "From section 4: \"Training was performed on GPU resources from the Berzelius Superpod, which is currently the fastest supercomputer in Sweden, equipped with 60 Nvidia DGXA100 servers, each of which consists of 8 Nvidia A100GPUs with 320 GB Total GPU memory. Our trainingprocess took 2.5 days utilizing 16 of the DGX A100servers (in total 128 GPUs).\"2.5*24*60**2 * 128 * 1.56E+14 * 0.3 = 1.3e21"
      },
      "numParams": {
        "value": "3500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "16700000000",
        "source": "epoch",
        "citation": "100GB Swedish corpus, assume Swedish has similar 167M words per GB as German.100*167e6 = 1.67e10"
      },
      "trainingTimeDays": {
        "value": "60",
        "source": "epoch",
        "citation": "\"Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs).\""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Make-A-Video",
    "fields": {
      "releaseDate": {
        "value": "2022-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "aLSTM(depth-2)+RecurrentPolicy (WT2)",
    "fields": {
      "flops": {
        "value": "7.59E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "32000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MENACE",
    "fields": {
      "releaseDate": {
        "value": "1963-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GShard (600B)",
    "fields": {
      "flops": {
        "value": "1.33E+22",
        "source": "epoch",
        "citation": "https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdfTable 4"
      },
      "numParams": {
        "value": "600000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "260000000000",
        "source": "epoch",
        "citation": "\"We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training\"Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words."
      },
      "costDollars": {
        "value": "27609.80743",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Bayesian Starcraft",
    "fields": {
      "numParams": {
        "value": "13125",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2011-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Conformer",
    "fields": {
      "numParams": {
        "value": "118800000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Phi-2",
    "fields": {
      "flops": {
        "value": "2.27E+22",
        "source": "epoch",
        "citation": "2.7B params, trained on 1.4T tokens2.7 billion * 1.4 trillion * 6 = 2.27e2296*14 A100-days14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22"
      },
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "336",
        "source": "epoch",
        "citation": "14 days"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stable Beluga 2",
    "fields": {
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-33B",
    "fields": {
      "flops": {
        "value": "2.73E+23",
        "source": "epoch",
        "citation": "1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP"
      },
      "numParams": {
        "value": "32500000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Megatron-LM (355M)",
    "fields": {
      "flops": {
        "value": "3.35E+20",
        "source": "epoch",
        "citation": "355M is a GPT-2-based model (Table 2).\"For GPT-2 models, all training is performed with sequencesof 1024 subword units at a batch size of 512 for 300k iterations\" I interpret the above as 1024*512*300k = 157B training tokens (or things that require a forward+backward pass). 6 * 1024*512*300,000 * 355,000,000  = 3.35e20"
      },
      "numParams": {
        "value": "355000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "34800000000",
        "source": "epoch",
        "citation": "\"The resulting aggregatecorpus contains 174 GB of deduplicated text.\"assuming 200M tokens per GB"
      },
      "releaseDate": {
        "value": "2019-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SEER",
    "fields": {
      "flops": {
        "value": "4.42E+21",
        "source": "epoch",
        "citation": "Numbers from section 3.2512 GPUs * 0.1 * 8days * 24h/day * 3600s/h * 125 TFLOP/s"
      },
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000000",
        "source": "epoch",
        "citation": "\"Overall, we trainon 1B images for a total of 122K iterations.\""
      },
      "costDollars": {
        "value": "16058.79531",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "192",
        "source": "epoch",
        "citation": "8 days"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-07-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GGNN",
    "fields": {
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CTC-Trained LSTM",
    "fields": {
      "numParams": {
        "value": "114662",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "41620",
        "source": "epoch",
        "citation": "4162 utterances, guesstimated avg 10 words per utterance"
      },
      "releaseDate": {
        "value": "2006-06-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLM-10B-unidirectional",
    "fields": {
      "releaseDate": {
        "value": "2021-03-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ABAB",
    "fields": {
      "releaseDate": {
        "value": "2023-08-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Megatron-LM (8.3B)",
    "fields": {
      "flops": {
        "value": "9.10E+21",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodbother estimates:8.3B is a GPT-2-based model (Table 2). \"For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations\" I interpret the above as 1024*512*300k = 157B training tokens 6 * 157 billion * 8.3 billion  = 7.8e21Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)2.1 days per epoch, ~4.4 epochs2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22These are both close to the akronomicon estimate"
      },
      "numParams": {
        "value": "8300000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "34800000000",
        "source": "epoch",
        "citation": "\"The resulting aggregatecorpus contains 174 GB of deduplicated text.\""
      },
      "costDollars": {
        "value": "33212.50847",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "327",
        "source": "epoch",
        "citation": "Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUsAssume total compute is 9.1e21 FLOP.Then training time is 327 hours.https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.1162",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pattern recognition and reading by machine",
    "fields": {
      "numParams": {
        "value": "2625",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1959-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProGen",
    "fields": {
      "flops": {
        "value": "3.7E+20",
        "source": "epoch",
        "citation": "Our model was implemented in TensorFlow (Abadi et al.,2016) and trained with a global batch size of 64 distributedacross 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)4.00E+12*256*60**2*24*14*0.3 = 3.7e20"
      },
      "numParams": {
        "value": "1200000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "623.7514344",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-03-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adaptive Input Transformer + RD",
    "fields": {
      "flops": {
        "value": "8.2E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Refact-1.6B",
    "fields": {
      "flops": {
        "value": "1.15E+22",
        "source": "epoch",
        "citation": "6ND = 6 * 1.6B * 1.2T = 11520000000000000000000 = 1.152e22citation \"model trained for 1.2T tokens. \"alternativeflops = (64) * (27770 * 10**9) * (28 * 24 * 3600) * (0.3) = 1.2898787328e+21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)Precision: bfloat16GPUs 64 NVidia A5000Training time 28 days27.77 TFLOPS - peak flop from https://www.techpowerup.com/gpu-specs/rtx-a5000.c3748"
      },
      "numParams": {
        "value": "1600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000000000",
        "source": "epoch",
        "citation": "1.2T from \"The text to code proportion was 50:50, model trained for 1.2T tokens. \""
      },
      "trainingTimeDays": {
        "value": "672",
        "source": "epoch",
        "citation": "from \"Model Stats\""
      },
      "gpuCount": {
        "value": "6",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA RTX A5000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "aLSTM(depth-2)+RecurrentPolicy (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Selfish-RNN (SNT-ASGD) Stacked LSTMs",
    "fields": {
      "flops": {
        "value": "1.4E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "25200000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ADP-FAIRSEQ+NGRAMRES",
    "fields": {
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Punish/Reward",
    "fields": {
      "numParams": {
        "value": "21",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1973-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RFA-GATE-Gaussian-Stateful Big",
    "fields": {
      "flops": {
        "value": "7.14E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "242000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DiffStk-MRNN",
    "fields": {
      "flops": {
        "value": "282000000000000",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "1010000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-04-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DNCON2",
    "fields": {
      "flops": {
        "value": "9.5E+16",
        "source": "epoch",
        "citation": "\"Our training was conducted on Tesla K20 Nvidia GPUs each having 5 GB of GPU memory, on which, training one model took around 12\u2009h.\"\"We train each CNN for a total of 1600 epochs with each epoch of training taking around 2\u2009min.\"Assumptions:peakFLOP rate 3.52e12FLOP/s (from: https://www.techpowerup.com/gpu-specs/tesla-k20c.c564)30% utilization rate1 GPUEstimate 1: \"training one model took around 12h\" => unclear how many GPUs(12 *3600) s * 3.52e12 FLOP/s * 0.3 = 4.5e16 FLOPEstimate 2: \"We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.\"(1600 epochs * 2 min/epoch * 60 s/min) * 3.52e12 FLOP/s * 0.3 =  2e17 FLOPGeometric mean: 9.5e16"
      },
      "numTokens": {
        "value": "1426",
        "source": "epoch",
        "citation": "\"Our raw feature files for all 1426 training proteins\""
      },
      "trainingTimeDays": {
        "value": "12",
        "source": "epoch",
        "citation": "\"training one model took around 12\u2009h\""
      },
      "releaseDate": {
        "value": "2018-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN+LDA+KN5+cache",
    "fields": {
      "numParams": {
        "value": "9000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Big-Little Net",
    "fields": {
      "numParams": {
        "value": "77360000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-07-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hyena-3-slim",
    "fields": {
      "numParams": {
        "value": "125000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE-ViLG",
    "fields": {
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "145000000",
        "source": "epoch",
        "citation": "To explore the landscape of large-scale pre-training for bidirectional text-image generation,we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs."
      },
      "releaseDate": {
        "value": "2021-12-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Meta Pseudo Labels",
    "fields": {
      "flops": {
        "value": "4.79E+22",
        "source": "epoch",
        "citation": "From communication with author:22671 TPU days on specific hardware.Which hardware did you use and in which configuration?2048 cores of TPU v3.Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)So the compute estimate is:1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP"
      },
      "numParams": {
        "value": "480000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "130000000",
        "source": "epoch",
        "citation": "Section 4Datasets. For this experiment, we use the entire ImageNettraining set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, andthen is filtered down to 130 million images by Noisy Studentusing confidence thresholds and up-sampling [77]. We usethe same 130 million images as Noisy Student"
      },
      "costDollars": {
        "value": "369462.8182",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "264",
        "source": "epoch",
        "citation": "11 days from section 4:\"We train the model for 1 million steps in total,which takes about 11 days for EfficientNet-L2 and 10 daysfor EfficientNet-B6-Wide. \"\"Specifically, our training process runs on a cluster of 2,048TPUv3 cores. \""
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RT-1",
    "fields": {
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "True-Regularization+Finetune+Dynamic-Eval",
    "fields": {
      "numParams": {
        "value": "7000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "M6-10B",
    "fields": {
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1900000000000",
        "source": "epoch",
        "citation": "\"1.9TB images and 292GB texts\"TODO: figure out what to do for multimodal pretraining datasets"
      },
      "releaseDate": {
        "value": "2021-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "4 layer QRNN + dynamic evaluation",
    "fields": {
      "releaseDate": {
        "value": "2019-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WizardLM 70B",
    "fields": {
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaFold-Multimer",
    "fields": {
      "flops": {
        "value": "4.35E+21",
        "source": "epoch",
        "citation": "Section: 2.5. Training Regimen\"We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)\"Assuming: FP16 and utilization 0.4Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs"
      },
      "numTokens": {
        "value": "147328",
        "source": "epoch",
        "citation": "See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018"
      },
      "trainingTimeDays": {
        "value": "384",
        "source": "epoch",
        "citation": "Section: 2.5. Training Regimen\"We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PolySphere-1",
    "fields": {
      "numParams": {
        "value": "14000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FMMformer (2-kernel fast weight + Band20)",
    "fields": {
      "flops": {
        "value": "4.3E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "40000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sandstorm (DARPA Grand Challenge I)",
    "fields": {
      "releaseDate": {
        "value": "2004-06-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Lyra-Fr 10B",
    "fields": {
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EDSR",
    "fields": {
      "releaseDate": {
        "value": "2017-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-65B",
    "fields": {
      "flops": {
        "value": "5.50E+23",
        "source": "epoch",
        "citation": "1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOPCompared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29"
      },
      "numParams": {
        "value": "65200000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1050000000000",
        "source": "epoch",
        "citation": "1.4 trillion tokens * 0.75 words/token = 1.05 trillion words"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "1179384.75",
        "source": "epoch",
        "citation": "1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.$1391674 / 1.18 = $1179385 in 2020 USD."
      },
      "trainingTimeDays": {
        "value": "500",
        "source": "epoch",
        "citation": "\"When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.\""
      },
      "gpuCount": {
        "value": "2048",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.4746",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenAI Five Rerun",
    "fields": {
      "flops": {
        "value": "1.30E+22",
        "source": "epoch",
        "citation": "THIS CALCULATION IS FOR RERUNsource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389"
      },
      "numParams": {
        "value": "159000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "53084160000",
        "source": "epoch",
        "citation": "54k iterations (Fig 7)with a batch size of 983040 (Table 2)"
      },
      "costDollars": {
        "value": "32217.12669",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NEAT in neuroevolution",
    "fields": {
      "releaseDate": {
        "value": "2002-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM (WT103)",
    "fields": {
      "releaseDate": {
        "value": "2016-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Falcon 180B",
    "fields": {
      "flops": {
        "value": "3.76E+24",
        "source": "epoch",
        "citation": "43,500 petaflop-days per Table 1 of the paper43500 * 1e15 * 24 * 3600 = 3.76e24C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP"
      },
      "numParams": {
        "value": "180000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2625000000000",
        "source": "epoch",
        "citation": "3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words"
      },
      "batchSize": {
        "value": "4194304",
        "source": "epoch",
        "citation": "from paper (https://arxiv.org/pdf/2311.16867.pdf):Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.\"All Falcon models are pretrained with a 2,048 sequence length\"2048*2048 = 4194304"
      },
      "trainingTimeDays": {
        "value": "4320",
        "source": "epoch",
        "citation": "Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that."
      },
      "gpuCount": {
        "value": "4096",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.1876",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CALM",
    "fields": {
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mistral Medium",
    "fields": {
      "releaseDate": {
        "value": "2023-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Japanese StableLM Base Alpha 7B",
    "fields": {
      "flops": {
        "value": "3.15E+22",
        "source": "epoch",
        "citation": "7b params, 750b tokens7b * 750b * 6 = 3.15e22"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TrellisNet-MoS (1.4x larger)",
    "fields": {
      "releaseDate": {
        "value": "2018-10-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BloombergGPT",
    "fields": {
      "flops": {
        "value": "2.36E+23",
        "source": "epoch",
        "citation": "2.36e23 per Table 4(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)"
      },
      "numParams": {
        "value": "50558868480",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "532000000000",
        "source": "epoch",
        "citation": "708.9 billion tokens. At 0.75 English words per token, that's 532B words"
      },
      "batchSize": {
        "value": "4200000",
        "source": "epoch",
        "citation": "\"in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.\""
      },
      "trainingTimeDays": {
        "value": "1270",
        "source": "epoch",
        "citation": "\"~53 days\""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.32",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE-GEN (large)",
    "fields": {
      "numParams": {
        "value": "340000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransfoRNN(d=1024)(2-layer) (WT2)",
    "fields": {
      "numParams": {
        "value": "97600000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SenseTime SenseNova",
    "fields": {
      "releaseDate": {
        "value": "2023-04-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CD-GraB (WT103)",
    "fields": {
      "releaseDate": {
        "value": "2023-02-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "base LM+GNN+kNN",
    "fields": {
      "flops": {
        "value": "7.3E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "274000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TranceptEve",
    "fields": {
      "releaseDate": {
        "value": "2022-12-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mesh-TensorFlow Transformer 4.9B (language modelling)",
    "fields": {
      "flops": {
        "value": "1.61741E+20",
        "source": "epoch",
        "citation": "flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 45TFLOPs per chips"
      },
      "numParams": {
        "value": "4900000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4750000000",
        "source": "epoch",
        "citation": "from section 9.1 Wikipedia and one-billion-word language modeling benchmark.there were around 5B tokens from wikipedia - assuming 0,75 words per token we have 3750000000.0 words from wikipedia and 1B words from the benchmarkand citation from section 9.1: 'We have included random samples from these models in Appendix C. On the languagemodel_wiki_noref_v128k_l1k dataset from the Tensor2Tensor library1, consisting of over 5 billion tokens of text from Wikipedia, perplexity continued to improve significantly with a model size of 5 billion parameters.'"
      },
      "trainingTimeDays": {
        "value": "13",
        "source": "epoch",
        "citation": "from section 9.1 \"For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FAST",
    "fields": {
      "releaseDate": {
        "value": "2006-05-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepSeek Coder 33B",
    "fields": {
      "flops": {
        "value": "3.96E+23",
        "source": "epoch",
        "citation": "\"Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).\"This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23"
      },
      "numParams": {
        "value": "33000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer + Average Attention Network",
    "fields": {
      "releaseDate": {
        "value": "2019-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MegaSyn",
    "fields": {
      "releaseDate": {
        "value": "2022-03-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "M6-100B",
    "fields": {
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1900000000000",
        "source": "epoch",
        "citation": "\"1.9TB images and 292GB texts\"TODO: figure out what to do for multimodal pretraining datasets"
      },
      "releaseDate": {
        "value": "2021-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenChat 3.5-7B",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "UniRep",
    "fields": {
      "flops": {
        "value": "2.2E+19",
        "source": "epoch",
        "citation": "\"Training was performed using data parallelism on four Nvidia K80 GPUs (mLSTM-1,900) or two Nvidia K-40s (4\u00d7 mLSTM-256, 4\u00d7 mLSTM-64). The mLSTM-1,900 model was trained for ~770,000 weight updates, or ~3.5\u2009weeks wall clock time, corresponding to ~1 epoch.\" [Methods - Unsupervised training dataset]Assuming 30% utilization rate and single-precision performanceEstimate: 3.5 weeks * 7 days/week * 24 hours/day * 60 min/hour * 60 sec/min * 4 GPUs *8.73e12 FLOP/sec * 0.3"
      },
      "numParams": {
        "value": "18200000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "588",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-03-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SemVec",
    "fields": {
      "releaseDate": {
        "value": "2013-06-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Semantic Hashing",
    "fields": {
      "numParams": {
        "value": "2600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "310521",
        "source": "epoch",
        "citation": "Section 4.1"
      },
      "releaseDate": {
        "value": "2008-12-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LongT5",
    "fields": {
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "562500000000",
        "source": "epoch",
        "citation": "size of C4, from https://huggingface.co/datasets/c4 , C4 dataset is a collection of about 750GB of English-language text, so around 0.75 * 750e9 = 562500000000 words"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BigBiGAN",
    "fields": {
      "numParams": {
        "value": "86000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Lyria",
    "fields": {
      "releaseDate": {
        "value": "2023-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransformerXL-LayerFusion-CA",
    "fields": {
      "releaseDate": {
        "value": "2020-07-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WaveNet",
    "fields": {
      "releaseDate": {
        "value": "2016-09-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CT-MoS + DynamicEval (WT2)",
    "fields": {
      "flops": {
        "value": "5.62E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "45000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer LM + MinSen",
    "fields": {
      "releaseDate": {
        "value": "2021-11-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Player of Games",
    "fields": {
      "releaseDate": {
        "value": "2021-12-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-125M(finetuned)",
    "fields": {
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RQ-Transformer (LSUN-cat dataset)",
    "fields": {
      "flops": {
        "value": "2.91133E+18",
        "source": "epoch",
        "citation": "flops = (4) * (3120 * 10**9) * (9*24 * 3600) * (0.3) = 2911334400000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\"We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. \"we provide details for LSUN-cat with largest compute"
      },
      "numParams": {
        "value": "612000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1657266",
        "source": "epoch",
        "citation": "size of LSUN-cat"
      },
      "trainingTimeDays": {
        "value": "216",
        "source": "epoch",
        "citation": "9 days for LSUN-cat\"We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. \""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProteinDT",
    "fields": {
      "releaseDate": {
        "value": "2023-02-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MADALINE II",
    "fields": {
      "releaseDate": {
        "value": "1988-07-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Otter",
    "fields": {
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce RTX 3090",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deeply-supervised nets",
    "fields": {
      "numTokens": {
        "value": "870000",
        "source": "epoch",
        "citation": "60000+50000+60000+600000"
      },
      "releaseDate": {
        "value": "2014-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT2-LayerFusion-WS",
    "fields": {
      "releaseDate": {
        "value": "2020-07-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "data2vec (vision)",
    "fields": {
      "numParams": {
        "value": "705134592",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1281167",
        "source": "epoch",
        "citation": "Section 5.1: \"we pretrain data2vec on the images of the ImageNet-1K trainingset\""
      },
      "releaseDate": {
        "value": "2022-01-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BlenderBot 3",
    "fields": {
      "flops": {
        "value": "4.30E+23",
        "source": "epoch",
        "citation": "(taken from OPT-175 base)"
      },
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "262144",
        "source": "epoch",
        "citation": "Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.\"The 175B model was trained with a batch size of 2^18\"2^18 = 262144"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-08-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "4 layer Densely Connected LSTM",
    "fields": {
      "releaseDate": {
        "value": "2017-07-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PLaMo-13B",
    "fields": {
      "flops": {
        "value": "1.17E+23",
        "source": "epoch",
        "citation": "6ND = 6*13e9*1.5e12=1.17e+23from https://huggingface.co/pfnet/plamo-13b#model-details"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1170000000000",
        "source": "epoch",
        "citation": "0.75*1.32T + 0.18T = 11700000000000.75 words per token for English1 for Japanese"
      },
      "releaseDate": {
        "value": "2023-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StableLM-Base-Alpha-7B",
    "fields": {
      "flops": {
        "value": "4.50E+22",
        "source": "epoch",
        "citation": "\"StableLM-Base-Alpha-7B-v2 is pre-trained using a multi-stage context length extension schedule following similar work (Nijkamp et al. 2023); first pre-training at a context length of 2048 for 1 trillion tokens, then fine-tuning at a context length of 4096 for another 100B tokens\"6890209280 params * 1.1 trillion tokens * 6 = 4.5e22alternatively: \"StableLM-Base-Alpha-7B-v2 was trained on the Stability AI cluster - occupying 384 NVIDIA A100 40GB GPUs across AWS P4d instances. Training took approximately 16.33 days to complete across both stages.\"312 teraflops * 384 * 16.33 * 24 * 3600 * 0.3 = 5.07e22"
      },
      "numParams": {
        "value": "6890209280",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1 trillion tokens"
      },
      "trainingTimeDays": {
        "value": "392",
        "source": "epoch",
        "citation": "16.33 days"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "R-CNN (T-net)",
    "fields": {
      "numParams": {
        "value": "69003872",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-11-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
    "fields": {
      "flops": {
        "value": "9.85E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "34000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KoGPT",
    "fields": {
      "numParams": {
        "value": "6166502400",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DARTS (second order)",
    "fields": {
      "releaseDate": {
        "value": "2018-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deep rectifier networks",
    "fields": {
      "releaseDate": {
        "value": "2011-04-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HyperCLOVA",
    "fields": {
      "flops": {
        "value": "1.48E+23",
        "source": "epoch",
        "citation": "\"For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.\"82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOPCalculation using GPU time corroborates this:- \"Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.\"- \"It takes 13.4 days to train a model with 82B parameters with 150B tokens.\" Assume 300B tokens takes twice as long, 26.8 days.- Assume the default of 30% utilization rate for large language models.1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP"
      },
      "numParams": {
        "value": "82000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "190000000000",
        "source": "epoch",
        "citation": "\"We introduce HyperCLOVA, a large-scaleKorean in-context learning-based LM withnearly 100B parameters, by constructing alarge Korean-centric corpus of 560B tokens.\"Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.Based on that, 5.6e11 Korean tokens ~= 1.9e11 words"
      },
      "costDollars": {
        "value": "103802.3144",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "643.2",
        "source": "epoch",
        "citation": "see compute notes"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Base LM + kNN LM + Continuous Cache",
    "fields": {
      "flops": {
        "value": "7.3E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LongNet",
    "fields": {
      "flops": {
        "value": "4.86E+21",
        "source": "epoch",
        "citation": "2.7B params * 300B tokens * 6 = 4.86e21Note: not sure if there are very long sequences in the training data that would affect this calculation. Per paper, complexity of their attention mechanism scales linearly with sequence length."
      },
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cross-Lingual POS Tagger",
    "fields": {
      "releaseDate": {
        "value": "2011-06-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Local Binary Patterns for facial recognition",
    "fields": {
      "releaseDate": {
        "value": "2006-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Constituency-Tree LSTM",
    "fields": {
      "numParams": {
        "value": "205190",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-02-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ALVINN",
    "fields": {
      "flops": {
        "value": "81187041441",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numParams": {
        "value": "3994",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200",
        "source": "epoch",
        "citation": "\"Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels\""
      },
      "releaseDate": {
        "value": "1989-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Subformer (96M)",
    "fields": {
      "releaseDate": {
        "value": "2021-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SparseOPT-30B",
    "fields": {
      "numParams": {
        "value": "30000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL + AutoDropout (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2021-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Netflix Recommender System",
    "fields": {
      "releaseDate": {
        "value": "2015-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "QMoE: compressed 1T model",
    "fields": {
      "numParams": {
        "value": "1600000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-DOC (fin) (23M)",
    "fields": {
      "releaseDate": {
        "value": "2018-08-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM+Adam+Lookahead",
    "fields": {
      "numParams": {
        "value": "7190000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DINOv2",
    "fields": {
      "flops": {
        "value": "7.42E+21",
        "source": "epoch",
        "citation": "table 1422016 * 3600 * 312 * 10 ** 12 * 3//10 = 7.41851136e+21gpu hours in seconds * flops of A100 * assumed utilization  rate"
      },
      "numParams": {
        "value": "1140000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "142000000",
        "source": "epoch",
        "citation": "new dataset  - named LVD142M Table 15"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SRGAN",
    "fields": {
      "releaseDate": {
        "value": "2017-05-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ULM-FiT",
    "fields": {
      "releaseDate": {
        "value": "2018-01-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 3 Opus",
    "fields": {
      "releaseDate": {
        "value": "2024-03-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MobileNet",
    "fields": {
      "numParams": {
        "value": "4200000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-04-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Joint Probability Machine Translation",
    "fields": {
      "numTokens": {
        "value": "1073480",
        "source": "epoch",
        "citation": "[WORDS]\"To evaluate our system, we trained [...] our jointprobability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132unique tokens)\""
      },
      "releaseDate": {
        "value": "2002-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Subformer (83M)",
    "fields": {
      "releaseDate": {
        "value": "2021-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "T2R + Pretrain",
    "fields": {
      "releaseDate": {
        "value": "2021-03-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOM-1B",
    "fields": {
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensorized Transformer (151M)",
    "fields": {
      "releaseDate": {
        "value": "2019-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaGo Master",
    "fields": {
      "flops": {
        "value": "1.50E+23",
        "source": "epoch",
        "citation": "This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: https://deepmind.com/blog/article/alphago-zero-starting-scratchSince AGZ reaches the ELO of AlphaGo Master in about 20 days (half of the total training time), I estimate the compute to be around half that of AGZ. I round this down to 1.5e23, and I expect this to only be accurate within an OOM."
      },
      "costDollars": {
        "value": "852748.081",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v1",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XGLM-7.5B",
    "fields": {
      "flops": {
        "value": "4.35E+22",
        "source": "epoch",
        "citation": "\" The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second\"so 312e12 * 256 * 3*7*24*3600 *0.3 = 4.347592704e+22alternative:6ND = 6*7.5e9*500e9 = 2.25e22 - we have 7.5B params and 500B tokens from \"we train four multilingual generative language models (up to 7.5 billion parameters),XGLM\u2019s, and present a comprehensive study ofmultilingual zero- and in-context few-shot learning.We train the models using a large-scale corpus of500B tokens that comprises 30 diverse languages\""
      },
      "numParams": {
        "value": "7500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "565367040000",
        "source": "epoch",
        "citation": "\"The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second\"so 311.6e3 * 3*7*24*3600 = 565367040000 words- around 1.13 words per token"
      },
      "trainingTimeDays": {
        "value": "504",
        "source": "epoch",
        "citation": "appendix A : \"The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer (Adaptive Input Embeddings)",
    "fields": {
      "flops": {
        "value": "7.3E+19",
        "source": "epoch",
        "citation": "Table 2. 8 * 67 GPU-hours. GPU is V100125 teraflop/s * 8 * 67 * 3600 * 0.3 = ~7.3e19"
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "67",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS+PDR + dynamic evaluation (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Graph-based structural reasoning",
    "fields": {
      "releaseDate": {
        "value": "1970-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProteinLM",
    "fields": {
      "flops": {
        "value": "1.60E+22",
        "source": "epoch",
        "citation": "\"We pretrained two large models on a 480 GPUs (TeslaV100-32GB) cluster for about three weeks\"21 * 24* 3600 * 480 * 125 teraFLOP/s * 0.3 (utilization) * 0.5 (two models) = 1.6e22"
      },
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "252",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "48",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Reading Twice for NLU",
    "fields": {
      "releaseDate": {
        "value": "2017-06-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hide and Seek",
    "fields": {
      "flops": {
        "value": "1.15E+18",
        "source": "epoch",
        "citation": "1.6e6 * 2 * 120e9 * 3 ~= 1.15e18 FLOP. We assume the single convolution on the lidar input is negligible, and the rest of the model (which consists of MLPs, self-attention and LSTM) has roughly 1 multiply-add per parameter. The following source has a similar estimate but does not adjust for the full number of episodes: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389"
      },
      "numParams": {
        "value": "1600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "120000000000",
        "source": "epoch",
        "citation": "\"The default model, which uses a batch size of 64,000 and 1.6 million parameters, requires 132.3 million episodes (31.7 billion frames) over 34 hours of training to reach stage 4 of the skill progression, i.e. ramp defense.\" But the full number of episodes, e.g. Figure 1, is 500 million. 500 / 132.3 * 31.7B ~= 120B."
      },
      "costDollars": {
        "value": "0.7950966035",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Verbatim Memory Transformer (108M)",
    "fields": {
      "releaseDate": {
        "value": "2022-10-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Genetic algorithm",
    "fields": {
      "releaseDate": {
        "value": "1954-07-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XLM-RoBERTa",
    "fields": {
      "numParams": {
        "value": "550000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "125250000000",
        "source": "epoch",
        "citation": "size of CC100 - copied from other rows"
      },
      "gpuCount": {
        "value": "500",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)",
    "fields": {
      "releaseDate": {
        "value": "2017-11-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WizardLM-7B",
    "fields": {
      "flops": {
        "value": "4.02E+22",
        "source": "epoch",
        "citation": "\"We use pre-trained LLaMA 7B [4] to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 \u00d710\u22125, a maximum number of tokens 2048, and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs\"Llama-7b was ~4e22. 8*70 V100-hours is ~2e20, so fine-tuning was <1% of base training."
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "70",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOM-3B",
    "fields": {
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DDPM-IP (CelebA)",
    "fields": {
      "flops": {
        "value": "3.5E+20",
        "source": "epoch",
        "citation": "\"We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). Inmore detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32\u00d732for 34 days. For LSUN tower 64\u00d764, CelebA 64\u00d764 and FFHQ 128\u00d7128, we used 16 GPUs to train the models for 3 days,5 days and 4 days, respectively\"5*16 V100-days for CelebA.5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20"
      },
      "numParams": {
        "value": "295000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "203000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "5 days"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLIDE",
    "fields": {
      "numParams": {
        "value": "3500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "250000000",
        "source": "epoch",
        "citation": "Section 4:\"We train our model on the same dataset as DALL-E (Rameshet al., 2021)\"This paper used 250M image-text pairshttps://arxiv.org/pdf/2102.12092.pdf"
      },
      "releaseDate": {
        "value": "2021-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stable Diffusion XL",
    "fields": {
      "numParams": {
        "value": "3400000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Naive Bayes",
    "fields": {
      "releaseDate": {
        "value": "1974-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Routing Transformer",
    "fields": {
      "numParams": {
        "value": "79500000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-03-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenFlamingo",
    "fields": {
      "numParams": {
        "value": "9000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "180000000",
        "source": "epoch",
        "citation": "\"OpenFlamingo models were trained for 60M interleaved (MMC4) examples1 and 120M LAION-2B examples.\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CoCa",
    "fields": {
      "flops": {
        "value": "7.30E+22",
        "source": "epoch",
        "citation": "\"Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips\"275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22"
      },
      "numParams": {
        "value": "2100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4800000000",
        "source": "epoch",
        "citation": "JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images"
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "5 days"
      },
      "gpuCount": {
        "value": "2048",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MADALINE I",
    "fields": {
      "releaseDate": {
        "value": "1962-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TeleChat",
    "fields": {
      "releaseDate": {
        "value": "2023-07-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RQ-Transformer (1.4B params ImageNet dataset)",
    "fields": {
      "flops": {
        "value": "2.91133E+18",
        "source": "epoch",
        "citation": "flops = (8) * (3120 * 10**9) * (4.5*24 * 3600) * (0.3) = 2911334400000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\"We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. \"we provide details for LSUN-cat with largest compute"
      },
      "numParams": {
        "value": "1388000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000",
        "source": "epoch",
        "citation": "size of ImageNet"
      },
      "trainingTimeDays": {
        "value": "108",
        "source": "epoch",
        "citation": "4.5  days for ImageNet for 1.4B model\"We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. \""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Trajectory-pooled conv nets",
    "fields": {
      "numParams": {
        "value": "9106245",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-05-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Unsupervised High-level Feature Learner",
    "fields": {
      "flops": {
        "value": "6E+17",
        "source": "epoch",
        "citation": "Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "10000000",
        "source": "epoch",
        "citation": "10 million 200x200 images extracted from Youtube videos"
      },
      "trainingTimeDays": {
        "value": "72",
        "source": "epoch",
        "citation": "\"We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. \""
      },
      "releaseDate": {
        "value": "2012-07-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-1.3B (finetuned)",
    "fields": {
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "UL2",
    "fields": {
      "flops": {
        "value": "1.20E+23",
        "source": "epoch",
        "citation": "Trained on 1T tokens20B * 1T * 6 = 1.2e23"
      },
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1T tokens, assuming 0.75 words per token we have 0.75T words"
      },
      "batchSize": {
        "value": "65536",
        "source": "epoch",
        "citation": "\"We pre-train all models for 500K steps with a batch size of 128 and a sequence length of 512 inputs and 512 targets using the C4 corpus. The total approximate tokens seen during pre-training is approximately 32 billion tokens.\"500k*128*512 ~= 32B128*512=65,536"
      },
      "trainingTimeDays": {
        "value": "744",
        "source": "epoch",
        "citation": "around 31 days from 'Pre-training took approximately slight more than one month for about 1 trilliontokens.' from section 5.1so around 31*24 = 744"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.318",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SVM for face detection",
    "fields": {
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": "Section 1: \"The problem that we have to solve involves training a classifierto discriminate between face and non-face patterns, using adata set of 50,000points. \""
      },
      "releaseDate": {
        "value": "1997-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Compressive Transformers for Long-Range Sequence Modelling",
    "fields": {
      "flops": {
        "value": "1.6E+20",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DNABERT",
    "fields": {
      "flops": {
        "value": "1.4E+20",
        "source": "epoch",
        "citation": "\"Since the pre-training of DNABERT model is resource-intensive (about 25\u2009days on 8 NVIDIA 2080Ti GPUs)\"Assuming FP16 and 30% utilizationCalculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP"
      },
      "numParams": {
        "value": "110000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "600",
        "source": "epoch",
        "citation": "\"Since the pre-training of DNABERT model is resource-intensive (about 25\u2009days on 8 NVIDIA 2080Ti GPUs)\""
      },
      "gpuType": {
        "value": "NVIDIA GeForce RTX 2080 Ti",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "360 Smart Brain",
    "fields": {
      "releaseDate": {
        "value": "2023-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TriNet",
    "fields": {
      "releaseDate": {
        "value": "2017-11-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (117M, SLW 110K)",
    "fields": {
      "releaseDate": {
        "value": "2021-08-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Unsupervised Scale-Invariant Learning",
    "fields": {
      "numParams": {
        "value": "451",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3500",
        "source": "epoch",
        "citation": "See Table 2 and Figure 1.There are 7 datasets, each with 200-800 of pictures. I pick 500 as the avg number of pictures"
      },
      "releaseDate": {
        "value": "2003-06-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RL for helicopter flight",
    "fields": {
      "releaseDate": {
        "value": "2006-03-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "STT Conformer-Transducer XL",
    "fields": {
      "numParams": {
        "value": "600000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DiT-XL/2 + Discriminator Guidance",
    "fields": {
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WeNet (WT2)",
    "fields": {
      "numParams": {
        "value": "33000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BOXES",
    "fields": {
      "releaseDate": {
        "value": "1968-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Big-Little Net (speech)",
    "fields": {
      "flops": {
        "value": "4.29005E+17",
        "source": "epoch",
        "citation": "980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000"
      },
      "numParams": {
        "value": "3320000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "27360000",
        "source": "epoch",
        "citation": "\"We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers\"2000h * 13680 words per hour = 27360000https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq"
      },
      "releaseDate": {
        "value": "2018-07-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fisher-Boost",
    "fields": {
      "releaseDate": {
        "value": "2010-09-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HuBERT",
    "fields": {
      "flops": {
        "value": "5.54E+21",
        "source": "epoch",
        "citation": "GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/sNumbers from Section IV part C0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "820800000",
        "source": "epoch",
        "citation": "\"When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.\"1h ~ 13,680 words13,680 * 60,000 = 820800000"
      },
      "costDollars": {
        "value": "8632.106645",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-07-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Spatiotemporal fusion ConvNet",
    "fields": {
      "numTokens": {
        "value": "97200",
        "source": "epoch",
        "citation": "[SECONDS OF VIDEO]They use UCF101, whose paper says\"We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data\""
      },
      "releaseDate": {
        "value": "2016-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Support Vector Machines",
    "fields": {
      "numParams": {
        "value": "100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "Section 6.2: \"The large database consists of 60,000 training and 10,000 test patterns\""
      },
      "releaseDate": {
        "value": "1995-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM2-35M",
    "fields": {
      "flops": {
        "value": "2.1E+20",
        "source": "epoch",
        "citation": "\"All language models were trained for 500K updates, except the 15B language model\" \"All models used 2 million tokens as batch size except the 15B model\"[Supplementary Materials]Hence: 1000B training tokens (500k steps, 2M tokens/batch)Estimate: 35M*2*1000B + 35M*4*1000B"
      },
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Rational DQN Average",
    "fields": {
      "numParams": {
        "value": "1683456",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-02-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepLabV3+",
    "fields": {
      "releaseDate": {
        "value": "2018-02-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Projected GAN",
    "fields": {
      "gpuType": {
        "value": "NVIDIA V100,NVIDIA Quadro RTX 6000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaFold 2",
    "fields": {
      "flops": {
        "value": "2.99E+21",
        "source": "epoch",
        "citation": "123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOPhttps://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4\"Training regimen\" section: \"We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPUcore, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.\""
      },
      "numTokens": {
        "value": "530000",
        "source": "epoch",
        "citation": "3 different types of input data to the network:(1) Amino acid sequence(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins(3) Template structures (3D atom coordinates of homologous structures), where availableTraining data is processed into the following two datasets that are sampled with different probabilities. Supplementary Material, Section 1.2.4. Training data:\"With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank\"Supplementary Material, Section 1.3 Self-distillation dataset:\"This gives a final dataset of 355,993 sequences\". An initial model was used to predict structures for these sequences.PDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures172788Therefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]"
      },
      "trainingTimeDays": {
        "value": "264",
        "source": "epoch",
        "citation": "7 days pretrain and 4 days finetune"
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "mini-GPT-2+Active-AdamW",
    "fields": {
      "releaseDate": {
        "value": "2023-01-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Agent57",
    "fields": {
      "releaseDate": {
        "value": "2020-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 2.1",
    "fields": {
      "releaseDate": {
        "value": "2023-11-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SpecAugment",
    "fields": {
      "releaseDate": {
        "value": "2019-04-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Japanese-GPT-1B",
    "fields": {
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XLM",
    "fields": {
      "numParams": {
        "value": "665000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PDP model for serial order",
    "fields": {
      "releaseDate": {
        "value": "1986-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stacked-LSTM+Pruning",
    "fields": {
      "numParams": {
        "value": "6160000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Linear Transformer (small)",
    "fields": {
      "releaseDate": {
        "value": "2021-02-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PULI GPTrio",
    "fields": {
      "flops": {
        "value": "5.80E+21",
        "source": "epoch",
        "citation": "8 A100s for three months8 * 312 trillion * 24 * 3600 * 90 * 0.3 (utilization assumption) = 5.8e21"
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "214000000000",
        "source": "epoch",
        "citation": "adding up column in Table 2.might be slightly off because it's counting non-Chinese tokens in the Chinese data, rather than non-Chinese words, but close."
      },
      "trainingTimeDays": {
        "value": "2200",
        "source": "epoch",
        "citation": "3 months"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Robot Parkour",
    "fields": {
      "numParams": {
        "value": "500000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce RTX 3090",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer Large + HCP",
    "fields": {
      "flops": {
        "value": "6.06E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BiLSTM for Speech",
    "fields": {
      "flops": {
        "value": "24124575958775",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numParams": {
        "value": "152061",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "36960",
        "source": "epoch",
        "citation": "https://catalog.ldc.upenn.edu/LDC93s1One sample utterance has around 10 words3696 utterances * 10 words = around 37k words"
      },
      "releaseDate": {
        "value": "2005-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RetNet",
    "fields": {
      "flops": {
        "value": "4.02E+21",
        "source": "epoch",
        "citation": "C = 6ND = 6 * 6.7 billion * 100 billion"
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "75000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "4M"
      },
      "releaseDate": {
        "value": "2023-07-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 3 Haiku",
    "fields": {
      "releaseDate": {
        "value": "2024-03-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM + DeFINE",
    "fields": {
      "releaseDate": {
        "value": "2019-11-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
    "fields": {
      "flops": {
        "value": "3.8E+20",
        "source": "epoch",
        "citation": "Numbers in Appendix B10.6h for the CIFAR model (batch size 128, 21 step/s)2.2 step/s for the LSUN model, 1.15M steps so 702.8 hoursThis is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores)https://cloud.google.com/tpu/docs/regions-zones1.25E14 FLOP/s * (8 cores / 2 cores/chip) * 702.8h * 3600s/h * 0.3 = 3.8e20"
      },
      "numParams": {
        "value": "256000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3033042",
        "source": "epoch",
        "citation": "\"We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.\"\"The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024\u00d71024 resolution.\"https://paperswithcode.com/dataset/celeba-hqLSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.https://www.tensorflow.org/datasets/catalog/lsun"
      },
      "costDollars": {
        "value": "2.601893275",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "IBM-5",
    "fields": {
      "numParams": {
        "value": "1658364",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "53358600",
        "source": "epoch",
        "citation": "\"They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.\""
      },
      "releaseDate": {
        "value": "1993-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNNLM + Dynamic KL Regularization",
    "fields": {
      "releaseDate": {
        "value": "2018-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Florence",
    "fields": {
      "flops": {
        "value": "4.83E+22",
        "source": "epoch",
        "citation": "\"The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.\"512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP"
      },
      "numParams": {
        "value": "893000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "900000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "240",
        "source": "epoch",
        "citation": "10 days on 512 A100 40GB"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ADM",
    "fields": {
      "flops": {
        "value": "5E+20",
        "source": "epoch",
        "citation": "For LSUN horse, they report 116 V100-days in pre-training.125 teraFLOP/s * 116 * 24 * 3600 * 0.4 = 5e20"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gopher (280B)",
    "fields": {
      "flops": {
        "value": "6.31E+23",
        "source": "epoch",
        "citation": "Table A266.31E+08 Train PFLOPs"
      },
      "numParams": {
        "value": "280000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "225000000000",
        "source": "epoch",
        "citation": "\"We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.\"1 token ~ 0.75 words"
      },
      "batchSize": {
        "value": "6000000",
        "source": "epoch",
        "citation": "Table 1. \"Furthermore, we increase Gopher\u2019s batch size from three to six million tokens per batch during training\""
      },
      "costDollars": {
        "value": "891638.8043",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "920",
        "source": "epoch",
        "citation": "\"We trained Gopher for 920 hours in November and December 2020 in Google\u2019s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e\""
      },
      "gpuCount": {
        "value": "4096",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.378",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RETRO-7B",
    "fields": {
      "numParams": {
        "value": "7500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "315000000000",
        "source": "epoch",
        "citation": "\"we train for 419,430,400,000 training tokens\" ~= 315B words."
      },
      "releaseDate": {
        "value": "2022-02-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gemini Ultra",
    "fields": {
      "flops": {
        "value": "5.00E+25",
        "source": "epoch",
        "citation": "This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c"
      },
      "trainingTimeDays": {
        "value": "2400",
        "source": "epoch",
        "citation": "Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days."
      },
      "gpuCount": {
        "value": "55000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransE",
    "fields": {
      "flops": {
        "value": "1.34093E+18",
        "source": "epoch",
        "citation": "8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)8 hours 0.33 util rate"
      },
      "numParams": {
        "value": "942000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "17000000",
        "source": "epoch",
        "citation": "\"it can be successfully trained on a large scale data set with 1Mentities, 25k relationships and more than 17M training samples\""
      },
      "costDollars": {
        "value": "17.57961617",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-MemoryAug (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "All-attention network + adaptive span",
    "fields": {
      "flops": {
        "value": "4.6E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "133000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen-VL-Max",
    "fields": {
      "releaseDate": {
        "value": "2024-01-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Delta RNN (+ full context)",
    "fields": {
      "flops": {
        "value": "1100000000000000100",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "44600000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hybrid H3-1.3B",
    "fields": {
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gato",
    "fields": {
      "flops": {
        "value": "5.44E+21",
        "source": "epoch",
        "citation": "256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.5 utilization = 5.44e21 FLOPs"
      },
      "numParams": {
        "value": "1180000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "6781.082295",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "96",
        "source": "epoch",
        "citation": "4 days"
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SRU++ Large only 2 attention layers (k=5)",
    "fields": {
      "releaseDate": {
        "value": "2021-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NoPos",
    "fields": {
      "flops": {
        "value": "1.61E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT3-6.7B (rerun of original)",
    "fields": {
      "flops": {
        "value": "1.20E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Taiyi-Stable Diffusion",
    "fields": {
      "flops": {
        "value": "5.10E+22",
        "source": "epoch",
        "citation": "Fine-tuning: 32 NVIDIA A100 GPUs for 100 hours32 * 312e12 * 30% * 100 * 60 * 60 = 1.078272e+21 FLOPBase model: Stable Diffusion, 5e+22 FLOP"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "100",
        "source": "epoch",
        "citation": "32 NVIDIA A100 GPUs for 100 hours"
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNet-1001",
    "fields": {
      "numParams": {
        "value": "10200000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Flan T5-XXL + BLIP-2",
    "fields": {
      "flops": {
        "value": "1.20E+21",
        "source": "epoch",
        "citation": "fine-tuned from Flan-T5 XXL (11B) and ViT-gfine-tuning compute:\"using a single 16-A100(40G) machine, our largest model withViT-g and FlanT5-XXL requires less than 6 days for the firststage and less than 3 days for the second stage.\"16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21"
      },
      "numParams": {
        "value": "12100000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "200",
        "source": "epoch",
        "citation": "\"less than 6 days for the firststage and less than 3 days for the second stage\"9*24 is 216, rounding down a bit is 200 hours"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Code Llama-7B",
    "fields": {
      "flops": {
        "value": "1.10E+23",
        "source": "epoch",
        "citation": "2.5e22 finetune compute + 8.4e22 base compute for Llama 2-7B, for ~1.1e23 compute overall"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaCode",
    "fields": {
      "flops": {
        "value": "1.57E+23",
        "source": "epoch",
        "citation": "Figure 7 (a) shows a maximum training compute budget of approx 20000 TPU-days per model.20000 days * 275 TFLOPS * 0.33 utilization = 1.6e23 FLOPhttps://www.wolframalpha.com/input?i=20000+*+275+teraFLOPS+*+1+day+*+0.33"
      },
      "numParams": {
        "value": "41100000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4718592",
        "source": "epoch",
        "citation": "2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592trained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup"
      },
      "gpuCount": {
        "value": "3750",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4,Google TPU v4i",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-02-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-FWM (WT2)",
    "fields": {
      "flops": {
        "value": "7.39E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "37000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NASNet-A",
    "fields": {
      "numParams": {
        "value": "89000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-30B",
    "fields": {
      "numParams": {
        "value": "30000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Goat-7B",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A10 PCIe",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VideoMAE V2",
    "fields": {
      "flops": {
        "value": "9.70E+21",
        "source": "epoch",
        "citation": "finetuned on ViT-g (smaller than ViT-G with 1B params)\"It takes more than two weeks to pre-train a ViT-g model with VideoMAEon 64 A100 GPUs\"64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "330",
        "source": "epoch",
        "citation": "2 weeks"
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (117M)",
    "fields": {
      "releaseDate": {
        "value": "2019-02-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer + Simple Recurrent Unit",
    "fields": {
      "flops": {
        "value": "1.1E+19",
        "source": "epoch",
        "citation": "\"We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtainedusing 8 GPUs in parallel, which provide a large effective batch size during training. To approximatethe setup, we update the model parameters every 5\u00d75120 tokens and use 16,000 warm-up stepsfollowing OpenNMT suggestions. We train eachmodel for 40 epochs (250,000 steps), and perform3 independent trials for each model configuration.A single run takes about 3.5 days with a Tesla V100 GPU.\"125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19"
      },
      "numParams": {
        "value": "90000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ContextNet + Noisy Student",
    "fields": {
      "flops": {
        "value": "5.40E+21",
        "source": "epoch",
        "citation": "\"Five generations of models numbered 0 to 4, are trained,where the baseline model is taken to be the generation-zeromodel. The baseline ContextNet model has the same encoderas CN-2, but has a one-layer RNN decoder with dimension 640.Meanwhile, CN-w with w=1.25, 1.75, 2.25 and 2.5 have beenset to be the ASR model from generation 1 through 4. Eachgeneration is trained on 128 Google Cloud TPU chips for 1-3days\"Roughly assuming each generation is an average of 2 days. The TPU version is likely v3 given this is a 2020 paper.we get 128 * 10 * 24 * 3600 * 123 tflops * 0.4  (assumed utilization) = 5.4e21"
      },
      "trainingTimeDays": {
        "value": "240",
        "source": "epoch",
        "citation": "roughly 10 days"
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-01-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GCRN-M1, dropout",
    "fields": {
      "flops": {
        "value": "3.04E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "42000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-12-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Megatron-BERT",
    "fields": {
      "flops": {
        "value": "6.03E+22",
        "source": "epoch",
        "citation": "A source: https://lair.lighton.ai/akronomicon/ claims 5.7e22The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdfParam-based calculation:6ND = 6*3.9e9*2e6*1024*1024 = 4.8e22 FLOPTime-based calculation:The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.BERT: batch size 1024, 2e6 iterations total.So we should expect 4B => 1.0 days per epoch (69e3*512 examples)=> 2e6*1024/(69e3*512) = 58 days trainingOn 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.C=15.1 PFLOPS * 58 days = 7.6e22 FLOP.The param and time calculations seem more trustworthy. Geometric mean is 6.027e22 FLOP"
      },
      "numParams": {
        "value": "3900000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "34800000000",
        "source": "epoch",
        "citation": "\"The resulting aggregate corpus contains 174 GB of deduplicated text.\""
      },
      "batchSize": {
        "value": "524288",
        "source": "epoch",
        "citation": "\"we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearlyover 2 million iterations. Other training parameters are keptthe same as (Devlin et al., 2018).\"in Devlin et al (BERT), sequences are 512 tokens"
      },
      "costDollars": {
        "value": "208034.3937",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1392",
        "source": "epoch",
        "citation": "The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.BERT: batch size 1024, 2e6 iterations total.So we should expect 4B => 1.0 days per epoch (69e3*512 examples)=> 2e6*1024/(69e3*512) = 58 days training"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100S PCIe 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.2269",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multipop Adaptive Continuous Stack (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Once for All",
    "fields": {
      "flops": {
        "value": "1.78E+21",
        "source": "epoch",
        "citation": "4.2k V100-hours (table 1)0.33 utilization rate"
      },
      "numParams": {
        "value": "7700000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "13000",
        "source": "epoch",
        "citation": "from Table 1"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-04-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Conv-DBN",
    "fields": {
      "releaseDate": {
        "value": "2009-06-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "N-gram+Cache",
    "fields": {
      "releaseDate": {
        "value": "2014-12-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransformerXL + PowerSGD + L-Greco",
    "fields": {
      "flops": {
        "value": "4.14E+17",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MedBERT",
    "fields": {
      "flops": {
        "value": "8.54582E+18",
        "source": "epoch",
        "citation": "flops = (1) * (2826 * 10**10) * (24*7 * 3600) * (0.5) = 8545824000000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)I assume higher utilization rate, because only 1 GPU is used.Citation from the text:\"We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size).\" - page 11"
      },
      "numParams": {
        "value": "17000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "\"We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size).\" - page 11"
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GraphCast",
    "fields": {
      "releaseDate": {
        "value": "2023-11-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MGK 4 heads (medium)",
    "fields": {
      "flops": {
        "value": "6.67E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "90000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepConPred2",
    "fields": {
      "numTokens": {
        "value": "3443",
        "source": "epoch",
        "citation": "\"Finally, our training set contained 3443 protein domains\""
      },
      "releaseDate": {
        "value": "2018-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Instruct-GPT + Mind's Eye",
    "fields": {
      "numParams": {
        "value": "176500000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeBERTaV3-large + KEAR",
    "fields": {
      "numParams": {
        "value": "418000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM+NeuralCache",
    "fields": {
      "flops": {
        "value": "1.02E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "2100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-09-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TSLM+MoS (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2019-01-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AFP+FPI (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2021-06-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LeNet-5",
    "fields": {
      "flops": {
        "value": "2810937600000",
        "source": "epoch",
        "citation": "\"[LeNet5] contains 390408 connections\" = multiply-addsMNIST - 60,000 data points20 epochs"
      },
      "numParams": {
        "value": "60000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)"
      },
      "releaseDate": {
        "value": "1998-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Max-Margin Markov Networks",
    "fields": {
      "numTokens": {
        "value": "600",
        "source": "epoch",
        "citation": "The data set is divided into 10 folds of \u223c 600 training and \u223c 5500 testing examples.The accuracy results, ... are averages over the 10 folds"
      },
      "releaseDate": {
        "value": "2004-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RGC+ASQ (WT2)",
    "fields": {
      "numParams": {
        "value": "209000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-08-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Volcano 13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "30",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen-14B",
    "fields": {
      "flops": {
        "value": "2.50E+23",
        "source": "epoch",
        "citation": "3T tokens per Table 114b*3T*6 = 2.5e23"
      },
      "numParams": {
        "value": "14000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "Table 1"
      },
      "releaseDate": {
        "value": "2023-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT-Base/32",
    "fields": {
      "numParams": {
        "value": "86000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XGLM",
    "fields": {
      "numParams": {
        "value": "7500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1740000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VD-RHN",
    "fields": {
      "flops": {
        "value": "3.57E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "32000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-07-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Large regularized LSTM",
    "fields": {
      "flops": {
        "value": "9.1E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "66000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-09-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ShuffleNet v2",
    "fields": {
      "numParams": {
        "value": "2280000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cross-lingual alignment",
    "fields": {
      "flops": {
        "value": "2.56E+18",
        "source": "epoch",
        "citation": "From author communication:Precision: float32Hardware: 4 GPU  NVIDIA 1080TiNVIDIA 1080Ti: 1.06E+13Compute: 7 GPU-days0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h= 2.56E+18"
      },
      "costDollars": {
        "value": "7.832518623",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XuanYuan 2.0",
    "fields": {
      "numParams": {
        "value": "176200000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XGen-7B",
    "fields": {
      "flops": {
        "value": "8.02E+22",
        "source": "epoch",
        "citation": "270,336 TPUv4-hours per the carbon emissions section. They must mean chip-hours, not core-hours, because they multiply by 192W, which is the max power consumption of a TPU-v4 chip.https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 270336 * 275 teraflop/s * 3600 * 0.3 = 8.02e22 FLOPalso, using 6ND:1484 billion tokens * 6.7 billion * 6 = 5.97e22"
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1113000000000",
        "source": "epoch",
        "citation": "1484B tokens per Table 2. 1113B words at 0.75 words/token"
      },
      "batchSize": {
        "value": "1048576",
        "source": "epoch",
        "citation": "\"batch size of 128, and a sequence length of 8,192 tokens\""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RHN(depth=40)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Youtube recommendation model",
    "fields": {
      "releaseDate": {
        "value": "2016-09-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cohere Command",
    "fields": {
      "numParams": {
        "value": "52000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mogrifier RLSTM (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2022-11-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tongyi Qianwen (Qwen) 2.0",
    "fields": {
      "releaseDate": {
        "value": "2023-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RGC+ASQ (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-08-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (762M)",
    "fields": {
      "releaseDate": {
        "value": "2019-02-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SimCSE",
    "fields": {
      "releaseDate": {
        "value": "2022-05-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenLLaMA-13B",
    "fields": {
      "flops": {
        "value": "7.80E+22",
        "source": "epoch",
        "citation": "13b * 1T * 6 = 7.8e22"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1T tokens, or ~750B words"
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "gpt-sw3-40b",
    "fields": {
      "flops": {
        "value": "7.68E+22",
        "source": "epoch",
        "citation": "aproximation 6ND = 6*320E9*40e9 = 7.68e22\"GPT-SW3 has been trained on a dataset containing 320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code.\""
      },
      "numParams": {
        "value": "40000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT2-Large+LHOPT",
    "fields": {
      "flops": {
        "value": "1.60E+21",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "760000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stockmark-13B",
    "fields": {
      "flops": {
        "value": "1.72E+22",
        "source": "epoch",
        "citation": "6ND = 6*13B*220B = 17160000000000000000000\"Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens. This model is developed by Stockmark Inc.\""
      },
      "numParams": {
        "value": "13200000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "220000000000",
        "source": "epoch",
        "citation": "220B tokens so -  assuming 1 word per token - 220B words"
      },
      "releaseDate": {
        "value": "2023-10-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Spark 3.0",
    "fields": {
      "releaseDate": {
        "value": "2023-10-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Char-CNN-BiLSTM",
    "fields": {
      "releaseDate": {
        "value": "2019-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BellKor 2009",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\""
      },
      "releaseDate": {
        "value": "2009-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProBERTa",
    "fields": {
      "flops": {
        "value": "9.72E+18",
        "source": "epoch",
        "citation": "\"we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours\"4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18"
      },
      "numParams": {
        "value": "44000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "18",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Refined Part Pooling",
    "fields": {
      "releaseDate": {
        "value": "2018-01-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NMST+GPT-2",
    "fields": {
      "flops": {
        "value": "1.2E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "124000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNet-50 Billion-scale",
    "fields": {
      "numParams": {
        "value": "25000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
    "fields": {
      "flops": {
        "value": "4.37E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-11-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Whisper",
    "fields": {
      "flops": {
        "value": "4.65E+22",
        "source": "epoch",
        "citation": "See figure 9"
      },
      "numParams": {
        "value": "1550000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9302400000",
        "source": "epoch",
        "citation": "\"When scaled to 680,000 hours of multilingual and multitasksupervision, the resulting models generalize wellto standard benchmarks and are often competitivewith prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.\"13,680 words/h * 680,000h = 9,302,400,000 words"
      },
      "releaseDate": {
        "value": "2022-09-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLUUMI",
    "fields": {
      "numParams": {
        "value": "176000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "38000000000",
        "source": "epoch",
        "citation": "38B tokens"
      },
      "gpuType": {
        "value": "AMD Instinct MI250X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Libratus",
    "fields": {
      "flops": {
        "value": "5.51E+20",
        "source": "epoch",
        "citation": "\"In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.\"\"Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.\"I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).The first system has 752 nodes a 2CPUs a 14cores each.source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/1. 12M core hours for 196 cores2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores2.1 That's 42.5 GFLOPS per core.3. Running this for 12M h3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs4. Assuming 30% utilization 1.823e21 * 0.3\u2192 5.51e20 FLOPs"
      },
      "costDollars": {
        "value": "6253.485928",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RT-1 + AutoRT",
    "fields": {
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Low-Cost Collaborative Network",
    "fields": {
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-05-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN+LDA",
    "fields": {
      "releaseDate": {
        "value": "2012-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fuzzy NN",
    "fields": {
      "flops": {
        "value": "1403117760",
        "source": "epoch",
        "citation": "1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples"
      },
      "numParams": {
        "value": "1166",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "436",
        "source": "epoch",
        "citation": "\"The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds\" and 50% of the dataset was used. 871*0.5 ~= 436"
      },
      "releaseDate": {
        "value": "1992-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Jurassic-2 Jumbo",
    "fields": {
      "numParams": {
        "value": "178000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MicroNet (Adaptive, Cache)",
    "fields": {
      "numParams": {
        "value": "8300000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WGAN-GP",
    "fields": {
      "releaseDate": {
        "value": "2017-03-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "M6-T",
    "fields": {
      "flops": {
        "value": "5.50E+21",
        "source": "epoch",
        "citation": "Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape"
      },
      "numParams": {
        "value": "1000000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1900000000000",
        "source": "epoch",
        "citation": "Images"
      },
      "gpuCount": {
        "value": "480",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen-72B",
    "fields": {
      "flops": {
        "value": "1.30E+24",
        "source": "epoch",
        "citation": "72 billion params, 3 trillion tokens72b * 3T * 6 = 1.3e24"
      },
      "numParams": {
        "value": "72000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "Table 1 https://arxiv.org/abs/2309.16609(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)"
      },
      "releaseDate": {
        "value": "2023-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 3 Sonnet",
    "fields": {
      "releaseDate": {
        "value": "2024-03-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Neural cache model (size=2000)",
    "fields": {
      "releaseDate": {
        "value": "2016-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
    "fields": {
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Characterizing Verbatim Short-Term Memory in Neural Language Models (182M)",
    "fields": {
      "numParams": {
        "value": "182000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TCN (P-MNIST)",
    "fields": {
      "numParams": {
        "value": "42000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "6-Act Tether",
    "fields": {
      "numParams": {
        "value": "5000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "German ELECTRA Large",
    "fields": {
      "flops": {
        "value": "1.43E+21",
        "source": "epoch",
        "citation": "flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2"
      },
      "numParams": {
        "value": "335000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "27287800000",
        "source": "epoch",
        "citation": "163.4GB from Table 1 in the paperassuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0"
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "7 days from Table 2"
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stack RNN",
    "fields": {
      "numParams": {
        "value": "2010000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Image Classification with the Fisher Vector: Theory and Practice",
    "fields": {
      "flops": {
        "value": "90842400000000",
        "source": "epoch",
        "citation": "They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003"
      },
      "costDollars": {
        "value": "0.001409574533",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-06-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Boss (DARPA Urban Challenge)",
    "fields": {
      "releaseDate": {
        "value": "2008-07-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StarCoder 2 3B",
    "fields": {
      "flops": {
        "value": "5.94E+22",
        "source": "epoch",
        "citation": "estimation is given in Table 6"
      },
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3100000000000",
        "source": "epoch",
        "citation": "from Table 7, 31T tokens "
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-02-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DD-PPO",
    "fields": {
      "flops": {
        "value": "7.8E+20",
        "source": "epoch",
        "citation": "\"Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days \u2013 180 GPU-days of training\"125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20"
      },
      "trainingTimeDays": {
        "value": "66",
        "source": "epoch",
        "citation": "2.75 days"
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeT5-large",
    "fields": {
      "flops": {
        "value": "2.72E+21",
        "source": "epoch",
        "citation": "\"We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days\"16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21"
      },
      "numParams": {
        "value": "770000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "504",
        "source": "epoch",
        "citation": "21 days"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Walking Minotaur robot",
    "fields": {
      "releaseDate": {
        "value": "2019-06-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Statement Curriculum Learning",
    "fields": {
      "numTokens": {
        "value": "275000000000",
        "source": "epoch",
        "citation": "Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB."
      },
      "releaseDate": {
        "value": "2022-03-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Selfish-RNN (ON-LSTM)",
    "fields": {
      "releaseDate": {
        "value": "2021-01-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "rTop-k(distributed setting)",
    "fields": {
      "flops": {
        "value": "1.46E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "69000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransfoRNN(d=1024)(2-layer) (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2021-04-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "IDEFICS",
    "fields": {
      "flops": {
        "value": "1.16E+23",
        "source": "epoch",
        "citation": "flops = 512 * 312e12 * 28*24*3600 * 0.3(num gpus) * (peak perforemence) * (time in seconds) * (assumed utilization rate)\"The IDEFICS models were trained on an AWS SageMaker cluster with 8x80GB A100 GPUs nodes and EFA network.    IDEFICS-80B took ~28 days of training on 64 nodes (512 GPUs).\""
      },
      "numParams": {
        "value": "80000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen-VL",
    "fields": {
      "numParams": {
        "value": "9600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1400000000",
        "source": "epoch",
        "citation": "1.4B text-image pairs"
      },
      "releaseDate": {
        "value": "2023-08-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ShuffleNet v1",
    "fields": {
      "numParams": {
        "value": "2430000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-07-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RetinaNet-R50",
    "fields": {
      "numParams": {
        "value": "34000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Orca 2-13B",
    "fields": {
      "flops": {
        "value": "4.60E+22",
        "source": "epoch",
        "citation": "4.55e22 base compute from Llama-13 + 8.6e20 finetune compute ~= 4.6e22"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "80",
        "source": "epoch",
        "citation": "17+40+23 hours\"We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16.For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch,~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continuetraining on ~1.8 million GPT-4 data for 4 epochs\""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DEQ-TrellisNet",
    "fields": {
      "releaseDate": {
        "value": "2019-09-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL + AutoDropout (WT2)",
    "fields": {
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GenSLM",
    "fields": {
      "flops": {
        "value": "1.42E+21",
        "source": "epoch",
        "citation": "See Table 3Overall ZettaFlops 1.42"
      },
      "numParams": {
        "value": "25000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multiresolution CNN",
    "fields": {
      "numParams": {
        "value": "126125568",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "50000000",
        "source": "epoch",
        "citation": "\"We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.\"So 5e+7 datapoints and 10 epochs."
      },
      "releaseDate": {
        "value": "2014-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VLM-4",
    "fields": {
      "releaseDate": {
        "value": "2022-04-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NeuMF (Pinterest)",
    "fields": {
      "releaseDate": {
        "value": "2017-08-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPPNet",
    "fields": {
      "flops": {
        "value": "3.41107E+18",
        "source": "epoch",
        "citation": "\"All networks in this paper can betrained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.\"4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation"
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": "Section 3.1: \"We train the networks on the 1000-category trainingset of ImageNet 2012.\""
      },
      "costDollars": {
        "value": "65.07107045",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "672",
        "source": "epoch",
        "citation": "\"All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.\""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX TITAN",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-06-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TSLM+MoS (WT2)",
    "fields": {
      "numParams": {
        "value": "9120000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPN-4",
    "fields": {
      "releaseDate": {
        "value": "2014-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "B2T connection (16L)",
    "fields": {
      "flops": {
        "value": "2.8E+19",
        "source": "epoch",
        "citation": "192*7 GPU (P100) hours per Table 619 TFLOP/s 19 trillion * 192 * 7 * 3600 * 0.3 = 2.76e19"
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNet-200",
    "fields": {
      "trainingTimeDays": {
        "value": "500",
        "source": "epoch",
        "citation": "\"about 3 weeks\""
      },
      "releaseDate": {
        "value": "2016-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-7B (LoRA finetuned)",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProtT5-XXL-BFD",
    "fields": {
      "flops": {
        "value": "3.70E+22",
        "source": "epoch",
        "citation": "FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, "
      },
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Image generation",
    "fields": {
      "flops": {
        "value": "475200000000000",
        "source": "epoch",
        "citation": "From https://openai.com/blog/ai-and-compute/ Appendix\"less than 0.0000055 pfs-days\"(86400*10^15*0.0000055)"
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "\"We trained generative models of images from the MNIST and Frey Face datasets\"MNIST has 60k imageshttps://en.wikipedia.org/wiki/MNIST_databaseFrey Face has 2k imageshttps://cs.nyu.edu/~roweis/data.html"
      },
      "costDollars": {
        "value": "0.006416238762",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sandwich Transformer",
    "fields": {
      "flops": {
        "value": "1.58E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "209000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MatrixFac for Recommenders",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2009-08-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-08-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SeamlessM4T",
    "fields": {
      "numParams": {
        "value": "2300000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multilingual-E5-large",
    "fields": {
      "numParams": {
        "value": "560000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DQN",
    "fields": {
      "flops": {
        "value": "2.3E+15",
        "source": "epoch",
        "citation": "Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connectedFirst layer: 20*20*3*16*8*8 = 1.23M add-multipliesSecond layer: 9*9*16*32*4*4 = 0.66M add-multipliesThird layer: 9*9*32*256 = 0.66M add-mutlipliesTotal ~ 2.55M add-multiplies2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass= 2.3 PF = 2.7e-5 pfs-days"
      },
      "numParams": {
        "value": "836096",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "0.04037043829",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-12-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HMM Word Alignment",
    "fields": {
      "numTokens": {
        "value": "442316",
        "source": "epoch",
        "citation": "[WORDS]Table 1.I take the sum of all words. Maybe it would be better to use only the sum of English or German words?"
      },
      "releaseDate": {
        "value": "1996-08-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Maximum Entropy Models for machine translation",
    "fields": {
      "numTokens": {
        "value": "519523",
        "source": "epoch",
        "citation": "[WORDS]Table 1"
      },
      "releaseDate": {
        "value": "2002-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DiffQ Transformer (16L)",
    "fields": {
      "flops": {
        "value": "3.36E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DensePhrases",
    "fields": {
      "flops": {
        "value": "2.09952E+18",
        "source": "epoch",
        "citation": "flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)model of GPU from appendix B (Titan Xp)number of GPUs from table in appendix Aflops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948"
      },
      "numTokens": {
        "value": "58000000",
        "source": "epoch",
        "citation": "from appendix D \"The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.\"assuming 40 words per question we get around ~ 58M"
      },
      "trainingTimeDays": {
        "value": "20",
        "source": "epoch",
        "citation": "appendix A row 3"
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA TITAN Xp",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PolyNet",
    "fields": {
      "flops": {
        "value": "6.4E+19",
        "source": "epoch",
        "citation": "Section 5: \"ResNet-500 [has] similar computationcosts to our Very Deep PolyNet\".ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.560k iterations, batch size 512:Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19"
      },
      "numParams": {
        "value": "92000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GTX Titan X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-11-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VD-LSTM+REAL Medium",
    "fields": {
      "releaseDate": {
        "value": "2016-11-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-Char-Large",
    "fields": {
      "flops": {
        "value": "2.65E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "19000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-08-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MGK 8 heads (small)",
    "fields": {
      "releaseDate": {
        "value": "2021-10-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaGeometry",
    "fields": {
      "numParams": {
        "value": "151000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNet-152 (ImageNet)",
    "fields": {
      "flops": {
        "value": "1.21E+19",
        "source": "epoch",
        "citation": "(11.4 *10^9) mult-adds per forward pass2 FLOPS/ mult-add3.5 for forward & backward pass1.2 * 10^6 examples in dataset128 epochsSource:x"
      },
      "numParams": {
        "value": "60000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": "\"We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images\""
      },
      "costDollars": {
        "value": "92.03143352",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-12-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Domain Adaptation",
    "fields": {
      "numParams": {
        "value": "15260",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4652",
        "source": "epoch",
        "citation": "Dataset introduced in 'Adapting Visual Category Models to NewDomains'"
      },
      "releaseDate": {
        "value": "2011-11-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XLMR-XXL",
    "fields": {
      "numParams": {
        "value": "10700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "125250000000",
        "source": "epoch",
        "citation": "\"We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.\"1 token ~ 0.7 words"
      },
      "releaseDate": {
        "value": "2021-08-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2017-08-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SACHS",
    "fields": {
      "numParams": {
        "value": "178",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "5400",
        "source": "epoch",
        "citation": "I think? \" The truncated singlecell data set (420 data points) shows a large(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). \"Seems potentially wrong by maybe 20%. Might need to add 1200."
      },
      "releaseDate": {
        "value": "2005-04-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao Aquila",
    "fields": {
      "numParams": {
        "value": "33000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Kosmos-2",
    "fields": {
      "flops": {
        "value": "2.50026E+20",
        "source": "epoch",
        "citation": "flops = (256) * (2826* 10**10) * (24 * 3600) * (0.4) = 2.500263936e+20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)peak flop from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957citation from text:\"We train the model on 256 V100 GPUs and the training takes approximately one day to complete\"\"We train KOSMOS-2 for 60k steps, equivalent to around 25 billion tokens\"alternative:6ND = 6*25B*1.6B = 2.4e20"
      },
      "numParams": {
        "value": "1600000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "\" We train the model on 256 V100 GPUs and the training takes approximately one day to complete\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PaLM-SayCan",
    "fields": {
      "numParams": {
        "value": "540000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "100",
        "source": "epoch",
        "citation": "The RL model is trained using 16 TPUv3 chips and for about 100 hours"
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-08-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dolly 2.0-12b",
    "fields": {
      "numParams": {
        "value": "12000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SRU++ Large",
    "fields": {
      "flops": {
        "value": "1.1E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "234000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SimCLR",
    "fields": {
      "numParams": {
        "value": "375000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Polarity Classifier",
    "fields": {
      "numTokens": {
        "value": "11112",
        "source": "epoch",
        "citation": "Section 3.3 reveals there are 11,112 sentences. Since this is phrase-level sentiment analysis sentences seem like the best unit"
      },
      "releaseDate": {
        "value": "2009-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-2.7B (finetuned on PTB)",
    "fields": {
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Decoupled weight decay regularization",
    "fields": {
      "flops": {
        "value": "2.47E+18",
        "source": "epoch",
        "citation": "From author communicationPer image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs = 2.47E+18 FLOPs"
      },
      "numParams": {
        "value": "36500000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "8.072882644",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PG-SWGAN",
    "fields": {
      "releaseDate": {
        "value": "2019-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pandemonium (morse)",
    "fields": {
      "flops": {
        "value": "600000000",
        "source": "epoch",
        "citation": "The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M"
      },
      "releaseDate": {
        "value": "1959-02-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-2.8b",
    "fields": {
      "numParams": {
        "value": "2800000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Theseus 6/768",
    "fields": {
      "numParams": {
        "value": "66000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-C",
    "fields": {
      "flops": {
        "value": "2.1E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "148000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepNet",
    "fields": {
      "numParams": {
        "value": "3200000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "12000000000",
        "source": "epoch",
        "citation": "\" The final data consists of 102 languages, 1932 directions, and12B sentence pairs.\""
      },
      "releaseDate": {
        "value": "2022-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gated HORNN (3rd order)",
    "fields": {
      "numParams": {
        "value": "8970000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-04-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Helpful Harmless preference model",
    "fields": {
      "numParams": {
        "value": "52000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Spatially-Sparse CNN",
    "fields": {
      "releaseDate": {
        "value": "2014-09-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TFE SVM",
    "fields": {
      "releaseDate": {
        "value": "2006-02-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama 2-70B",
    "fields": {
      "flops": {
        "value": "8.10E+23",
        "source": "epoch",
        "citation": "\"Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB\" of which 1720320 GPU hours were used to train the 70B model.311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP."
      },
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1500000000000",
        "source": "epoch",
        "citation": "2 trillion tokens ~= 1.5 trillion words"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "1620000",
        "source": "epoch",
        "citation": "A100 cost in 2023: $1.10/hourTraining time: 1720320 A100 GPU-hoursInflation adjustment: $1.000 2020 = $1.145 2023"
      },
      "trainingTimeDays": {
        "value": "2160",
        "source": "epoch",
        "citation": "Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods."
      },
      "gpuCount": {
        "value": "1000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.435",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS+Noisin+dynamic evaluation",
    "fields": {
      "releaseDate": {
        "value": "2018-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fine-tuned-AWD-LSTM-DOC(fin)",
    "fields": {
      "flops": {
        "value": "1.92E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "23000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-11-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "InstructBLIP",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "36",
        "source": "epoch",
        "citation": "\"All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.\""
      },
      "gpuCount": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Progressive LRD",
    "fields": {
      "flops": {
        "value": "6.2E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "31000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Selfish-RNN (SNT-ASGD)RHNs",
    "fields": {
      "releaseDate": {
        "value": "2021-01-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Context-dependent RNN",
    "fields": {
      "releaseDate": {
        "value": "2012-07-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MathGPT",
    "fields": {
      "releaseDate": {
        "value": "2023-08-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL + SIS",
    "fields": {
      "flops": {
        "value": "1.04E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "246000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensor-Transformer(1core)+PN (WT103)",
    "fields": {
      "flops": {
        "value": "1.58E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "85300000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-03-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SparseOPT-66B",
    "fields": {
      "numParams": {
        "value": "66000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeFuse-13B",
    "fields": {
      "flops": {
        "value": "3.30E+23",
        "source": "epoch",
        "citation": "\"CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, witha Hardware FLOPs Utilization (HFU) of approximately 60%. Thetraining process took approximately 40 days to complete\"512 * 312 trillion * 40 * 24 * 3600 * 0.6 = 3.3e23Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000000000",
        "source": "epoch",
        "citation": "1T tokens, mostly code but some Chinese/English"
      },
      "batchSize": {
        "value": "16777216",
        "source": "epoch",
        "citation": "4096 batch size, 4096 sequence length"
      },
      "trainingTimeDays": {
        "value": "960",
        "source": "epoch",
        "citation": "~40 days"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLEE",
    "fields": {
      "releaseDate": {
        "value": "1968-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Bayesian automated hyperparameter tuning",
    "fields": {
      "releaseDate": {
        "value": "2012-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DenseNet-264",
    "fields": {
      "numParams": {
        "value": "34000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-08-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "4 layer QRNN (h=2500)",
    "fields": {
      "flops": {
        "value": "2.4E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "26000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-03-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Detic",
    "fields": {
      "flops": {
        "value": "2.344E+19",
        "source": "epoch",
        "citation": "28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization ratefor Swin-B model from page 8 :  'Training our ResNet50 model takes \u223c 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in \u223c 24 hours on 32 GPUs.'"
      },
      "numParams": {
        "value": "88000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "16900000",
        "source": "epoch",
        "citation": "14M + 1.5M + 1.2M + 100K + 100K = 16900000.0table above section 5.1"
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "from page 8 :  'Training our ResNet50 model takes \u223c 22 hours on 8 V100GPUs. The large 21K Swin-B model trains in \u223c 24 hours on 32 GPUs.'"
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL+AdamGapAware(GA)",
    "fields": {
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SimCLRv2",
    "fields": {
      "numParams": {
        "value": "795000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "mBART-50",
    "fields": {
      "flops": {
        "value": "3.28E+21",
        "source": "epoch",
        "citation": "flops = (256) * (2826 * 10**10) * (2.5 * 7 * 24 * 3600) * (0.3) = 3.281596416e+21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\"mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs\"V100 have peak flop 28.26 TFLOPS  from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957"
      },
      "numParams": {
        "value": "610000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "420",
        "source": "epoch",
        "citation": "\"mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Attend-Infer-Repeat",
    "fields": {
      "flops": {
        "value": "6.4489E+16",
        "source": "epoch",
        "citation": "(peak FLOPs for GPU - 1244 GFLOPs) times (training time - 3600 * 48 second) * (0.3 assumed utilization rate)"
      },
      "numParams": {
        "value": "82130304",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "60000 MNIST images"
      },
      "trainingTimeDays": {
        "value": "48",
        "source": "epoch",
        "citation": "48 hours for MNIST model, 72 hours for 3D scenes model"
      },
      "gpuType": {
        "value": "NVIDIA Quadro K4000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-08-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransformerXL + spectrum control",
    "fields": {
      "flops": {
        "value": "459999999999999940",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "151000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-03-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GSM",
    "fields": {
      "numTokens": {
        "value": "4000000",
        "source": "epoch",
        "citation": "from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairsdownload-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=downloadwc -w on train-v.1.1 returns 4017471 words so around 4M words"
      },
      "releaseDate": {
        "value": "2017-07-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Starling-LM-7B-alpha",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EnCodec",
    "fields": {
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multi-cause Binary Clustering",
    "fields": {
      "releaseDate": {
        "value": "1995-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DiT-XL/2",
    "fields": {
      "flops": {
        "value": "6E+20",
        "source": "epoch",
        "citation": "~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. 3M iterations with a batch size of 256."
      },
      "numParams": {
        "value": "675000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DistilProtBert",
    "fields": {
      "flops": {
        "value": "1.9E+20",
        "source": "epoch",
        "citation": "\"Pretraining was done on five v100 32-GB Nvidia GPUs from a DGXcluster with a local batch size of 16 examples... Every epoch run took approximately 4 days, resulting in total pretraining time of 12 days\"5 * 125 teraFLOP/s * 12 * 24 * 3600 * 0.3 (assumed utilization) = 1.9e20"
      },
      "numParams": {
        "value": "230000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "288",
        "source": "epoch",
        "citation": "12 days"
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Regularized SVD for Collaborative Filtering",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2007-08-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Contriever",
    "fields": {
      "numParams": {
        "value": "110000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen1.5 72B",
    "fields": {
      "numParams": {
        "value": "72000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-02-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Perceptron (1960)",
    "fields": {
      "flops": {
        "value": "720000000",
        "source": "epoch",
        "citation": "4000 * 12000 * 15from the text \"This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visualstimuli in the perceptron,\"from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.\" For the first system, the computing time averaged about 15 seconds per stimulus cycle, \"In Fig 10 we see up to 4000 stimuli"
      },
      "numParams": {
        "value": "1000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "5000",
        "source": "epoch",
        "citation": "from the text \"The two main simulation programs total about 5000 words each.\""
      },
      "releaseDate": {
        "value": "1960-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "InternLM",
    "fields": {
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "\"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens\" equals approximately 750B words for English, but the tokenizer's conversion ratio may be different for Chinese."
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE 4.0",
    "fields": {
      "releaseDate": {
        "value": "2023-10-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Word Representations",
    "fields": {
      "numTokens": {
        "value": "37000000",
        "source": "epoch",
        "citation": "Section 6: \"After cleaning, there are 37 million words (58%of the original) in 1.3 million sentences\""
      },
      "releaseDate": {
        "value": "2010-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer - LibriVox + Decoding/Rescoring",
    "fields": {
      "numParams": {
        "value": "296000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DALL-E",
    "fields": {
      "flops": {
        "value": "4.70E+22",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb"
      },
      "numParams": {
        "value": "12000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "250000000",
        "source": "epoch",
        "citation": "\"To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting250 million text-images pairs from the internet. \""
      },
      "costDollars": {
        "value": "171537.1317",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 16 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AltCLIP",
    "fields": {
      "releaseDate": {
        "value": "2022-11-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BASIC-L + Lion",
    "fields": {
      "numParams": {
        "value": "3070000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Engin-XL(NE)",
    "fields": {
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaStar",
    "fields": {
      "flops": {
        "value": "5.93E+22",
        "source": "epoch",
        "citation": "384 TPUv3 chips for 44 days. Assume 33% utilization.https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days"
      },
      "numParams": {
        "value": "139000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "512765.2699",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1056",
        "source": "epoch",
        "citation": "\"Each agent was trained using 32 third-generation tensorprocessing units (TPUs) over 44 days\""
      },
      "gpuCount": {
        "value": "384",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL + RMS dynamic eval",
    "fields": {
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Bidirectional RNN",
    "fields": {
      "numParams": {
        "value": "13000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "73920",
        "source": "epoch",
        "citation": "\"the training data set consisting of 3696 sentencesfrom 462 speakers\"Assuming avg sentence length of 20 words3696 * 20 total words"
      },
      "releaseDate": {
        "value": "1997-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenAI Five",
    "fields": {
      "flops": {
        "value": "6.70E+22",
        "source": "epoch",
        "citation": "\"770\u00b150 PFlops/s\u00b7days of compute\" for the model that played against world champions. They did a single training run that took 10 months.While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820\u00b150 PFlops/s\u00b7days.Finally, they also trained a Rerun model with 150\u00b15 PFlops/s\u00b7days of compute.Source: Dota 2 with Large Scale Deep Reinforcement Learninghttps://arxiv.org/abs/1912.06680You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: \" the number of GPUs (up to 1536 at the peak)\""
      },
      "numParams": {
        "value": "159000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "454321373184",
        "source": "epoch",
        "citation": "\"Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4thframe which we call a timestep\"--> 7.5 timesteps/s\"OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. \" --> 296 days296 * 24*3600 * 7.5 = 1.92e8This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?EDIT 14/06/2022Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11"
      },
      "costDollars": {
        "value": "166042.1145",
        "source": "epoch",
        "citation": "Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: \" the number of GPUs (up to 1536 at the peak)\""
      },
      "trainingTimeDays": {
        "value": "7104",
        "source": "epoch",
        "citation": "\"OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. \" --> 296 days"
      },
      "gpuCount": {
        "value": "1536",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeciCoder",
    "fields": {
      "flops": {
        "value": "2.90E+21",
        "source": "epoch",
        "citation": "446b * 1.1b * 6 = 2.9e21"
      },
      "numParams": {
        "value": "1100000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-1.3B",
    "fields": {
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer local-attention (NesT-B)",
    "fields": {
      "flops": {
        "value": "2.40576E+19",
        "source": "epoch",
        "citation": "17.9 GFLOPS per forward pass300 epochs1.28M training examples3.5 f_to_b pass ratio(From Imagenet paper-data, Besiroglu et al., forthcoming) "
      },
      "numParams": {
        "value": "90100000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "39.51323723",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Continuous speech recognition by statistical methods",
    "fields": {
      "numTokens": {
        "value": "12000",
        "source": "epoch",
        "citation": "800 sentences, counting 15 words per sentence gives 12000 words\"All the results given are for a training set of 800 sentences and a test set of 100 sentences\""
      },
      "releaseDate": {
        "value": "1976-04-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-2.7B (finetuned)",
    "fields": {
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Quantized ADMM",
    "fields": {
      "releaseDate": {
        "value": "2021-11-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Part-of-sentence tagging model",
    "fields": {
      "flops": {
        "value": "1.45411E+17",
        "source": "epoch",
        "citation": "12 hours of training for POS taggingGeForce GTX TITAN X GPU0.33 utilization rate"
      },
      "numTokens": {
        "value": "912344",
        "source": "epoch",
        "citation": "Table 2"
      },
      "costDollars": {
        "value": "0.9677873947",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "12",
        "source": "epoch",
        "citation": "\"the model training requires about 12 hours for POS tagging and 8hours for NER\""
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX TITAN X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SEST",
    "fields": {
      "numTokens": {
        "value": "4000000",
        "source": "epoch",
        "citation": "size of SQuAD"
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 1080",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-03-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EI-REHN-1000D",
    "fields": {
      "flops": {
        "value": "1.06E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "19000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wide Residual Network",
    "fields": {
      "releaseDate": {
        "value": "2016-09-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fraternal dropout + AWD-LSTM 3-layer (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2017-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Compress-LSTM (66M)",
    "fields": {
      "flops": {
        "value": "3.31E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "66000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-02-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2016-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Amended-DARTS",
    "fields": {
      "numParams": {
        "value": "23000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenCALM",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multipop Adaptive Continuous Stack (WT2)",
    "fields": {
      "numParams": {
        "value": "26000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sparse coding model for V1 receptive fields",
    "fields": {
      "numTokens": {
        "value": "10",
        "source": "epoch",
        "citation": "In Simulation Methods: \"The data for training were taken from ten 512 \u00d7 512pixel images of natural surroundings\""
      },
      "releaseDate": {
        "value": "1997-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Linear Transformer (large)",
    "fields": {
      "flops": {
        "value": "3.89E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "90000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-02-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "T2R + Random Init",
    "fields": {
      "flops": {
        "value": "6.1E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "450000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Segatron XL large, M=384",
    "fields": {
      "flops": {
        "value": "2.65E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-04-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DLRM-2022",
    "fields": {
      "flops": {
        "value": "1.10E+21",
        "source": "epoch",
        "citation": "Figure 1https://arxiv.org/abs/2104.05158"
      },
      "numParams": {
        "value": "3000000000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "2394.066783",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB,NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RoboCat",
    "fields": {
      "numParams": {
        "value": "1180000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PLATO-XL",
    "fields": {
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NPLM",
    "fields": {
      "flops": {
        "value": "1.3039E+15",
        "source": "epoch",
        "citation": "\"For example, consider the following architecture used in the experiments on the AP (AssociatedPress) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the orderof the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm\"The first 800,000 words were used for training... reducing the vocabulary size to |V| = 16,383convergence of the stochastic gradient ascent procedure was obtained after around 10to 20 epochs for the Brown corpusNOTE: there are two corpuses. The one represented in this calculation is the Brown one, which got a better improvement over sota"
      },
      "numParams": {
        "value": "11904264",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000",
        "source": "epoch",
        "citation": "\"Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency \u2264 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.\""
      },
      "releaseDate": {
        "value": "2003-03-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Noisy Student (L2)",
    "fields": {
      "flops": {
        "value": "8.49347E+20",
        "source": "epoch",
        "citation": "\"Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size\"2048*4.00E+12*60**2*24*4*0.3 = 8.5e20"
      },
      "numParams": {
        "value": "480000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "81000000",
        "source": "epoch",
        "citation": "\"Due to duplications, there are only 81M unique images among these 130M images.\""
      },
      "trainingTimeDays": {
        "value": "144",
        "source": "epoch",
        "citation": "6 days"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1-670M (UR100)",
    "fields": {
      "flops": {
        "value": "1.4E+20",
        "source": "epoch",
        "citation": "Information: 128 NVIDIA V100 GPUs [Pre-training details]275k steps [See Table S2: Hyperparameters]131,072 tokens per batch [\"We trained with 131,072 tokens per batch (128 gpus x 1024 tokens).\" - Pre-training details]Estimate:  275e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 1.4e20 FLOP"
      },
      "numParams": {
        "value": "669200000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PaLM-E",
    "fields": {
      "numParams": {
        "value": "562000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Whisper v2",
    "fields": {
      "flops": {
        "value": "1.10E+23",
        "source": "epoch",
        "citation": "\"Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.\"We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23"
      },
      "numParams": {
        "value": "1550000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9302400000",
        "source": "epoch",
        "citation": "\"When scaled to 680,000 hours of multilingual and multitasksupervision, the resulting models generalize wellto standard benchmarks and are often competitivewith prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.\"13,680 words/h (estimate) * 680,000h = 9,302,400,000 words"
      },
      "releaseDate": {
        "value": "2022-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gen-2",
    "fields": {
      "releaseDate": {
        "value": "2023-03-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ONLSTM-SYD",
    "fields": {
      "flops": {
        "value": "138999999999999980",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "25000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CoRe",
    "fields": {
      "numParams": {
        "value": "12400000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Amazon Titan",
    "fields": {
      "releaseDate": {
        "value": "2023-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EfficientNet-L2",
    "fields": {
      "numParams": {
        "value": "480000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Vespa",
    "fields": {
      "numParams": {
        "value": "231000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "A Bayesian Approach to Unsupervised One-Shot Learning of Object Categories",
    "fields": {
      "numParams": {
        "value": "100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2003-10-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProtT5-XXL",
    "fields": {
      "flops": {
        "value": "7.37E+22",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb"
      },
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "393000000000",
        "source": "epoch",
        "citation": "\"Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids.\""
      },
      "costDollars": {
        "value": "123918.3628",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepSeek LLM 67B",
    "fields": {
      "flops": {
        "value": "8.04E+23",
        "source": "epoch",
        "citation": "67B * 2T * 6 = 8.04e23"
      },
      "numParams": {
        "value": "67000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1750000000000",
        "source": "epoch",
        "citation": "\"We collect 2 trillion tokens for pre-training, primarily in Chinese and English\"if it's half English and half Chinese, that's750B English words + 1T Chinese words = 1.75T wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.1m0mpkhs9ljx"
      },
      "releaseDate": {
        "value": "2024-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2017-08-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Denoising Autoencoders",
    "fields": {
      "releaseDate": {
        "value": "2008-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Elastic weight consolidation",
    "fields": {
      "releaseDate": {
        "value": "2016-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LMS",
    "fields": {
      "releaseDate": {
        "value": "1960-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlexaTM 20B",
    "fields": {
      "flops": {
        "value": "2.04E+23",
        "source": "epoch",
        "citation": "Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.\"We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.\"Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper."
      },
      "numParams": {
        "value": "19750000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "2000000",
        "source": "epoch",
        "citation": "\"We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens\""
      },
      "trainingTimeDays": {
        "value": "2880",
        "source": "epoch",
        "citation": "See p.5 of the paper: \"We trained AlexaTM 20B for 120 days on 128 A100 GPUs...\""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.4935",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-08-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CLIP ViT-H/14 - LAION-2B",
    "fields": {
      "numParams": {
        "value": "986000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2000000000",
        "source": "epoch",
        "citation": "2B size of  LAION-2Binput is image text pair\"A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).\""
      },
      "releaseDate": {
        "value": "2022-09-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TensorReasoner",
    "fields": {
      "releaseDate": {
        "value": "2013-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Internal functionality of visual invariants",
    "fields": {
      "releaseDate": {
        "value": "1979-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "R-Transformer",
    "fields": {
      "flops": {
        "value": "8.4E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "15800000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LingoWhale-8B",
    "fields": {
      "numParams": {
        "value": "8000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pointer Sentinel-LSTM (medium)",
    "fields": {
      "flops": {
        "value": "7.49E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "21000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeGeeX",
    "fields": {
      "flops": {
        "value": "6.63E+22",
        "source": "epoch",
        "citation": "Assume 1 epoch on 850B tokens.C=6DN=6*850B*13Bhttps://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "637500000000",
        "source": "epoch",
        "citation": "As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens"
      },
      "batchSize": {
        "value": "6291456",
        "source": "epoch",
        "citation": "Table 3. 2048 * 3072"
      },
      "trainingTimeDays": {
        "value": "156.1",
        "source": "epoch",
        "citation": "Assume 30% utilization on 1536 Ascend 910 calculating in FP16.https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion+FLOP+%2F+%280.30*1536*256+TFLOPS%29If they used INT8 precision, the training time would be half of this."
      },
      "gpuType": {
        "value": "Huawei Ascend 910",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CryptoGRU",
    "fields": {
      "releaseDate": {
        "value": "2020-10-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE 3.0",
    "fields": {
      "flops": {
        "value": "2.25E+22",
        "source": "epoch",
        "citation": "Section 3.3.3: \"\"The model is trained fora total of 375 billion tokens\"Total compute approximated as 6*N*D"
      },
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "668000000000",
        "source": "epoch",
        "citation": "\"To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.\"1 GB ~ 167M chinese words"
      },
      "gpuCount": {
        "value": "384",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GNMT",
    "fields": {
      "flops": {
        "value": "6.90E+21",
        "source": "epoch",
        "citation": "sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.96 K80 GPU\u2019s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  = 6.9e6 PF = 79 pfs-dayssource: https://openai.com/blog/ai-and-compute/"
      },
      "numParams": {
        "value": "278000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "360000000",
        "source": "epoch",
        "citation": "[WORDS]\" On WMT En\u2192Fr, the training set contains 36M sentence pairs. On WMT En\u2192De, the training set contains 5M sentence pairs.\"36M sentences * 10 words/sentence"
      },
      "costDollars": {
        "value": "307573.499",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "4320",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "96",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AmoebaNet-A (F=448)",
    "fields": {
      "flops": {
        "value": "3.85297E+20",
        "source": "epoch",
        "citation": "450 K40 GPUs for 20k models (approx. 7 days).(From Imagenet paper-data, Besiroglu et al., forthcoming) "
      },
      "numParams": {
        "value": "469000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "5858.754384",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "\"Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).\""
      },
      "gpuCount": {
        "value": "450",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K40s",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SearchFusion",
    "fields": {
      "releaseDate": {
        "value": "2013-04-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-65B (LoRA finetuned)",
    "fields": {
      "numParams": {
        "value": "65200000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DITTO",
    "fields": {
      "flops": {
        "value": "1.1E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "750000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CaLM",
    "fields": {
      "flops": {
        "value": "2.9E+19",
        "source": "epoch",
        "citation": "\"4 NVIDIA Quadro RTX4000 GPUs for 40 days\"Calculation assuming FP32, utilization 30%:= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU"
      },
      "numParams": {
        "value": "86000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9000000",
        "source": "epoch",
        "citation": "\"a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing\""
      },
      "trainingTimeDays": {
        "value": "960",
        "source": "epoch",
        "citation": "\"The model reported in this work was trained on 4 NVIDIA QuadroRTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)\""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Quadro RTX 4000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StarCoder 2 7B",
    "fields": {
      "flops": {
        "value": "1.55E+23",
        "source": "epoch",
        "citation": "estimation is given in Table 6"
      },
      "numParams": {
        "value": "15000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3500000000000",
        "source": "epoch",
        "citation": "from Table 7, 3.5T tokens "
      },
      "gpuType": {
        "value": "NVIDIA H100 SXM5",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-02-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProteinBERT",
    "fields": {
      "flops": {
        "value": "6.5E+19",
        "source": "epoch",
        "citation": "\"Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28\u2009days over \u223c670M records\"28 * 24 * 3600 * 89 tFLOP/s * 0.3 (assumed utilization) = 6.5e19"
      },
      "numParams": {
        "value": "16000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "672",
        "source": "epoch",
        "citation": "28 days"
      },
      "gpuType": {
        "value": "NVIDIA Quadro RTX 5000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-02-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaFold",
    "fields": {
      "flops": {
        "value": "1E+20",
        "source": "epoch",
        "citation": "Estimated in the blogpost belowhttps://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening\"AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.\""
      },
      "numParams": {
        "value": "16340840",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "241.5931337",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "\"Training time: about 5 days for 600,000 steps\""
      },
      "releaseDate": {
        "value": "2020-01-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Platypus-70B",
    "fields": {
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RT-2",
    "fields": {
      "numParams": {
        "value": "55000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KwaiYiiMath",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "JFT",
    "fields": {
      "flops": {
        "value": "8.43E+20",
        "source": "epoch",
        "citation": "Tesla K80 performance: 8.13 TFLOP/sAssume 40% utilization60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP"
      },
      "numTokens": {
        "value": "300000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "21396.41732",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1440",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "50",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-07-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RWKV-4 World (7B)",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RBM-tuning",
    "fields": {
      "releaseDate": {
        "value": "2010-08-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM + dynamic eval (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2017-09-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GroupLens",
    "fields": {
      "numTokens": {
        "value": "100000000",
        "source": "epoch",
        "citation": "For each pair of users, the system computes the correlation between their scores in the articles they have rated.Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users "
      },
      "releaseDate": {
        "value": "1994-10-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN (SGD+CLR) (PTB)",
    "fields": {
      "numParams": {
        "value": "2050000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-12-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CogVLM",
    "fields": {
      "flops": {
        "value": "1.99E+22",
        "source": "epoch",
        "citation": "from table 8 on page 17230.1 FLOPS*days so 10**15*24*3600*230.1= 1.988e22"
      },
      "numParams": {
        "value": "17000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Generative BST",
    "fields": {
      "numParams": {
        "value": "9400000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TAPE Transformer",
    "fields": {
      "flops": {
        "value": "3E+19",
        "source": "epoch",
        "citation": "\"All self-supervised models are trained on four NVIDIA V100 GPUs for one week\"(7 * 24 * 3600) s * 4 GPUs * 3.1e13 FLOP/s * 0.4 (utilization assumption) = 3e19"
      },
      "numParams": {
        "value": "38000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stacked Denoising Autoencoders",
    "fields": {
      "releaseDate": {
        "value": "2010-01-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-1.3B (finetuned on PTB)",
    "fields": {
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-6.9b",
    "fields": {
      "numParams": {
        "value": "6900000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Neuro-Symbolic Concept Learner",
    "fields": {
      "releaseDate": {
        "value": "2019-04-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Super-vector coding",
    "fields": {
      "numTokens": {
        "value": "24706",
        "source": "epoch",
        "citation": "9,963 + 14,743\"PASCAL VOC 2007 consists of 9,963 images which are divided intothree subsets: training data (2501 images), validation data (2510 images), andtest data (4952 images). PASCAL VOC 2009 consists of 14,743 images and corre-spondingly are divided into three subsets: training data(3473 images), validationdata(3581 images), and testing data (7689 images).\""
      },
      "releaseDate": {
        "value": "2010-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BPE",
    "fields": {
      "numTokens": {
        "value": "37500000",
        "source": "epoch",
        "citation": "[WORDS]\"We perform experiments on data from the shared translation task of WMT 2015. For English\u2192German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English\u2192Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens\"100M tokens, around half will be in English, 0.75 words per token"
      },
      "releaseDate": {
        "value": "2015-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Flan UL2",
    "fields": {
      "numParams": {
        "value": "19500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-3 175B (davinci)",
    "fields": {
      "flops": {
        "value": "3.14E+23",
        "source": "epoch",
        "citation": "Table D.1https://arxiv.org/abs/2005.14165"
      },
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "374000000000",
        "source": "epoch",
        "citation": "From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. We multiply this by 0.75 to give 374B words. 3.74e11========================[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]\"The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. \"Converted to words using http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html2.85e11"
      },
      "batchSize": {
        "value": "3200000",
        "source": "epoch",
        "citation": "3.2M, per table 2.1"
      },
      "costDollars": {
        "value": "1131415.124",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "355.2",
        "source": "epoch",
        "citation": "14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf"
      },
      "gpuCount": {
        "value": "10000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.2196",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Baichuan2-53B",
    "fields": {
      "numParams": {
        "value": "53000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Zidong Taichu",
    "fields": {
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CT-MoS (WT2)",
    "fields": {
      "flops": {
        "value": "5.62E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "45000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Inflated 3D ConvNet",
    "fields": {
      "releaseDate": {
        "value": "2017-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Zoneout + Variational LSTM (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2016-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mistral Large",
    "fields": {
      "flops": {
        "value": "2.00E+25",
        "source": "epoch",
        "citation": "https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48https://x.com/EMostaque/status/1762152740938031484?s=20\"assuming this is on H100s with @Scaleway who are \u20ac1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:\" -Emad MostaqueAssuming bf16 or fp16, H100 PCIe performance is 1513 TFLOPSAt 1.9 euro per H100-hour and 33% utilization, spending 20M euro produces 1.9*10^25 FLOP.https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+3958+TFLOPS+*+0.33https://www.scaleway.com/en/h100-pcie-try-it-now/"
      },
      "costDollars": {
        "value": "18500000",
        "source": "epoch",
        "citation": "In February 2024, 20M EUR = 22M USDConverting to 2020 USD, this is 18.5Mhttps://www.in2013dollars.com/us/inflation/2024?endYear=2020&amount=22000000"
      },
      "trainingTimeDays": {
        "value": "2500",
        "source": "epoch",
        "citation": "Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s."
      },
      "gpuType": {
        "value": "NVIDIA H100 PCIe",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-02-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MSRA (C, PReLU)",
    "fields": {
      "flops": {
        "value": "2.3974E+19",
        "source": "epoch",
        "citation": "\"training C on eight K40 GPUs, takes about 3-4 weeks\"0.33 util rate(From Imagenet paper-data, Besiroglu et al., forthcoming) "
      },
      "numParams": {
        "value": "87048800",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": "\"We perform the experiments on the 1000-class ImageNet 2012 dataset\", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k"
      },
      "costDollars": {
        "value": "2166.218105",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "588",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-02-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2)",
    "fields": {
      "flops": {
        "value": "6.93E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "185000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-08-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN + char4-MS-vec",
    "fields": {
      "numParams": {
        "value": "226000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepSeekMoE-16B",
    "fields": {
      "flops": {
        "value": "3.40E+22",
        "source": "epoch",
        "citation": "\"With the DeepSeekMoE architecture, we scale up our MoE model to a larger scale with 16B totalparameters and train it on 2T tokens\"\"Evaluation results reveal that with only about 40% of computations, DeepSeekMoE 16B achieves comparable performancewith DeepSeek 7B (DeepSeek-AI, 2024), a dense model trained on the same 2T corpus\"40% * 7B = 2.8B, so 2.8B effective parameters2.8B * 2T * 6 ~= 3.4e22"
      },
      "numParams": {
        "value": "16000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1800000000000",
        "source": "epoch",
        "citation": "\"Leveraging our architecture, we subsequently scale up the model parameters to 16B andtrain DeepSeekMoE 16B on a large-scale corpus with 2T tokens.\"Probably a mix of English and Chinese. 1T English tokens is 0.75T words; 1T Chinese tokens is 1T words, so ~1.8T total"
      },
      "gpuType": {
        "value": "NVIDIA A100,NVIDIA H800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Luminous-supreme",
    "fields": {
      "flops": {
        "value": "2.80E+23",
        "source": "epoch",
        "citation": "\"~839000h\" GPU-hours on A100s, per Environmental Impact section of model card.312 trillion * 839000 * 3600 * 0.3 = 2.8e23"
      },
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-08-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "T0-XXL",
    "fields": {
      "flops": {
        "value": "1.79E+22",
        "source": "epoch",
        "citation": "From section B.1: \"These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.\" (512 cores for 270 hours)"
      },
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "270",
        "source": "epoch",
        "citation": "\"These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AdvSoft + 4 layer QRNN + dynamic evaluation",
    "fields": {
      "releaseDate": {
        "value": "2019-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (1.5B)",
    "fields": {
      "flops": {
        "value": "4.30E+21",
        "source": "epoch",
        "citation": "We use COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT* N EPOCHS * N TOKENS IN TRAINING DATASETThe number of epochs is not reported, but this other paper [1] claims in table 1 that it is 20 or 100 epochs. 100 epochs is consistent with the original GPT paper. 40GB dataset is 8B words, or 1/0.75 * 8B = 10.66B tokens.6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e216 * (40 * 200 million * 1/0.75 * 100) * 1.5 billion parameters = 9.6e21Geometric mean is 4.29e21[1] https://arxiv.org/abs/1906.06669"
      },
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000000",
        "source": "epoch",
        "citation": "\u201cAll results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.\u201d40GB is approximately 8e9 words."
      },
      "costDollars": {
        "value": "4692.886892",
        "source": "epoch",
        "citation": "https://en.wikipedia.org/wiki/GPT-2#:~:text=The%20cloud%20compute%20costs%20for,full%201.5%20billion%20parameter%20model)."
      },
      "releaseDate": {
        "value": "2019-02-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ConvNet similarity metric",
    "fields": {
      "numTokens": {
        "value": "140000",
        "source": "epoch",
        "citation": "The actual training set that was used contained140,000 image pairs that were evenly split between genuineand impostor."
      },
      "releaseDate": {
        "value": "2005-06-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TaLK Convolution",
    "fields": {
      "flops": {
        "value": "2.78E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "240000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPipe (Transformer)",
    "fields": {
      "numParams": {
        "value": "6000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "20000000000",
        "source": "epoch",
        "citation": "[WORDS]Section 5: \"We use acorpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language\"10^9 sentences * 20 words per sentence"
      },
      "releaseDate": {
        "value": "2018-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Word2Vec (small)",
    "fields": {
      "numParams": {
        "value": "207600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "692000",
        "source": "epoch",
        "citation": "\"For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K\""
      },
      "releaseDate": {
        "value": "2013-10-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Yuan 2.0",
    "fields": {
      "numParams": {
        "value": "102600000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TrOCR",
    "fields": {
      "numParams": {
        "value": "558000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "703300000",
        "source": "epoch",
        "citation": "The input data to the model are images.684M + 3.3M + 16Mfrom Experiment section: \"In total, the first-stage pre-training dataset contains 684M textlines.\" \"In total, the printed dataset consists of 3.3M textlines.\"and from MJSynth, SynthText datasets there is \"about 16M text images.\""
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Restricted Bolzmann machines",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "The training data set consists of 100,480,507ratings"
      },
      "releaseDate": {
        "value": "2007-06-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Megatron-Turing NLG 530B",
    "fields": {
      "flops": {
        "value": "1.17E+24",
        "source": "epoch",
        "citation": "https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx"
      },
      "numParams": {
        "value": "530000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "202500000000",
        "source": "epoch",
        "citation": "\"Our training dataset consists of 339 billion tokens and wetrained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.\"1 token ~ 0.75 words"
      },
      "batchSize": {
        "value": "3932160",
        "source": "epoch",
        "citation": "\"The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e \u22125 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920\" Final batch size is 1920 * 2048 = 3932160"
      },
      "costDollars": {
        "value": "3046994.087",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "770",
        "source": "epoch",
        "citation": "Total compute was 1.17*10^24 FLOP.They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours."
      },
      "gpuCount": {
        "value": "4480",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.302",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Conditional probability machines",
    "fields": {
      "releaseDate": {
        "value": "1956-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Textual Imager",
    "fields": {
      "releaseDate": {
        "value": "2013-01-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mamba-24M (SC09)",
    "fields": {
      "numParams": {
        "value": "23400000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MLP as Bayesian Approximator",
    "fields": {
      "releaseDate": {
        "value": "1990-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-160m",
    "fields": {
      "numParams": {
        "value": "160000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adaptive Broom Balancer",
    "fields": {
      "numParams": {
        "value": "110",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1988-07-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL DeFINE (107M)",
    "fields": {
      "releaseDate": {
        "value": "2019-11-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Yi-34B",
    "fields": {
      "flops": {
        "value": "6.10E+23",
        "source": "epoch",
        "citation": "\"The dataset we use contains Chinese & English only. We used approximately 3T tokens\" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?If so, 34b * 3T * 6 = 6.1e23"
      },
      "numParams": {
        "value": "34000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PermuteFormer",
    "fields": {
      "flops": {
        "value": "3.1E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "33000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BigGAN-deep 512x512",
    "fields": {
      "flops": {
        "value": "3.00E+21",
        "source": "epoch",
        "citation": "3e21, estimate taken from:https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening"
      },
      "numParams": {
        "value": "112694781",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "292000000",
        "source": "epoch",
        "citation": "\"To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images \u2013 two orders of magnitude larger than ImageNet. \""
      },
      "costDollars": {
        "value": "10448.43787",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "48",
        "source": "epoch",
        "citation": "\"We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128\u00d7128, 256 for 256\u00d7256, and 512 for 512\u00d7512. Training takes between 24 and 48 hours for most models\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Population-based DRL",
    "fields": {
      "flops": {
        "value": "3.49E+19",
        "source": "epoch",
        "citation": "Source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389"
      },
      "numParams": {
        "value": "122000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "130.3626995",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-07-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ALiBi (L=3072, Lvalid = 3072)",
    "fields": {
      "flops": {
        "value": "8.1E+20",
        "source": "epoch",
        "citation": "From figure 5, 6000 GPU hours (Nvidia V100) 6000*  125 teraflop/s * 3600 * 0.3 = 8.1e20"
      },
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pangu 3.0",
    "fields": {
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama 2-7B",
    "fields": {
      "flops": {
        "value": "8.40E+22",
        "source": "epoch",
        "citation": "Trained on 2 trillion tokens per Table 1. C = 6ND = 6*7B*2T = 8.4e+22 FLOP.Also, 7B model was trained on 184320 GPU-hours312 trillion * 184320 * 3600 * 0.3 = 6.21e22"
      },
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1500000000000",
        "source": "epoch",
        "citation": "2 trillion tokens ~= 1.5 trillion words"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (345M)",
    "fields": {
      "numTokens": {
        "value": "3000000000",
        "source": "epoch",
        "citation": "\u201cAll results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.\u201d40GB is approximately 3e9 words."
      },
      "releaseDate": {
        "value": "2019-02-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Whisper v3",
    "fields": {
      "flops": {
        "value": "2.70E+23",
        "source": "epoch",
        "citation": "Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23"
      },
      "numParams": {
        "value": "1550000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000000000",
        "source": "epoch",
        "citation": "English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ceThe dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have200*60*5 million hours = 60,000,000,000 (60B) words"
      },
      "releaseDate": {
        "value": "2023-11-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Neocognitron",
    "fields": {
      "flops": {
        "value": "228115200",
        "source": "epoch",
        "citation": "\"It does not necessarily mean that all of these input synapses arealways fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.\"We multiply by 2 to account for multadds"
      },
      "numParams": {
        "value": "1140576",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "5",
        "source": "epoch",
        "citation": "\"In order to self-organize the network, we have presented five stimulus patterns \"0\", \"1\", \"2\", \"3\", and \"4\", which are shown in Fig. 6\""
      },
      "releaseDate": {
        "value": "1980-04-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaVA 1.5",
    "fields": {
      "flops": {
        "value": "6.5E+19",
        "source": "epoch",
        "citation": "6.5e19 finetuning compute. fine-tuned from Vicuna-13B which is fine-tuned Llama-13B, which was 4.55e22 FLOP"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "from abstract \"Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. \""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EVA",
    "fields": {
      "flops": {
        "value": "3.75E+21",
        "source": "epoch",
        "citation": "flops = (128) * (77.97 * 10**12) * (14.5 * 24 * 3600) * (0.3) = 3.75e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16"
      },
      "numParams": {
        "value": "1011000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "29600000",
        "source": "epoch",
        "citation": "from table 3: 29.6M images"
      },
      "trainingTimeDays": {
        "value": "348",
        "source": "epoch",
        "citation": "from Table 3 14.5 days = 348 hours"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XLNet",
    "fields": {
      "numParams": {
        "value": "360268800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LTM-1",
    "fields": {
      "releaseDate": {
        "value": "2023-06-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BASIC-L",
    "fields": {
      "flops": {
        "value": "4.12E+22",
        "source": "epoch",
        "citation": "6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8Two cores per chip, and 275 teraflop/s per chip (https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22"
      },
      "numParams": {
        "value": "3070000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "6700000000",
        "source": "epoch",
        "citation": "6.7B image-text pairs"
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Immediate trihead",
    "fields": {
      "releaseDate": {
        "value": "2001-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TCN (148M)",
    "fields": {
      "numParams": {
        "value": "148000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MemoReader",
    "fields": {
      "gpuType": {
        "value": "NVIDIA M40",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "M6-10T",
    "fields": {
      "flops": {
        "value": "5.53E+21",
        "source": "epoch",
        "citation": "512 GPUs in 10 days - using NVIDIA V100 GPUsUsing the NVIDIA V100 Specifications this works out to be: 0.30 * 125E12 * 512 * 10 * 86400 = 1.66E22(Assuming 30% utilisation, and 125 TFLOPS)"
      },
      "numParams": {
        "value": "10000000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "8000000000",
        "source": "epoch",
        "citation": "\"We conduct experiments for pretraining and finetuning to analyze model competence in upstream anddownstream tasks. Following the classical data setup for pretraining and finetuning, we pretrain the model on BookCorpus [52] and English Wikipedia [9], which are corpora with around 16GB of plaintexts.\"I used http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html for the conversion to number of words"
      },
      "costDollars": {
        "value": "20073.49413",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Falcon-7B",
    "fields": {
      "flops": {
        "value": "6.30E+22",
        "source": "epoch",
        "citation": "6ND = 6 * 7B * 1.5T = 6.3e22\"Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.\""
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1125000000000",
        "source": "epoch",
        "citation": "1125000000000.0 words  assuming 0.75 words per token (1.5T tokens)\"Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.\""
      },
      "gpuCount": {
        "value": "384",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CTR-BERT",
    "fields": {
      "flops": {
        "value": "6.46963E+19",
        "source": "epoch",
        "citation": "flops = (8) * (312 * 10**12) * (24 * 3600) * (0.3)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)'CTR-BERTuses about 70 million parameters which can be trained on 8 A100 GPUs in less than a day (<1000USD).'"
      },
      "numParams": {
        "value": "70000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gemini 1 Pro",
    "fields": {
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM",
    "fields": {
      "numParams": {
        "value": "24000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gemini Nano-1",
    "fields": {
      "numParams": {
        "value": "1800000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v5e",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DMN",
    "fields": {
      "releaseDate": {
        "value": "2016-06-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao Aquila2 34B",
    "fields": {
      "numParams": {
        "value": "34000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MuZero",
    "fields": {
      "flops": {
        "value": "4.8E+19",
        "source": "epoch",
        "citation": "third-generation Google Cloud TPU(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-playTraining for 12 hours (for Atari)Data from Parameter, Compute and Data Trends in Machine LearningGoogle v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)Utilization rate In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%Calculations for Atari:12 hours \u2192 43200 seconds(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOPTraining time missing for boardgamesAssumption also 12 hours Also: 2.4017472 * 10^19 FLOPTotal cost \u2248 4.8 * 10^19 FLOP"
      },
      "numParams": {
        "value": "36864000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "20000000000",
        "source": "epoch",
        "citation": "Table 1https://arxiv.org/pdf/1911.08265.pdf"
      },
      "costDollars": {
        "value": "121.1790382",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1-43M",
    "fields": {
      "flops": {
        "value": "2.8E+19",
        "source": "epoch",
        "citation": "Information: 128 NVIDIA V100 GPUs [Pre-training details]840k steps [See Table S2: Hyperparameters]131,072 tokens per batch [\"We trained with 131,072 tokens per batch (128 gpus x 1024 tokens).\" - Pre-training details]Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 42.6e6 parameters = 2.8e19 FLOP"
      },
      "numParams": {
        "value": "42600000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CapsNet (MNIST)",
    "fields": {
      "numParams": {
        "value": "8200000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "Section 5: The dataset has 60K and 10K images for training and testing respectively."
      },
      "releaseDate": {
        "value": "2017-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Heuristic problem solving for AI",
    "fields": {
      "releaseDate": {
        "value": "1961-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "W2v-BERT",
    "fields": {
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen-Audio-Chat",
    "fields": {
      "numParams": {
        "value": "8460000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KnGPT2",
    "fields": {
      "flops": {
        "value": "1.24E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "83000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-FWM (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNMT+",
    "fields": {
      "flops": {
        "value": "3.95062E+19",
        "source": "epoch",
        "citation": "32 * 9.526 TFLOPS * (120 * 3600) * 0.3 = 39506227200000000000(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)\"All models were trained with synchronoustraining. RNMT+ and ConvS2S were trained with32 NVIDIA P100 GPUs \"performence of P100 is 9.526 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-16-gb.c2888training time is 120h from Table 1"
      },
      "numParams": {
        "value": "378900000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "544500000",
        "source": "epoch",
        "citation": "\"We train our models on the standard WMT\u201914 En\u2192Fr and En\u2192De datasets that comprise 36.3M\"We estimate 15 english words in one sentence."
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "from Table 1"
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA P100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-04-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama 3",
    "fields": {
      "gpuCount": {
        "value": "24576",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA H100 SXM5",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Social and content-based classification",
    "fields": {
      "numTokens": {
        "value": "45000",
        "source": "epoch",
        "citation": "\"Our data set consists of more than 45,000 movie rat-ings collected from approximately 260 users.\""
      },
      "releaseDate": {
        "value": "1998-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DNA Fine-Tuned Language Model (DFLM)",
    "fields": {
      "flops": {
        "value": "3.5E+18",
        "source": "epoch",
        "citation": "\"The pre-training of Human genome language model isresource-intensive (about 7-10 days on 2 NVIDIA TITAN X GPU).\"Assuming FP32 and 30% utilizationEstimate: (10*24*3600) s * 6.7e12 FLOP/s * 2 * 0.3 = 3.5e18"
      },
      "gpuType": {
        "value": "NVIDIA TITAN Xp",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EGRU (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2022-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LRSO-GAN",
    "fields": {
      "releaseDate": {
        "value": "2017-10-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "3-Layer-Tensor-Transformer+AdaHessian",
    "fields": {
      "releaseDate": {
        "value": "2020-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DMPFold",
    "fields": {
      "flops": {
        "value": "12000000000000",
        "source": "epoch",
        "citation": "\"Training of all models was performed using the Adam optimiser for 75epochs with default parameters\"\"The training set here was based on the same 6729 protein chains, \u2264500 residues in length, with non-redundancy at the 25% sequence identity level and no unresolved main chain atoms\"Estimate = 2 * 3.8e6 * 3 * 6729 * 75 ~ 1.2e13 FLOP"
      },
      "numParams": {
        "value": "3800000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "6729",
        "source": "epoch",
        "citation": "\"The training set here was based on the same 6729 protein chains\""
      },
      "releaseDate": {
        "value": "2018-11-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2+Active-SGD",
    "fields": {
      "flops": {
        "value": "3.1E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "124000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cohere Command Light",
    "fields": {
      "flops": {
        "value": "1.00E+22",
        "source": "epoch",
        "citation": "https://docs.cohere.com/docs/environmental-impact2700kg CO2 equivalent. Cohere used this calculator: https://mlco2.github.io/impact/ This calculator claims that ~40000 TPUv3 hours causes ~3000 kg CO2 emissions in the \"us-west1\", \"us-west2\", and \"us-west3\" regions. Not clear what region the data center Cohere used was in. Google has data centers around the world; *most* regions are similarly carbon intensive as us-west but north-america-northeast is 10x less carbon intensive and south Asia is 5x more carbon intensive. So the calculation below could be quite off.Cohere most likely used TPUv4s, which the calculator does not support, which seem to be much more efficient (2.7x more, according to this https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)40000 hours * 123 teraflops * 3600 * 0.3 utilization * 2.7 = 1.4e22"
      },
      "numParams": {
        "value": "6000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PSPNet",
    "fields": {
      "releaseDate": {
        "value": "2017-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeLight",
    "fields": {
      "flops": {
        "value": "2.4E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "99000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BERT-Large-CAS (WT103)",
    "fields": {
      "releaseDate": {
        "value": "2019-04-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProxylessNAS",
    "fields": {
      "flops": {
        "value": "3.70656E+19",
        "source": "epoch",
        "citation": "For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.At FP32, a V100 GPU has a peak performance of 1.56E+14 FLOPS.Utilization rate of 0.33."
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "135.0398696",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-02-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ELECTRA",
    "fields": {
      "flops": {
        "value": "3.10E+21",
        "source": "epoch",
        "citation": "Table 8: \"ELECTRA-1.75M\" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps."
      },
      "numParams": {
        "value": "335000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-03-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-7B (protein-oriented instructions finetuned)",
    "fields": {
      "flops": {
        "value": "2.78E+22",
        "source": "epoch",
        "citation": "Estimate 1: 1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOPfrom paper, Llama-7B took 82,432 GPU hours using A100sEstimate 2: 312 trillion FLOP/s * (82,432 * 3600) s * 0.3 = 2.78e22 FLOP"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sparse Energy-Based Model",
    "fields": {
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2006-12-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-NeoX-Japanese",
    "fields": {
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM2-3B",
    "fields": {
      "flops": {
        "value": "3.00E+22",
        "source": "epoch",
        "citation": "from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 1.8e22 FLOPfrom the paper's Supplementary Materials: \"We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.\"30 days x 512 V100s x an imputed 30% utilization\": 5e22 FLOPGeometric mean: 3e22"
      },
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Emu (Meta)",
    "fields": {
      "numParams": {
        "value": "2800000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1100000000",
        "source": "epoch",
        "citation": "\"We curate a large internal pre-training dataset consisting of 1.1 billion images to train our model. The model is trained with progressively increasing resolutions\""
      },
      "releaseDate": {
        "value": "2023-09-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Perfusion",
    "fields": {
      "releaseDate": {
        "value": "2023-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1b",
    "fields": {
      "flops": {
        "value": "4.6E+20",
        "source": "epoch",
        "citation": "Information: 128 NVIDIA V100 GPUs [Pre-training details]8.5 hours on 64 GPUs per epoch, 56 epochs [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]128 NVIDIA V100 GPU, assuming  V100 PCIe single precision 14 TFLOPS and 0.3 utilization rateEstimate: (8.5*56*3600) s * 14e12 FLOP/s * 0.3 *64 = 4.6e20 FLOPs"
      },
      "numParams": {
        "value": "652400000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Markov-driven POS tagger",
    "fields": {
      "numParams": {
        "value": "2447124",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000",
        "source": "epoch",
        "citation": "\"We use the \"treebank\" data described in Beale (1988). It contains 42,186 sentences (about one million words) from the Associated Press.\"https://www.aclweb.org/anthology/J94-2001.pdf"
      },
      "releaseDate": {
        "value": "1994-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DARTS",
    "fields": {
      "flops": {
        "value": "1.1E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "33000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GRU + p-tHSM (pretrain via Brown) (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2017-08-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepLab",
    "fields": {
      "releaseDate": {
        "value": "2014-12-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MobileBERT",
    "fields": {
      "numParams": {
        "value": "25300000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-04-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LaNet-L (CIFAR-10)",
    "fields": {
      "numParams": {
        "value": "44100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN (SGD+CLR)",
    "fields": {
      "numParams": {
        "value": "195600",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-12-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-1.3B (finetuned)",
    "fields": {
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FrameNet role labeling",
    "fields": {
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": "Abstract: \"The system is based on statistical classifiers trained on roughly 50,000 sentences\""
      },
      "releaseDate": {
        "value": "2000-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StarGAN v2",
    "fields": {
      "releaseDate": {
        "value": "2019-12-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OR-WideResNet",
    "fields": {
      "numParams": {
        "value": "18200000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-01-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-33B (LoRA finetuned)",
    "fields": {
      "numParams": {
        "value": "33000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Aya",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-02-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adaptive Agent",
    "fields": {
      "flops": {
        "value": "2.80E+21",
        "source": "epoch",
        "citation": "\"AdA was implemented using JAX (Bradbury et al., 2018) and the DeepMind JAX Ecosystem (Babuschkin et al., 2020) and trained on 64 Google TPUv3 devices. The wall-clock time for training this version of AdA from scratch was approximately 5 weeks: 1 week to train the teacher, and 4 weeks to train AdA\"64 * 123 teraflop/s * 35 days * 24 * 3600 * 0.4 = 9.5e21This might be for all single-agent experiments in the paper, or just for the 76M model in Table D.1, I'm not sure.In Table E.2, the 533M-param model takes 2e20 FLOP to go through 5B learner steps, and was trained on 70B steps in total (Table 1). That would be 2.8e21 for 70B steps. That might be an underestimate because there are also teacher(?) steps."
      },
      "numParams": {
        "value": "533000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "840",
        "source": "epoch",
        "citation": "5 weeks. Possible that this is for multiple models"
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MetaMimic",
    "fields": {
      "numParams": {
        "value": "22000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gemma 7B",
    "fields": {
      "flops": {
        "value": "2.52E+23",
        "source": "epoch",
        "citation": "6ND aproximation 6*7B*6T = 2.5e23\"Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.\"we can also try to estimate from: \"We estimate the carbon emissions from pretrain-ing the Gemma models to be \u223c 131 \ud835\udc61\ud835\udc36\ud835\udc422\ud835\udc52\ud835\udc5e. \""
      },
      "numParams": {
        "value": "7751248896",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4500000000000",
        "source": "epoch",
        "citation": "assuming 0.75 words per token - so 4500000000000.0 words\"Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.\""
      },
      "gpuCount": {
        "value": "4096",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v5e",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-02-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LEP-AD",
    "fields": {
      "releaseDate": {
        "value": "2023-03-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stacked Semisuperviser Autoencoders",
    "fields": {
      "numParams": {
        "value": "3000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "66087",
        "source": "epoch",
        "citation": "\"The 20 Newsgroups dataset contains 18845postings taken from the Usenet newsgroup collection.Documents are partitioned into 20 topics. The datasetis split into 11314 training documents and 7531 testdocuments. Training and test articles are separated intime. Reuters has a predefined ModApte split of thedata into 11413 training documents and 4024 test doc-uments. Documents belong to one of 91 topics. TheOhsumed dataset has 34389 documents with 30689words and each document might be assigned to morethan one topic, for a total of 23 topics. The dataset issplit into training and test by randomly selecting the67% and the 33% of the data\"total # documents = 11314 + 11413 + 34389*0.6I'm using #documents here since the task is document representation. Using #words would increase the size by ~3 OOMs"
      },
      "releaseDate": {
        "value": "2008-07-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MusicLM",
    "fields": {
      "numParams": {
        "value": "860000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "2nd order FOFE-FNNLM",
    "fields": {
      "numParams": {
        "value": "6000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-05-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Monarch-GPT-2-Medium",
    "fields": {
      "flops": {
        "value": "4.36E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "165000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MobileNetV2",
    "fields": {
      "numParams": {
        "value": "3400000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-06-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multi-task Cascaded CNN",
    "fields": {
      "releaseDate": {
        "value": "2016-08-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RCAN",
    "fields": {
      "releaseDate": {
        "value": "2018-07-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo",
    "fields": {
      "flops": {
        "value": "7.90E+21",
        "source": "epoch",
        "citation": "source: https://www.aitracker.org/"
      },
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "885837004800",
        "source": "epoch",
        "citation": "\"In aggregate, the Pile consists of over 825GiB of raw text data\"(see GPT-NeoX)"
      },
      "costDollars": {
        "value": "13685.98918",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Big-Little Net (vision)",
    "fields": {
      "flops": {
        "value": "3.93677E+18",
        "source": "epoch",
        "citation": "number of epochs (appendix A1) times flops per inference (from table 2) times dataset size times 3 (to account for backpropagation)"
      },
      "numParams": {
        "value": "77360000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": "size of ImageNet"
      },
      "releaseDate": {
        "value": "2018-07-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProtGPT2",
    "fields": {
      "flops": {
        "value": "4.10E+21",
        "source": "epoch",
        "citation": "\"The model trained on 128 NVIDIA A100s in 4 days\"128 * 4 * 24 * 3600 * 312 trillion FLOP/s * 0.3 = 4.1e21"
      },
      "numParams": {
        "value": "738000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "96",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Subformer (122M)",
    "fields": {
      "flops": {
        "value": "5.3E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "122000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESRGAN",
    "fields": {
      "releaseDate": {
        "value": "2018-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNeXt-101 32x48d",
    "fields": {
      "flops": {
        "value": "8.74E+21",
        "source": "epoch",
        "citation": "Table 6: 153e9 mult-adds.Section 2.4: \"minibatches of 8,064 images\".Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP"
      },
      "numParams": {
        "value": "829000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9525000000",
        "source": "epoch",
        "citation": "Table 3: (300+1925+300+7000) million images"
      },
      "gpuCount": {
        "value": "336",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeGen2",
    "fields": {
      "numParams": {
        "value": "16000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM + Phrase Induction + finetuning",
    "fields": {
      "releaseDate": {
        "value": "2019-06-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Perceiver IO",
    "fields": {
      "numParams": {
        "value": "425000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Perceptron Mark I",
    "fields": {
      "flops": {
        "value": "694894.9377",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numParams": {
        "value": "1000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "6",
        "source": "epoch",
        "citation": "Appendix II describes an experiment with 6 stimulus patterns"
      },
      "releaseDate": {
        "value": "1957-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LDM-1.45B",
    "fields": {
      "numParams": {
        "value": "1450000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "400000000",
        "source": "epoch",
        "citation": "400M image-text pairs"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MPT-30B",
    "fields": {
      "flops": {
        "value": "1.80E+23",
        "source": "epoch",
        "citation": "30b * 1T tokens * 6 = 1.8e23"
      },
      "numParams": {
        "value": "30000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000000000",
        "source": "epoch",
        "citation": "~4T tokens across sources, or 3T words at 0.75 words/token (ignoring the fact that some of the data is code)"
      },
      "batchSize": {
        "value": "4096000",
        "source": "epoch",
        "citation": "last two batch sizes were 3,456,000 and 4,096,000, but 4,096,000 only used for last 5% of training\"To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long...The model was trained in three stages using the MosaicML Platform: (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens\""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB,NVIDIA H100 SXM5",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaMissense",
    "fields": {
      "releaseDate": {
        "value": "2023-09-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Error Propagation",
    "fields": {
      "releaseDate": {
        "value": "1986-01-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RoBERTa Large",
    "fields": {
      "flops": {
        "value": "4.15E+21",
        "source": "epoch",
        "citation": "Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: \"increasing the number of pretraining stepsfrom 100K to 300K, and then further to 500K\".So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision.C=5*1024*3.13E+13*60**2*24*0.3 = 4.2e21"
      },
      "numParams": {
        "value": "355000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "32000000000",
        "source": "epoch",
        "citation": "160GB*200M words/GB = 3.2e10 words"
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days."
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OverFeat",
    "fields": {
      "releaseDate": {
        "value": "2013-12-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CoEdiT-xxl",
    "fields": {
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000",
        "source": "epoch",
        "citation": "82k pairs of sentences. Roughly 20 words per sentence based on examples but mean length could be higher due to outliers.40*82k = ~3,000,000"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CogVideo",
    "fields": {
      "numParams": {
        "value": "9000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL+WN+AdamP",
    "fields": {
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeGen-Mono 16.1B",
    "fields": {
      "numParams": {
        "value": "16100000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "$\\infty$-former (SM)",
    "fields": {
      "flops": {
        "value": "1.20E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "117000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Grok-1",
    "fields": {
      "releaseDate": {
        "value": "2023-11-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Chinchilla",
    "fields": {
      "flops": {
        "value": "5.76E+23",
        "source": "epoch",
        "citation": "\"Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.\"We see the number of flops in table 3"
      },
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1050000000000",
        "source": "epoch",
        "citation": "Table 1 shows Chinchilla was training on 1.4 trillion tokens1 token ~ 0.75 words"
      },
      "batchSize": {
        "value": "3000000",
        "source": "epoch",
        "citation": "Table 1. \"1.5M \u2192 3M\""
      },
      "costDollars": {
        "value": "753491.5785",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4,Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ALIGN",
    "fields": {
      "flops": {
        "value": "2.60E+22",
        "source": "epoch",
        "citation": "From author communication14.82K TPUv3 core-daysPrecision: bfloat16EstimationTPUv3 at float16: 123 TFLOPS/chip123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOPhttps://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33"
      },
      "numParams": {
        "value": "820000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1600000000",
        "source": "epoch",
        "citation": "Dataset contains 1.8B image-text pairs, then some duplicates are removed."
      },
      "costDollars": {
        "value": "357760.3253",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "347.3",
        "source": "epoch",
        "citation": "14820 TPU core-hours / 1024 TPU cores = 347.3 hours"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stable Code 3B",
    "fields": {
      "flops": {
        "value": "2.11E+22",
        "source": "epoch",
        "citation": "6ND = 2.7e9 * 1.3e12 * 6 = 2,106E+22\"stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. \""
      },
      "numParams": {
        "value": "2796431360",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Perceptron for Large Margin Classification",
    "fields": {
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "\"The dataset consists of 60,000 training examples and 10,000 test examples.\""
      },
      "releaseDate": {
        "value": "1999-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ASE",
    "fields": {
      "flops": {
        "value": "6.10416E+18",
        "source": "epoch",
        "citation": "Training was done using the Isaac Gym simulator on an NVIDIA V100 GPU. The model was trained on over 10 billion samples, which equates to 10 years of simulated experience time. Training took around 10 days on a single GPU.14.13 TFLOP/s * 10 days * 86400 s/day * 0.50 utilization = 6.1e+18 FLOP"
      },
      "trainingTimeDays": {
        "value": "240",
        "source": "epoch",
        "citation": "Training took around 10 days on a single GPU."
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 PCIe 16 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "D-LSRC(200)+KN5",
    "fields": {
      "numParams": {
        "value": "7160000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Histograms of Oriented Gradients",
    "fields": {
      "numTokens": {
        "value": "1805",
        "source": "epoch",
        "citation": " we produced a new and significantly morechallenging data set, \u2018INRIA\u2019, containing 1805 64\u00d7128 im-ages"
      },
      "releaseDate": {
        "value": "2005-06-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1-670M (UR50/D)",
    "fields": {
      "flops": {
        "value": "4.8E+20",
        "source": "epoch",
        "citation": "Information: 128 NVIDIA V100 GPUs [Pre-training details]906k steps [See Table S2: Hyperparameters]131,072 tokens per batch [\"We trained with 131,072 tokens per batch (128 gpus x 1024 tokens).\" - Pre-training details]Estimate:  906e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 4.8e20 FLOP"
      },
      "numParams": {
        "value": "669200000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mistral 7B + OVM",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Variational (untied weights, MC) LSTM (Large)",
    "fields": {
      "flops": {
        "value": "5.62E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "66000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-12-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Learning past tenses",
    "fields": {
      "numParams": {
        "value": "211600",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1986-01-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Search-Proven Best LSTM",
    "fields": {
      "flops": {
        "value": "3.34E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "20000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AraGPT2-Mega",
    "fields": {
      "flops": {
        "value": "2.00E+21",
        "source": "epoch",
        "citation": "source: https://github.com/lightonai/akronomicon/blob/10adaca9c74afa7d11f196947e410d248f25abe9/akrodb/American%20University%20of%20Beirut/AraGPT2-Mega.jsonAkronomicon uses units of petaflop/s-days. 20 petaflop/s-days ~= 2e21 FLOP.Our own validation of this estimate is below.For the Mega model: 9 days on a TPUv3-128, bfloat16 precision  (from author communication)A TPUv3-128 has 128 cores (you can infer this from footnote 9 on p.4 of the paper - 128 * 16GB = 2TB). TPUv3 has 2 cores per chip. So 64 chips.TPUv3 FLOP/s: 1.23E+14Utilization: use default value of 30% for Language domain (https://epochai.org/blog/estimating-training-compute)64 chips * 30% * 1.23E+14 FLOP/s * 9 days * 24h/day * 3600s/h~= 2e21 FLOP"
      },
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "8800000000",
        "source": "epoch",
        "citation": "\"The total dataset size is 77GB with 8.8B words [word count was done after preprocessing, where a whitespace is inserted before and after punctuations, brackets, numbers... which increased the total word count]\""
      },
      "costDollars": {
        "value": "3685.432555",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPHINX (Llama 2 13B)",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "290",
        "source": "epoch",
        "citation": "\"The pre-training time is around 125 hours on 32 A100 GPUs with a 7Blanguage model and about twice the time with a 13B language model.\"\" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13Blanguage model.\""
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLaM",
    "fields": {
      "flops": {
        "value": "3.74E+23",
        "source": "epoch",
        "citation": "from paper: \"GLaM (64B/64E) training after 600B tokens consumes 456 MWh, about 1/3 of the energy cost of 1287 MWh used by GPT-3. Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we train using 1,024 TPU-v4 chips for 574 hours (with 280B tokens). This consumes 213 MWh or 1/6 of the GPT-3 energy cost\"600/280 is almost exactly 456/213 (2.14) so the later tokens have the same per-token energy cost. 2.14*574*1024 = 1,257,840 TPU-v4 hoursTPU-v4s are 275 teraFLOP/s. Using our usual 0.3 utilization assumption, 275 trillion * 1,257,840 * 3600 * 0.3 = 3.74e23Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization."
      },
      "numParams": {
        "value": "1200000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "800000000000",
        "source": "epoch",
        "citation": "The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words."
      },
      "batchSize": {
        "value": "1000000",
        "source": "epoch",
        "citation": "\"We use a maximum sequencelength of 1024 tokens, and pack each input example to haveup to 1 million tokens per batch.\""
      },
      "trainingTimeDays": {
        "value": "1366",
        "source": "epoch",
        "citation": "Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.\"326W measured system power per TPU-v4 chip\"\"The complete GLaM training using 600B tokens consumes only456 MWh\"1024 TPU v4 chips(456 MWh) / (326W/chip * 1024 chips) = 1366 hours"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-3-layer+Gadam",
    "fields": {
      "flops": {
        "value": "2.68E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "24000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-03-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Big Transfer (BiT-L)",
    "fields": {
      "numParams": {
        "value": "928000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SciBERT",
    "fields": {
      "flops": {
        "value": "8.92685E+19",
        "source": "epoch",
        "citation": "4*123e12*0.3*(7*24*3600) = 8.926848e+19(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)We have: 4 TPUv3 chips.123teraFLOPs per chip.7 days of training\"We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). \""
      },
      "numParams": {
        "value": "110000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2475000000",
        "source": "epoch",
        "citation": "assuming 0.75 words per token 3.3B*0.75 = 2475000000\"The average paper length is154 sentences (2,769 tokens) resulting in a corpussize of 3.17B tokens, similar to the 3.3B tokenson which BERT was trained.\""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "1 week"
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-03-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CoAtNet",
    "fields": {
      "flops": {
        "value": "4.27E+22",
        "source": "epoch",
        "citation": "20.1K TPU-v3 core-daysTPUs have two cores per chip, and a chip is 123 teraflop/s https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22"
      },
      "numParams": {
        "value": "2440000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaGo Fan",
    "fields": {
      "flops": {
        "value": "3.8E+20",
        "source": "epoch",
        "citation": "Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 weekTraining compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs"
      },
      "numParams": {
        "value": "8209984",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "3076.073703",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Megatron-LM (1T)",
    "fields": {
      "numParams": {
        "value": "1000000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ADP-FAIRSEQ",
    "fields": {
      "releaseDate": {
        "value": "2022-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CTM (CIFAR-10)",
    "fields": {
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BPL",
    "fields": {
      "releaseDate": {
        "value": "2015-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sparrow",
    "fields": {
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ALM 1.0",
    "fields": {
      "releaseDate": {
        "value": "2022-11-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VQGAN + CLIP",
    "fields": {
      "releaseDate": {
        "value": "2020-12-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Phrase-based translation",
    "fields": {
      "numParams": {
        "value": "9178890",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "20000000",
        "source": "epoch",
        "citation": "[WORDS]\"We used the freely available Europarl corpus to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.\"\"These results are consistentover training corpus sizes from 10,000 sentence pairs to320,000 sentence pairs. \"So 20 million words or 320k sentence pairs."
      },
      "releaseDate": {
        "value": "2003-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EquiDock",
    "fields": {
      "flops": {
        "value": "1.08E+19",
        "source": "epoch",
        "citation": "Training details here:https://docs.nvidia.com/bionemo-framework/latest/models/equidock.html32 A100s can do 30 epochs per hour on the DIPS dataset. Equidock was trained on 30 epochs on DIPS and 150 epochs on DB5.5. DIPS is about 100x bigger, so the large majority of compute was DIPS.32 A100-hours = 312 teraflops * 32 * 3600 * 0.3 ~= 1.08e19"
      },
      "releaseDate": {
        "value": "2021-11-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dou Bao",
    "fields": {
      "releaseDate": {
        "value": "2023-08-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fisher Kernel GMM",
    "fields": {
      "numTokens": {
        "value": "30000",
        "source": "epoch",
        "citation": "\"Approximately 30K images were available for training and 5K for testing. Both sets were manually multi-labeled\""
      },
      "trainingTimeDays": {
        "value": "2.5",
        "source": "epoch",
        "citation": "\"With Fisher kernels, the trainingcost is reduced down to approximately 2h30.\""
      },
      "releaseDate": {
        "value": "2017-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN+weight noise+dynamic eval",
    "fields": {
      "flops": {
        "value": "4.21E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "54000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-08-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Phenaki",
    "fields": {
      "numParams": {
        "value": "1800000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Parti",
    "fields": {
      "flops": {
        "value": "3.96E+23",
        "source": "epoch",
        "citation": "Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.Table 1 shows for the 20B model16 encoder layers64 decoder layersDmodel = 4096Dhidden = 16384Num heads = 64Just below table 1:\"We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024\"I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.Section 3, Training: \"a totalof 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.\""
      },
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4800000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "486659.767",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM + MoS + Partial Shuffled",
    "fields": {
      "flops": {
        "value": "3.28E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AbLang",
    "fields": {
      "numParams": {
        "value": "355000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "D-LSRC(100)+KN5",
    "fields": {
      "releaseDate": {
        "value": "2017-08-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DALL\u00b7E 3",
    "fields": {
      "releaseDate": {
        "value": "2023-10-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mogrifier RLSTM (WT2)",
    "fields": {
      "flops": {
        "value": "1.09E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensorized Transformer (small)",
    "fields": {
      "releaseDate": {
        "value": "2019-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOM-176B",
    "fields": {
      "flops": {
        "value": "3.60E+23",
        "source": "epoch",
        "citation": "https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32384 A100 GPUs * 116 days = 3.6e23 at 30% utilization"
      },
      "numParams": {
        "value": "176247271424",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "262500000000",
        "source": "epoch",
        "citation": "350B words ~= 262B tokens"
      },
      "batchSize": {
        "value": "4194304",
        "source": "epoch",
        "citation": "Table 3. 2048*2048"
      },
      "trainingTimeDays": {
        "value": "2808",
        "source": "epoch",
        "citation": "117 days * 24 hours/day"
      },
      "gpuCount": {
        "value": "384",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Enhanced Neighborhood-Based Filtering",
    "fields": {
      "releaseDate": {
        "value": "2007-10-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "base LM+GNN",
    "fields": {
      "releaseDate": {
        "value": "2021-10-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DLRM-2020",
    "fields": {
      "flops": {
        "value": "4E+18",
        "source": "epoch",
        "citation": "Figure 1https://arxiv.org/abs/2104.05158"
      },
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "14.59890482",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TD(0)",
    "fields": {
      "releaseDate": {
        "value": "1977-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "R-FCN",
    "fields": {
      "flops": {
        "value": "6.14929E+16",
        "source": "epoch",
        "citation": "1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)They used a Nvidia K40 GPU and report training time/image in seconds (table 3)Assumed a 0.33 util rate"
      },
      "numTokens": {
        "value": "94427",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "5.50580435",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CogView",
    "fields": {
      "flops": {
        "value": "2.68E+22",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb"
      },
      "numParams": {
        "value": "4000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "50000000000",
        "source": "epoch",
        "citation": "\"We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).\"250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokensSo 30M text-image pairs and 50 billion words"
      },
      "costDollars": {
        "value": "44452.39188",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 16 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeT5+",
    "fields": {
      "numParams": {
        "value": "16000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TPM-LVD",
    "fields": {
      "numParams": {
        "value": "1120000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Engin-Medium(NE)",
    "fields": {
      "releaseDate": {
        "value": "2021-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YOLOv3",
    "fields": {
      "flops": {
        "value": "5.09392E+19",
        "source": "epoch",
        "citation": "We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examplesAssuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf"
      },
      "numParams": {
        "value": "56933216",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1281167",
        "source": "epoch",
        "citation": "Source: https://image-net.org/download.php"
      },
      "costDollars": {
        "value": "295.7577869",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA M40,NVIDIA GTX Titan X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NLP from scratch",
    "fields": {
      "numParams": {
        "value": "5000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "852000000",
        "source": "epoch",
        "citation": "\"Section 4 leverages large unlabeled data sets (\u223c 852 million words)\""
      },
      "releaseDate": {
        "value": "2011-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Semi-Supervised Embedding for DL",
    "fields": {
      "releaseDate": {
        "value": "2008-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Universal approximation via Feedforward Networks",
    "fields": {
      "releaseDate": {
        "value": "1989-03-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Turing-NLG",
    "fields": {
      "flops": {
        "value": "1.57E+22",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb"
      },
      "numParams": {
        "value": "17000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "34800000000",
        "source": "epoch",
        "citation": "Authors say they pretrain on the same data as for Megatron-LM. From the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf\"The resulting aggregatecorpus contains 174 GB of deduplicated text.\"174GB * 2e8words/GB = 3.48e10 words"
      },
      "costDollars": {
        "value": "58395.61929",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer ELMo",
    "fields": {
      "numParams": {
        "value": "56000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XVERSE-13B-2",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2800000000000",
        "source": "epoch",
        "citation": "Multilingual, 3.2 trillion tokens. Likely majority Chinese and English, so I'll assume .87 words per token, or ~2.8 trillion words"
      },
      "releaseDate": {
        "value": "2023-11-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SemExp",
    "fields": {
      "releaseDate": {
        "value": "2020-07-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RaSoR",
    "fields": {
      "numTokens": {
        "value": "4000000",
        "source": "epoch",
        "citation": "number of words in SQuAD"
      },
      "releaseDate": {
        "value": "2020-12-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BART-large",
    "fields": {
      "numParams": {
        "value": "406291456",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EI-REHN-1200D",
    "fields": {
      "releaseDate": {
        "value": "2017-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT-22B",
    "fields": {
      "flops": {
        "value": "4.00E+23",
        "source": "epoch",
        "citation": "\"ViT-22B was trained using 256 visual tokens per image, where each token represents a14 \u00d7 14 patch extracted from 224 \u00d7 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k:approximately 3 epochs\"\"ViT-22B was trained on 1024 TPU V4 chips for 177K steps\"256 * 177k * 65k = 3T tokens6 * 22B * 3T = 3.96e23 ~= 4e23also, MFU was high:\"Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward andbackward pass) on TPUv4 (Jouppi et al., 2020). ViT-22B\u2019s model flops utilization (MFU) (Chowdhery et al.,2022; Dehghani et al., 2021a) is 54.9%, indicating a very efficient use of the hardware.\""
      },
      "numParams": {
        "value": "21743000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4000000000",
        "source": "epoch",
        "citation": "\"Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels\""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Table-GPT",
    "fields": {
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout-LSTM+Noise(Bernoulli) (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DALL\u00b7E 2",
    "fields": {
      "numParams": {
        "value": "3500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "650000000",
        "source": "epoch",
        "citation": "\"When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability\""
      },
      "releaseDate": {
        "value": "2022-04-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Densely Connected LSTM + Var. Dropout",
    "fields": {
      "flops": {
        "value": "1.28E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "23000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-07-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "A3C FF hs",
    "fields": {
      "releaseDate": {
        "value": "2016-02-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "eDiff-I",
    "fields": {
      "numParams": {
        "value": "9100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000000",
        "source": "epoch",
        "citation": "\"The final dataset to train our model contains about one billion text-image pairs\""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Motion-Driven 3D Feature Tracking",
    "fields": {
      "numTokens": {
        "value": "1500",
        "source": "epoch",
        "citation": "\"The total number of possible input patterns was 65,536. Training sets of 650 and 1500 patterns picked at random from this total were used.\""
      },
      "releaseDate": {
        "value": "1988-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FNetAR Medium",
    "fields": {
      "numParams": {
        "value": "34300000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-07-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fractional Max-Pooling",
    "fields": {
      "flops": {
        "value": "1E+17",
        "source": "epoch",
        "citation": "For the 12M param model, training required \"18 hours on a GeForce GTX 780\". So would be somewhat larger for 27M.4 TFLOPS * 18 * 3600 * 0.4 = 1e17"
      },
      "numParams": {
        "value": "27000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "18",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 780",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-12-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-1.3B",
    "fields": {
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RT-Trajectory",
    "fields": {
      "releaseDate": {
        "value": "2023-11-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DBLSTM",
    "fields": {
      "numParams": {
        "value": "29900000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DALL-E mini",
    "fields": {
      "flops": {
        "value": "3.82579E+17",
        "source": "epoch",
        "citation": "flops = (4) * (1230 * 10**9) * (72 * 3600) * (0.3) = 3.8e17(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions"
      },
      "numTokens": {
        "value": "30000000",
        "source": "epoch",
        "citation": "3M+12M+15M = 30M from https://huggingface.co/dalle-mini/dalle-mini#training-data"
      },
      "trainingTimeDays": {
        "value": "72",
        "source": "epoch",
        "citation": "from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions"
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StarCoder",
    "fields": {
      "flops": {
        "value": "1.12E+23",
        "source": "epoch",
        "citation": "\"We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time\"320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23"
      },
      "numParams": {
        "value": "15500000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4194304",
        "source": "epoch",
        "citation": "\"We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are observed\""
      },
      "trainingTimeDays": {
        "value": "625.5",
        "source": "epoch",
        "citation": "625.5 hours = 320256 /512512 GPUs from \"We trained our model on a GPU cluster with 512 A100 80 GB GPUs \"320256 GPU hours from \"Based on the total number of GPU hours that training took (320,256)\"citations from sections 5.6 and 5.7"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPIDER2",
    "fields": {
      "flops": {
        "value": "8540000000000",
        "source": "epoch",
        "citation": "120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. Each input takes 17 residues."
      },
      "numParams": {
        "value": "116112",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-10-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT + DINO",
    "fields": {
      "flops": {
        "value": "2.1E+20",
        "source": "epoch",
        "citation": "\"Overall, training DINO with Vision Transformersachieves 76.1 top-1 accuracy using two 8-GPU servers for 3days\"GPU is V10016 * 125 teraflops * 3 days * 0.4 utilization= 2.1e20However, this isn't the best result in the paper (which is 80.1% with ViT-B/8). 76.1% is the result from ViT-B/16 per Table 2, which may be 5x cheaper than ViT-B/8 based on Table 1?"
      },
      "numParams": {
        "value": "85000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CamemBERT",
    "fields": {
      "flops": {
        "value": "8.3E+20",
        "source": "epoch",
        "citation": "\"Unless otherwise specified,our models use the BASE architecture, and arepretrained for 100k backpropagation steps on 256Nvidia V100 GPUs (32GB each) for a day\"256 V100-days256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)= 8.3e20\"Following (Liu et al., 2019), weoptimize the model using Adam (Kingma and Ba,2014) (\u03b21 = 0.9, \u03b22 = 0.98) for 100k steps withlarge batch sizes of 8192 sequences, each sequencecontaining at most 512 tokens\"Using compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20"
      },
      "numParams": {
        "value": "335000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "24000000000",
        "source": "epoch",
        "citation": " 31.9B tokens, Table 6. 24B words using 0.75 words/token"
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "1 day for each model (may not have been a full 24 hours)"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DCN+",
    "fields": {
      "numTokens": {
        "value": "4000000",
        "source": "epoch",
        "citation": "from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairsdownload-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=downloadwc -w on train-v.1.1 returns 4017471 words so around 4M words"
      },
      "releaseDate": {
        "value": "2017-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Phi-1.5",
    "fields": {
      "flops": {
        "value": "1.17E+21",
        "source": "epoch",
        "citation": "150B training tokens150B*1.3B*6 = 1.17e21also, took 1.5k GPU-hours with A100s, per Table 11500 * 312 trillion * 3600 * 0.3 = 5.05e20"
      },
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "22500000000",
        "source": "epoch",
        "citation": "30B tokens, or ~22.5B words"
      },
      "trainingTimeDays": {
        "value": "192",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VGG19",
    "fields": {
      "numParams": {
        "value": "144000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1300000",
        "source": "epoch",
        "citation": "\"In this section, we present the image classification results achieved by the describedConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012\u20132014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).\""
      },
      "releaseDate": {
        "value": "2014-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Diffractive Deep Neural Network",
    "fields": {
      "numParams": {
        "value": "8000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "55000",
        "source": "epoch",
        "citation": "size of MNIST\"For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). \""
      },
      "releaseDate": {
        "value": "2018-04-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YuYan 11B",
    "fields": {
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FAIRSEQ Adaptive Inputs",
    "fields": {
      "flops": {
        "value": "7.3E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pragmatic Theory solution (Netflix 2009)",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\""
      },
      "releaseDate": {
        "value": "2009-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Diffusion-GAN",
    "fields": {
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Faster R-CNN",
    "fields": {
      "releaseDate": {
        "value": "2015-06-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout (MNIST)",
    "fields": {
      "flops": {
        "value": "6.03937E+15",
        "source": "epoch",
        "citation": "Num mul-add / forward pass2 FLOPs / mult-add3 total mult-add / fp mult-add3000 epochs60000 training samples"
      },
      "numParams": {
        "value": "5592010",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)"
      },
      "costDollars": {
        "value": "0.1021265581",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 580",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-06-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mixtral 8x7B",
    "fields": {
      "numParams": {
        "value": "46700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Code Llama-34B",
    "fields": {
      "flops": {
        "value": "5.30E+23",
        "source": "epoch",
        "citation": "1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute"
      },
      "numParams": {
        "value": "34000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. \"We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.\"Subsequent fine-tuning batch sizes are 500k-1M. \"For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default.\" "
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mnemonic Reader",
    "fields": {
      "numTokens": {
        "value": "4000000",
        "source": "epoch",
        "citation": "size of SQuAD"
      },
      "releaseDate": {
        "value": "2017-05-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1-670M (UR50/S)",
    "fields": {
      "flops": {
        "value": "4.4E+20",
        "source": "epoch",
        "citation": "Information: 128 NVIDIA V100 GPUs [Pre-training details]840k steps [See Table S2: Hyperparameters]131,072 tokens per batch [\"We trained with 131,072 tokens per batch (128 gpus x 1024 tokens).\" - Pre-training details]Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 4.4e20 FLOP"
      },
      "numParams": {
        "value": "669200000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Flan-T5 11B",
    "fields": {
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Inception v3",
    "fields": {
      "numParams": {
        "value": "23626728",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000",
        "source": "epoch",
        "citation": "The full dataset is a lot larger and has far more categories. When people say \"ImageNet\" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/"
      },
      "releaseDate": {
        "value": "2015-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Automated WSD via WordNet",
    "fields": {
      "numTokens": {
        "value": "5000",
        "source": "epoch",
        "citation": "They do two experiments, one on a dataset of 5.000 tagged words andanother one on two datasets containing a total of around 40 million words, of which they only select 38 unique words and manually annotate the senses?I think the first one is more representative"
      },
      "releaseDate": {
        "value": "2004-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Decision tree (classification)",
    "fields": {
      "flops": {
        "value": "63000000000000",
        "source": "epoch",
        "citation": "The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs"
      },
      "numParams": {
        "value": "120000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "14460",
        "source": "epoch",
        "citation": "Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460"
      },
      "releaseDate": {
        "value": "2001-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Solar-10.7B",
    "fields": {
      "numParams": {
        "value": "10700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "REINFORCE in Stochastic Connectionism",
    "fields": {
      "releaseDate": {
        "value": "1992-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "JIANG",
    "fields": {
      "flops": {
        "value": "4.03E+22",
        "source": "epoch",
        "citation": "\"The training was conducted using 96 A100 80G GPUs, and the entire process took approximately 52 days.\"312 teraflop/s * 96 * 52 * 24 * 3600 * 0.3 = 4e22"
      },
      "numTokens": {
        "value": "467000000000",
        "source": "epoch",
        "citation": "467B tokens (inferred from Table 1).It's a mix of Chinese and English text, I'll use our standard 1:1 token:words ratio for Chinese."
      },
      "batchSize": {
        "value": "6000000",
        "source": "epoch",
        "citation": "\"During the training process, we employed a large batch size of 6 million tokens to enhance the model\u2019s stability\""
      },
      "trainingTimeDays": {
        "value": "1200",
        "source": "epoch",
        "citation": "52 days"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EN^2AS with performance reward",
    "fields": {
      "numParams": {
        "value": "23000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-07-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Jiutian",
    "fields": {
      "releaseDate": {
        "value": "2023-10-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Empirical evaluation of deep architectures",
    "fields": {
      "releaseDate": {
        "value": "2007-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Diabetic Retinopathy Detection Net",
    "fields": {
      "releaseDate": {
        "value": "2016-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deep Deterministic Policy Gradients",
    "fields": {
      "releaseDate": {
        "value": "2015-09-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BellKor 2008",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\""
      },
      "releaseDate": {
        "value": "2009-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE 3.0 Titan",
    "fields": {
      "flops": {
        "value": "1.04E+24",
        "source": "epoch",
        "citation": "The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP"
      },
      "numParams": {
        "value": "260000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "668000000000",
        "source": "epoch",
        "citation": "\"To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB\"Assuming 167M words per GB"
      },
      "batchSize": {
        "value": "1048576",
        "source": "epoch",
        "citation": "\"The maximum sequence length of context andthe memory length of language generation is 512 and 128, respectively\"In table 1, they use a global batch size of 512 when data parallelism is \"1\" and 2048 when DP is \"4\". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.2048 * 512 = 1048576."
      },
      "gpuCount": {
        "value": "1920",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KN5 LM + RNN 400/10 (WSJ)",
    "fields": {
      "flops": {
        "value": "6.144E+16",
        "source": "epoch",
        "citation": "\"Convergence is usuallyachieved after 10-20 epochs.\"Assuming a backward-forward ratio of 2:1, since this is a shallow network"
      },
      "numParams": {
        "value": "80000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "6400000",
        "source": "epoch",
        "citation": "The training corpus consists of 37M words from NYT section of English Gigaword. As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models (300K sentences) - it takes several weeks to train the most complex models"
      },
      "costDollars": {
        "value": "2.028347928",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2010-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "IBM Model 4",
    "fields": {
      "numTokens": {
        "value": "800000",
        "source": "epoch",
        "citation": "[WORDS]See FIgure 6"
      },
      "releaseDate": {
        "value": "1999-07-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1v",
    "fields": {
      "flops": {
        "value": "135000000000000070000",
        "source": "epoch",
        "citation": "\"ESM-1v models are pre-trained for 6 days on 64 V100 GPUs\" [F - Compute costs]Assuming  V100 PCIe single precision 14 TFLOPS and 0.3 utilization rate Estimate: (6*24*3600) s * 14e12 FLOP/s * 0.3 *64 = 1.4e20 FLOPsAlternative estimate based on Figure 7: 10^(7.5) GPU-seconds * 14e12 FLOP/s * 0.3 = 1.3e20 FLOPsMean: 1.35e20 FLOP"
      },
      "numParams": {
        "value": "650000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "98000000",
        "source": "epoch",
        "citation": "\"We train ESM-1v, a 650M parameter transformer language model for prediction of variant effects, on 98 million diverse protein sequences across evolution\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MetNet",
    "fields": {
      "releaseDate": {
        "value": "2020-03-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Imagen Video",
    "fields": {
      "numParams": {
        "value": "11600000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "top-down frozen classifier",
    "fields": {
      "releaseDate": {
        "value": "2021-02-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FLM-101B",
    "fields": {
      "flops": {
        "value": "5.72E+22",
        "source": "epoch",
        "citation": "192 GPUs * 160 TFLOP/s per GPU (reported, adjusted for utilization) * 21.54 days * 24 * 3600 = 5.72e22"
      },
      "numParams": {
        "value": "101000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "350000000000",
        "source": "epoch",
        "citation": "Trained with 311.54B tokens. The dataset is approximately 50/50 English/Chinese: \"It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling\". We assume 1 Chinese word per token and 0.75 English words per token (0.875 on average). 311B/0.875 ~= 350B."
      },
      "batchSize": {
        "value": "4310000",
        "source": "epoch",
        "citation": "Table 1"
      },
      "costDollars": {
        "value": "84000",
        "source": "epoch",
        "citation": "Authors report $100k. Adjusted for inflation."
      },
      "trainingTimeDays": {
        "value": "517",
        "source": "epoch",
        "citation": "\"Under this growth schedule, the total time cost for our 101B model is 21.54 days\""
      },
      "gpuType": {
        "value": "NVIDIA A800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DistilBERT",
    "fields": {
      "flops": {
        "value": "1.24416E+19",
        "source": "epoch",
        "citation": "Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.1.6e13*8*60**2*90*0.3 = 1.2e19"
      },
      "numParams": {
        "value": "66000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "6-layer MLP (MNIST)",
    "fields": {
      "flops": {
        "value": "130788000000000",
        "source": "epoch",
        "citation": "\"Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.\"I assume that the number of passes per epoch is 60k, the training set size."
      },
      "numParams": {
        "value": "12110000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "\"MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images\""
      },
      "costDollars": {
        "value": "0.007381387265",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2010-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NeMO Megatron GPT 20B",
    "fields": {
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-09-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE-Doc (151M)",
    "fields": {
      "releaseDate": {
        "value": "2020-12-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL Large + Phrase Induction",
    "fields": {
      "flops": {
        "value": "7.3E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM",
    "fields": {
      "flops": {
        "value": "21008000000000",
        "source": "epoch",
        "citation": "\"Due to limited computation time, training is stopped after 5 million sequence presentations\"Each sequence has p=100 elements in the long-delay setting.COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN"
      },
      "numParams": {
        "value": "10504",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1273000",
        "source": "epoch",
        "citation": "Table 8. The rightmost column lists numbers of training sequences required to achieve the stoppingcriterion.This applies to experiment 5 (multiplication)Sequences have random lengths, on the order of 100-1000 (table 7 )"
      },
      "releaseDate": {
        "value": "1997-11-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNNLM + Dynamic KL Regularization (WT2)",
    "fields": {
      "flops": {
        "value": "2.19E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "87600000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SCRN(Structurally Constrained Recurrent Network)",
    "fields": {
      "numParams": {
        "value": "26500000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-12-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "mT5-XXL",
    "fields": {
      "flops": {
        "value": "7.80E+22",
        "source": "epoch",
        "citation": "\"We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.\"1 trillion tokens * 13 billion params * 6 = 7.8e22"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "\"totaling 6.6B pages and 6.3T tokens\"It's multilingual so we don't have a standard word:token ratio, but using the 0.75 for English that's ~5 trillion.Distribution by language is in Appendix A.The model was trained for 0.159 equivalent epochs of the full dataset, or 1 epoch on a subset of 1 trillion tokens."
      },
      "batchSize": {
        "value": "1048576",
        "source": "epoch",
        "citation": "\"We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.\""
      },
      "releaseDate": {
        "value": "2020-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Nucleotide Transformer",
    "fields": {
      "flops": {
        "value": "1.20E+22",
        "source": "epoch",
        "citation": "\"Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days\"Assuming 78 TFLOP / s for 32-bit calculations and 0.5 utilization rateEstimate: 78e12 FLOP/s * 128 GPUs * 28 days * 86400 seconds * 0.5 utilization rate = 1.2e22"
      },
      "numParams": {
        "value": "2500000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "672",
        "source": "epoch",
        "citation": "\"Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days\""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DImensionality Reduction",
    "fields": {
      "numParams": {
        "value": "3800000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "70000",
        "source": "epoch",
        "citation": "After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA(Fig. 2B)"
      },
      "releaseDate": {
        "value": "2006-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SqueezeBERT",
    "fields": {
      "numParams": {
        "value": "51100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Big Transformer for Back-Translation",
    "fields": {
      "flops": {
        "value": "1.08084E+20",
        "source": "epoch",
        "citation": "(128) * (28.26 * 10**12) * (27*3600 + 40*60) * (0.3)  = 108084326400000000000(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  \"We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are intercon-nected by Infiniband. Experiments are run on 16machines and we perform 30K synchronous up-dates. \"\"We train models with 16-bit floating pointoperations\"from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957 V100 have 28.26 TFLOPSin section 5.6 we have\"train this system we perform 300K training up-dates in 27h 40min on 128 GPUs;\""
      },
      "numTokens": {
        "value": "3390000000",
        "source": "epoch",
        "citation": "\"Finally, for WMT English-German we train on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs.\"We assume that 1 sentence have 15 words"
      },
      "trainingTimeDays": {
        "value": "27.666",
        "source": "epoch",
        "citation": "\"training updates in 27h 40min on 128 GPUs\""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-08-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hopfield Networks (2020)",
    "fields": {
      "releaseDate": {
        "value": "2020-07-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mamba-2.8B",
    "fields": {
      "flops": {
        "value": "5.40E+21",
        "source": "epoch",
        "citation": "\"Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models.\"3b * 300b * 6 = 5.4e21Note: this is a new architecture so not sure how well 6*params*data works as a heuristicFigure 4 shows perplexity curves where Mamba is trained up to 2e20 FLOP, but those are for the 125M and 1.3B variants."
      },
      "numParams": {
        "value": "2800000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "data2vec (speech)",
    "fields": {
      "numParams": {
        "value": "705134592",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "13132800",
        "source": "epoch",
        "citation": "Section 5.2:\"we pre-train data2vec on the 960hours of speech audio data from Librispeech (LS-960)\"13,680 words per hour"
      },
      "releaseDate": {
        "value": "2022-01-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SeqVec",
    "fields": {
      "flops": {
        "value": "4.1E+19",
        "source": "epoch",
        "citation": "3 weeks, 5 NVIDIA Titan GPUs (Assuming NVIDIA Titan V and 30% utilization rate for calculation) with 12 GB memory, "
      },
      "numParams": {
        "value": "93000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "508",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "5",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Titan V",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Visualizing CNNs",
    "fields": {
      "flops": {
        "value": "5.32E+17",
        "source": "epoch",
        "citation": "1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization = 532 PF = 0.0062 pfs-daysSource: https://openai.com/blog/ai-and-compute"
      },
      "costDollars": {
        "value": "9.021179303",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 580",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-11-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "UnifiedQA",
    "fields": {
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "36",
        "source": "epoch",
        "citation": "\"pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.\""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",
    "fields": {
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-300units",
    "fields": {
      "numParams": {
        "value": "12000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NMT Transformer 437M",
    "fields": {
      "numParams": {
        "value": "437700000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-02-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensorized Transformer (large PTB)",
    "fields": {
      "releaseDate": {
        "value": "2019-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RWKV-4 14B",
    "fields": {
      "flops": {
        "value": "2.78E+22",
        "source": "epoch",
        "citation": "from HuggingFace page: https://huggingface.co/BlinkDL/rwkv-4-pile-14btrained for 331B tokens14 billion * 331 billion * 6 = 2.78e22paper notes that a forward pass is almost exactly 2x parameters (within 2%): \"Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant.\" and that 6*params*tokens is a good approximation because it's not a transformer: \"FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D2L), which is thetwice (add and multiply) the number of parametersin linear layers. The backwards pass FLOPs can beapproximated as twice that of the forward pass. Sothe total is 6(V D + 13D2L) per token for training(3x fw FLOPs). It is noteworthy that FLOPs areindependent of the context length, unlike regulartransformers\""
      },
      "numParams": {
        "value": "14000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "262144",
        "source": "epoch",
        "citation": "262144 (or 131072?)\"To train the models mentioned, we... switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens\""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EfficientZero",
    "fields": {
      "releaseDate": {
        "value": "2021-10-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Jais",
    "fields": {
      "flops": {
        "value": "3.08E+22",
        "source": "epoch",
        "citation": "C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "300000000000",
        "source": "epoch",
        "citation": "395B tokens ~= 300B words"
      },
      "batchSize": {
        "value": "3932160",
        "source": "epoch",
        "citation": "\"After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. \""
      },
      "trainingTimeDays": {
        "value": "600",
        "source": "epoch",
        "citation": "2023 June 25 to July 18 = 25 days = 600 hours"
      },
      "releaseDate": {
        "value": "2023-08-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AudioGen",
    "fields": {
      "flops": {
        "value": "7.20E+21",
        "source": "epoch",
        "citation": "\"the large model was trained on 128 A100 GPUs for 200k steps (\u223c1 week)\"A100s are 312 teraflop/s128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "1 week"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tagging via Viterbi Decoding",
    "fields": {
      "releaseDate": {
        "value": "2002-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deep Belief Nets",
    "fields": {
      "numParams": {
        "value": "1600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "\"The network that performed best on the validation set wasthen tested and had an error rate of 1.39%. This network wasthen trained on all 60,000 training images8 until its error-rateon the full training set was as low as its final error-rate hadbeen on the initial training set of 44,000 images.\""
      },
      "releaseDate": {
        "value": "2006-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MnasNet-A1 + SSDLite",
    "fields": {
      "flops": {
        "value": "1.50E+21",
        "source": "epoch",
        "citation": "\"each architecture search takes 4.5 days on 64 TPUv2 devices\"This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPUAssuming a 33% utilization rate:4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOPHowever, it is unclear if \"64 TPUv2 devices\" refers to chips or modules, so the true compute might be 1/4 of this amount."
      },
      "numParams": {
        "value": "4900000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "118000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "4330.997218",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "108",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM with forget gates",
    "fields": {
      "numParams": {
        "value": "276",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "30000",
        "source": "epoch",
        "citation": "Training was stopped after at most 30000training streams, each of which was endedwhen the first prediction error or the100000th successive input symbol occurredNOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)"
      },
      "releaseDate": {
        "value": "1999-01-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Q-learning",
    "fields": {
      "releaseDate": {
        "value": "1989-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProtBERT-BFD",
    "fields": {
      "flops": {
        "value": "3.90E+22",
        "source": "epoch",
        "citation": "\"FLOP = 420M*2*(800k*512*32k+200k*2048*6k) +  420M*4*(800k*512*32k+200k*2048*6k), 1M steps total split into two phases, (1) 800k steps, seq length 512 (bath size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd\""
      },
      "numParams": {
        "value": "420000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VRNS-RNN-3-3-5",
    "fields": {
      "numParams": {
        "value": "1500000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE 3.5",
    "fields": {
      "releaseDate": {
        "value": "2023-06-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mesh-TensorFlow Transformer 2.9B (translation)",
    "fields": {
      "flops": {
        "value": "6.84288E+19",
        "source": "epoch",
        "citation": "flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from section 9.1 : \"On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.\"from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 45TFLOPs per chips"
      },
      "numParams": {
        "value": "2900000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "22",
        "source": "epoch",
        "citation": "from section 9.1 \"On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Decay RNN",
    "fields": {
      "numParams": {
        "value": "1400000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Self-Attention and Convolutional Layers",
    "fields": {
      "flops": {
        "value": "6.75E+17",
        "source": "epoch",
        "citation": "(15e9) * (300) * (50000) * 3 = 675000000000000000(inference compute) * (epochs) * (dataset size) * (constant to account for backpropagation) epochs from appendix B table 2inference compute from table 1"
      },
      "numParams": {
        "value": "29500000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": "size of CIFAR-10"
      },
      "releaseDate": {
        "value": "2019-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "4-gram + 8 DENN",
    "fields": {
      "numParams": {
        "value": "16100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-12-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN 500/10 + RT09 LM (NIST RT05)",
    "fields": {
      "flops": {
        "value": "3.41464E+15",
        "source": "epoch",
        "citation": "\"Convergence is usually achieved after 10-20 epochs.\"Assuming a backward-forward ratio of 2:1, since this is a shallow network"
      },
      "numParams": {
        "value": "5269500",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "5400000",
        "source": "epoch",
        "citation": "\"Table 4: Comparison of very large back-off LMs and RNN LMstrained only on limited in-domain data (5.4M words).\""
      },
      "costDollars": {
        "value": "0.1130554583",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2010-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao - Wen Su",
    "fields": {
      "releaseDate": {
        "value": "2021-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gen-1",
    "fields": {
      "releaseDate": {
        "value": "2023-02-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "T2R 75% + Pretrain",
    "fields": {
      "releaseDate": {
        "value": "2021-03-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TrellisNet",
    "fields": {
      "flops": {
        "value": "2.78E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "180000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-10-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-410m",
    "fields": {
      "numParams": {
        "value": "410000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Residual Dense Network",
    "fields": {
      "releaseDate": {
        "value": "2018-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM (2018)",
    "fields": {
      "numParams": {
        "value": "13000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-03-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Ngram corpus",
    "fields": {
      "releaseDate": {
        "value": "2012-07-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Conformer + Wav2vec 2.0 + Noisy Student",
    "fields": {
      "flops": {
        "value": "7.60E+21",
        "source": "epoch",
        "citation": "\"We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...We fine-tune the pre-trained checkpoints (400k steps) with global batchsize 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models\"TPU v3 chips are 123 teraflop/s. 2 chips per core512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "7 days"
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GoogLeNet / InceptionV1",
    "fields": {
      "flops": {
        "value": "1.55714E+18",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numParams": {
        "value": "6797700",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000",
        "source": "epoch",
        "citation": "\"The ILSVRC 2014 classification challenge involves thetask of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing[...]We participated in the challenge with no external dataused for training.\""
      },
      "costDollars": {
        "value": "14.16467815",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-06-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PreTrans-3L-250H",
    "fields": {
      "numParams": {
        "value": "43000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-03-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 1.3",
    "fields": {
      "releaseDate": {
        "value": "2023-04-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adaptive LSTM + DeFINE",
    "fields": {
      "releaseDate": {
        "value": "2019-11-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Learnability theory of language development",
    "fields": {
      "releaseDate": {
        "value": "1984-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ONE-PEACE",
    "fields": {
      "flops": {
        "value": "1.8E+20",
        "source": "epoch",
        "citation": "4 billion params * 7.5 billion data * 6 = 1.8e20.see training dataset size notes. this estimate required some more assumptions than usual."
      },
      "numParams": {
        "value": "4000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1600000000",
        "source": "epoch",
        "citation": "\"After these steps, we retain about 1.5 billion image-text pairs\"...\"We also perform simple cleaning on the data, which involves removing samples with text lengths less than 3 or greater than512, as well as texts containing non-English or emoji characters. Ultimately, we obtain about 2.4 million audio-text pairs, with a total duration of around 8,000 hours\"8000 hours = 480,000 minutes = ~109,440,000 words at 228 wpmhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pqTrained on 10 epochs for audio. For text, they train on \"200K steps with a batch size of 32768\" = 6,533,600,000Adding together, they train on ~ 7.5b data points on a dataset of 1.6b, for ~4.7 epochs on average."
      },
      "releaseDate": {
        "value": "2023-05-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MemSizer",
    "fields": {
      "flops": {
        "value": "7.3E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "357000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ADALINE",
    "fields": {
      "flops": {
        "value": "9900",
        "source": "epoch",
        "citation": "\"The method of searching that has proven most useful is the method of steepest descent\"Apparently each pattern was only shown once to the system.So the training compute is (forward pass compute) * (3 for backprop) * dataset size"
      },
      "numParams": {
        "value": "17",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "100",
        "source": "epoch",
        "citation": "\"The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.\"https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf"
      },
      "releaseDate": {
        "value": "1960-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DEQ-Transformer (Post-LN) + Jacobian Regularisation",
    "fields": {
      "flops": {
        "value": "2.9E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "98000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DQN-2015",
    "fields": {
      "numParams": {
        "value": "1693362",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "50000000",
        "source": "epoch",
        "citation": "Methods: \"we trained for a total of 50 million frames\""
      },
      "releaseDate": {
        "value": "2015-02-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EfficientNetV2",
    "fields": {
      "flops": {
        "value": "9.56E+19",
        "source": "epoch",
        "citation": "Table 7, page 7: 45 hours on 32 TPUv3 cores.\"Each v3 TPU chip contains two TensorCores.\"TPU performance per chip = 123e12 FLOP/s32 cores = 16 chips123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOPhttps://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3"
      },
      "numParams": {
        "value": "208000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "14197122",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "45",
        "source": "epoch",
        "citation": "Table 7"
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Inceptionv4",
    "fields": {
      "numParams": {
        "value": "43000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-02-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PPLX-70B-Online",
    "fields": {
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MegaMolBART",
    "fields": {
      "flops": {
        "value": "7.2E+20",
        "source": "epoch",
        "citation": "\"MegaMolBART was trained with data parallelism on 64 V100 32 GB GPUs (4 nodes x 16 GPUs) for 8 epochs (approximately 160k iterations or ~80 wall-clock hours) using a batch size of 32 molecules per GPU (micro batch)\"https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html64 * 130 teraflops * 80 * 3600 * 0.3 = 7.2e20"
      },
      "numParams": {
        "value": "45000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "80",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensorized Transformer (257M)",
    "fields": {
      "flops": {
        "value": "4.76E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaVA-NeXT-34B (LLaVA-1.6)",
    "fields": {
      "flops": {
        "value": "2.58785E+20",
        "source": "epoch",
        "citation": "2.6e20 = 32 * 312e12 * 0.3 * 24* 3600 = num gpus * peak flops * assumed utilization rate * time in seconds\"The largest 34B variant finishes training in ~1 day with 32 A100s.\""
      },
      "numParams": {
        "value": "34750000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "\"The largest 34B variant finishes training in ~1 day with 32 A100s.\""
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NoisyNet-Dueling",
    "fields": {
      "releaseDate": {
        "value": "2017-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout (TIMIT)",
    "fields": {
      "numParams": {
        "value": "48840185",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "41620",
        "source": "epoch",
        "citation": "4162 utterances, guesstimated avg 10 words per utterance"
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 580",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-06-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BIDAF",
    "fields": {
      "flops": {
        "value": "3.46861E+18",
        "source": "epoch",
        "citation": "flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =citation from the section about cloze test experiments \"The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4\" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).flops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632"
      },
      "numParams": {
        "value": "2600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "47160000",
        "source": "epoch",
        "citation": "\"In a cloze test, the reader is asked to fill in words that have been removed from a passage,for measuring one\u2019s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-sive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)examples from CNN and DailyMail news articles, respectively. \"assuming 40 words per example we get around 47160000 words (SQuAD have around 40 words per example - so I think it should be similar case for this dataset)"
      },
      "trainingTimeDays": {
        "value": "60",
        "source": "epoch",
        "citation": "see compute notes"
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GTX Titan X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Persia",
    "fields": {
      "numParams": {
        "value": "100000000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer + GFM",
    "fields": {
      "flops": {
        "value": "8039999999999999000",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "185000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VALL-E X",
    "fields": {
      "numParams": {
        "value": "700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "910000000",
        "source": "epoch",
        "citation": "70k hours, mix of Chinese and English but mostly English70k * 13k words/hour = 910,000,000 words"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TRIMELMlong (150M)",
    "fields": {
      "numParams": {
        "value": "150000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GloVe (32B)",
    "fields": {
      "numParams": {
        "value": "120000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "42000000000",
        "source": "epoch",
        "citation": "\"We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]\""
      },
      "releaseDate": {
        "value": "2014-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StarCoder 2 15B",
    "fields": {
      "flops": {
        "value": "3.87E+23",
        "source": "epoch",
        "citation": "estimation is given in Table 6"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4100000000000",
        "source": "epoch",
        "citation": "from Table 7, 4.1T tokens "
      },
      "releaseDate": {
        "value": "2024-02-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EGRU (WT2)",
    "fields": {
      "flops": {
        "value": "2.31E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "74000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Prototypical networks",
    "fields": {
      "releaseDate": {
        "value": "2017-03-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YOLOv2",
    "fields": {
      "numParams": {
        "value": "51000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-12-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TD-Gammon",
    "fields": {
      "flops": {
        "value": "18232157622833",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numParams": {
        "value": "25000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "6300000",
        "source": "epoch",
        "citation": "\"This network was trainedfor over 300,000 training games\"Each backgammon game has an avg of around 21 movementshttps://www.bkgm.com/rgb/rgb.cgi?view+712"
      },
      "releaseDate": {
        "value": "1992-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AdClickNet",
    "fields": {
      "releaseDate": {
        "value": "2014-08-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Inflection-2",
    "fields": {
      "flops": {
        "value": "1.00E+25",
        "source": "epoch",
        "citation": "\"Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10\u00b2\u2075 FLOPs\"(the second 1 is there because of airtable being wonky, it's not a real sig fig)"
      },
      "gpuCount": {
        "value": "5000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA H100 SXM5",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Alleviated TOI 10 (WT103)",
    "fields": {
      "releaseDate": {
        "value": "2019-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOM-7.1B",
    "fields": {
      "flops": {
        "value": "1.48E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "7070000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NetTalk (transcription)",
    "fields": {
      "flops": {
        "value": "28328002560",
        "source": "epoch",
        "citation": "18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word"
      },
      "numParams": {
        "value": "18629",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1024",
        "source": "epoch",
        "citation": "We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade"
      },
      "releaseDate": {
        "value": "1987-06-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Listen, Attend and Spell",
    "fields": {
      "releaseDate": {
        "value": "2015-08-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Calm2-7B",
    "fields": {
      "flops": {
        "value": "5.46E+22",
        "source": "epoch",
        "citation": "6*7B*1.3T = 546000000000000000000006ND aproximation"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1300000000000",
        "source": "epoch",
        "citation": "\"1.3T tokens of publicly available Japanese and English datasets. \" so around 1.3T words assuming 1 word per token."
      },
      "releaseDate": {
        "value": "2023-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MMS-1B",
    "fields": {
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Codex",
    "fields": {
      "numParams": {
        "value": "12000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "31800000000",
        "source": "epoch",
        "citation": "\"Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average linelength greater than 100, had maximum line length greaterthan 1000, or contained a small percentage of alphanumericcharacters. After filtering, our final dataset totaled 159 GB.\"1 GB ~ 200M words"
      },
      "releaseDate": {
        "value": "2021-07-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Memformer (4 encoder + 16 decoder)",
    "fields": {
      "flops": {
        "value": "1.2E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "76200000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "gLM",
    "fields": {
      "flops": {
        "value": "1.51E+20",
        "source": "epoch",
        "citation": "\"The training stage takes several weeks on four NVIDIA A100 GPUs.\"Assumption: 2 weeks, 40% utilization rate, 78 TFLOP peak rateEstimate: =(2*7*24*3600) s * 78e12 FLOP/s *4 GPU * 0.4"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-J-6B",
    "fields": {
      "flops": {
        "value": "1.50E+22",
        "source": "epoch",
        "citation": "source: zero shot evaluation table in GitHub"
      },
      "numParams": {
        "value": "6053381344",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "160000000000",
        "source": "epoch",
        "citation": "\"The model was trained on 400B tokens from The Pile dataset with 800GB text.\"1 GB ~ 200M words"
      },
      "costDollars": {
        "value": "25176.8016",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Honghu Graphic",
    "fields": {
      "numParams": {
        "value": "2000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fully Convolutional Networks",
    "fields": {
      "releaseDate": {
        "value": "2014-11-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NMM(LSTM+RNN)",
    "fields": {
      "numParams": {
        "value": "5180000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multiscale deformable part model",
    "fields": {
      "releaseDate": {
        "value": "2008-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN-SpeedUp",
    "fields": {
      "numTokens": {
        "value": "697500",
        "source": "epoch",
        "citation": "Section 3: \"The data used in the following experiments were obtained fromPenn Tree Bank: sections 0-20 were used as training data (about930K tokens)\"0.75 words per token for English"
      },
      "releaseDate": {
        "value": "2011-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fairseq-dense 13B",
    "fields": {
      "flops": {
        "value": "3.27E+22",
        "source": "epoch",
        "citation": "Table 1"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "84000000000",
        "source": "epoch",
        "citation": "112B tokens, or 84B words at 0.75 English words/token. \"We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the English subset of CC100, totalling 112B tokens\"...\"All models are trained for 300B tokens with a sequence length of 2048 tokens.\""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MetaLM",
    "fields": {
      "releaseDate": {
        "value": "2022-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RQ-Transformer (3.8B params ImageNet dataset) copy",
    "fields": {
      "flops": {
        "value": "1.45567E+18",
        "source": "epoch",
        "citation": "flops = (4) * (3120 * 10**9) * (4.5*24 * 3600) * (0.3) = 1455667200000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\"We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. \"we provide details for LSUN-cat with largest compute"
      },
      "numParams": {
        "value": "1388000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000",
        "source": "epoch",
        "citation": "size of ImageNet"
      },
      "trainingTimeDays": {
        "value": "108",
        "source": "epoch",
        "citation": "4.5  days for ImageNet for 3.8B model\"We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. \""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT-G/14",
    "fields": {
      "flops": {
        "value": "3.40E+21",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodbAlternatively: per paper, ViT-G required between 20-30k TPUv3 core-days to train (from eyeballing the tick marks in Figure 9).TPUv3 is 123 teraflop/s per chip, 2 cores per chip123 trillion * (1/2) * 25,000 * 3600 * 0.4 = 2.2e21"
      },
      "numParams": {
        "value": "1843000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000000",
        "source": "epoch",
        "citation": "\"For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset usedin many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists ofnearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automaticpipeline\""
      },
      "costDollars": {
        "value": "5541.837211",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LightOn Mini",
    "fields": {
      "flops": {
        "value": "2.40E+23",
        "source": "epoch",
        "citation": "6ND aproximation: 6*40B*1T = 2.4e23"
      },
      "numParams": {
        "value": "40000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "\"The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.\"  assuming 0.75 words per token - 750000000000.0 words"
      },
      "releaseDate": {
        "value": "2023-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Self Organizing System",
    "fields": {
      "numParams": {
        "value": "225",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "256",
        "source": "epoch",
        "citation": "\" The modifier was thendisabled so that no further changes in the net couldoccur and all 256 possible input patterns were then presented in turn.\"\"For these purposes, 16-element nets (8 input and 8output) were used because it was desired to exhaust allpossible input patterns, and we were limited to about2^8 inputs by available time. \""
      },
      "releaseDate": {
        "value": "1955-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Grok-0",
    "fields": {
      "numParams": {
        "value": "33000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "N-gram",
    "fields": {
      "releaseDate": {
        "value": "2014-12-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Samsung Gauss Language",
    "fields": {
      "releaseDate": {
        "value": "2023-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaZero",
    "fields": {
      "flops": {
        "value": "3.67E+22",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numTokens": {
        "value": "700000",
        "source": "epoch",
        "citation": "\"We trained a separate instance of AlphaZero for each game. Training proceededfor 700,000 steps\""
      },
      "costDollars": {
        "value": "162054.6972",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multitask Unified Model (MUM)",
    "fields": {
      "releaseDate": {
        "value": "2021-05-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Rubik's cube ADR robot",
    "fields": {
      "flops": {
        "value": "8.54E+20",
        "source": "epoch",
        "citation": "source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389"
      },
      "numParams": {
        "value": "27769565",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "62400000",
        "source": "epoch",
        "citation": "\" The cumulative amount of experience over that period used for training on theRubik\u2019s cube is roughly 13 thousand years, which is on the same order of magnitude as the 40 thousand years used byOpenAI Five\"13/40 * 1.92e8 = 6.24e7"
      },
      "costDollars": {
        "value": "3102.267275",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BigSSL",
    "fields": {
      "numParams": {
        "value": "8000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "42626880000",
        "source": "epoch",
        "citation": "Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hoursNote this involves significant self-training: \"Noisy student training (NST) [23], [41] is a self-trainingmethod where a teacher model generates pseudo-labels for alarge unlabeled dataset, which is in turn used to train a studentmodel with augmentation.\"1 hour ~ 13,680 words13680 * 3116000 = 42626880000"
      },
      "releaseDate": {
        "value": "2021-01-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Web mining + Decision tree recommender",
    "fields": {
      "releaseDate": {
        "value": "2002-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Segatron -XL base, M=150 + HCP",
    "fields": {
      "releaseDate": {
        "value": "2022-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPU implementation of neural networks",
    "fields": {
      "releaseDate": {
        "value": "2004-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM+WT+Cache+IOG (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2017-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ReLU (NORB)",
    "fields": {
      "numParams": {
        "value": "16210006",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "291600",
        "source": "epoch",
        "citation": "\"There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).\""
      },
      "releaseDate": {
        "value": "2010-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PaLM (540B)",
    "fields": {
      "flops": {
        "value": "2.53E+24",
        "source": "epoch",
        "citation": "See Table 20: https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.Equivalent to 6144 TPUv4 for 1368 hours.46.2% model FLOPs utilization"
      },
      "numParams": {
        "value": "540350000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "585000000000",
        "source": "epoch",
        "citation": "\"The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.\"1 token ~ 0.75 words"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "\"For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k\""
      },
      "costDollars": {
        "value": "3232806.533",
        "source": "epoch",
        "citation": "Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization."
      },
      "trainingTimeDays": {
        "value": "1368",
        "source": "epoch",
        "citation": "6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.Equivalent to 6144 TPUv4 for 1368 hours."
      },
      "gpuCount": {
        "value": "6144",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.462",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL DeFINE (141M)",
    "fields": {
      "flops": {
        "value": "6.2E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "141000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pangu-Weather",
    "fields": {
      "flops": {
        "value": "3.98E+22",
        "source": "epoch",
        "citation": "\"Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16\u2009days on a cluster of 192 NVIDIATesla-V100 GPUs.\"192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22"
      },
      "numParams": {
        "value": "256000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1536",
        "source": "epoch",
        "citation": "4*16 = 64 days\"Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16\u2009days on a cluster of 192 NVIDIATesla-V100 GPUs.\""
      },
      "gpuCount": {
        "value": "192",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CPC v2",
    "fields": {
      "numParams": {
        "value": "303000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pointer Sentinel-LSTM",
    "fields": {
      "releaseDate": {
        "value": "2016-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Japanese dialog transformers",
    "fields": {
      "numParams": {
        "value": "1600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2100000000",
        "source": "epoch",
        "citation": "[Pairs of text]\"We obtained 2.1 billion (521 GB) pairs by this method. The average number of utterances in the input context was 2.913, and the average number of characters was 62.3 for the input context and 20.3 for the target utterance\""
      },
      "releaseDate": {
        "value": "2021-11-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-4V",
    "fields": {
      "releaseDate": {
        "value": "2023-09-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama 2-34B",
    "fields": {
      "flops": {
        "value": "4.08E+23",
        "source": "epoch",
        "citation": "All models sizes trained on 2.0T tokens, per table 12T * 34b * 6 = 4.08e23Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%."
      },
      "numParams": {
        "value": "34000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fold2Seq",
    "fields": {
      "numTokens": {
        "value": "45995",
        "source": "epoch",
        "citation": "\"Training set includes 45995 proteins belonging to a total of 971 folds\""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Emu (BAAI)",
    "fields": {
      "flops": {
        "value": "2.70E+21",
        "source": "epoch",
        "citation": "\"We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.\"https://www.wolframalpha.com/input?i=128*312+TFLOPS+*+2+days+*+0.4"
      },
      "numParams": {
        "value": "14000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "48",
        "source": "epoch",
        "citation": "\"We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.\""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BigChaos OptiBlend",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\""
      },
      "releaseDate": {
        "value": "2009-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "bilingual-gpt-neox-4b",
    "fields": {
      "flops": {
        "value": "1.20E+22",
        "source": "epoch",
        "citation": "3.8 billion params * 524b tokens * 6 = 1.2e22"
      },
      "numParams": {
        "value": "3800000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CODA",
    "fields": {
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HyperNEAT",
    "fields": {
      "numParams": {
        "value": "239712",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-03-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-NeoX-20B",
    "fields": {
      "flops": {
        "value": "9.32E+22",
        "source": "epoch",
        "citation": "Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate."
      },
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "177167400000",
        "source": "epoch",
        "citation": "\"In aggregate, the Pile consists of over 825GiB of raw text data\"1 GB ~ 200M words"
      },
      "batchSize": {
        "value": "3150000",
        "source": "epoch",
        "citation": "\"we opt to use the same batch size as OpenAI\u2019s 175B model\u2013approximately 3.15M tokens, or 1538 contexts of 2048 tokens each, and train for a total of 150,000 steps\""
      },
      "costDollars": {
        "value": "202407.4644",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "2160",
        "source": "epoch",
        "citation": "see other notes"
      },
      "gpuCount": {
        "value": "96",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.375",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-02-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "U-PaLM (540B)",
    "fields": {
      "flops": {
        "value": "2.53E+24",
        "source": "epoch",
        "citation": "\"The total number of extra tokens we train on for the 540Bmodel is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.\"original PaLM was 2.527e+24. adding 0.16% is ~2.53e24"
      },
      "numParams": {
        "value": "540000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "5 days"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AMDIM",
    "fields": {
      "numParams": {
        "value": "626000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-06-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Samsung Gauss Code",
    "fields": {
      "releaseDate": {
        "value": "2023-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DreamLLM",
    "fields": {
      "flops": {
        "value": "7.5479E+20",
        "source": "epoch",
        "citation": "flops = (128) * ( 312 * 10**12) * (17.5 * 3600) * (0.3) = 7.5e20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from Table 11:128xA800 GPU(6+10+1.5) hours"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "17.5",
        "source": "epoch",
        "citation": "from Table 11: (6+10+1.5) hours"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN",
    "fields": {
      "releaseDate": {
        "value": "2012-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Iterative Bootstrapping WSD",
    "fields": {
      "numTokens": {
        "value": "460000000",
        "source": "epoch",
        "citation": "the data were extracted from a 460 million word corpus"
      },
      "releaseDate": {
        "value": "1995-06-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BERT-Large-CAS (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2019-04-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LUKE",
    "fields": {
      "flops": {
        "value": "1.758E+20",
        "source": "epoch",
        "citation": "(16) * (1413 * 10**10) * (30 * 24 * 3600) * (0.3) = 175799808000000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from appendix A: \"Werun the pretraining on NVIDIA\u2019s PyTorch Dockercontainer 19.02 hosted on a server with two IntelXeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30days.\"peak flops for fp32 from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957"
      },
      "numParams": {
        "value": "484000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3500000000",
        "source": "epoch",
        "citation": "\"As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. \""
      },
      "batchSize": {
        "value": "2048",
        "source": "epoch",
        "citation": "table in appendix A"
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": "see compute notes"
      },
      "gpuCount": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama Guard",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000",
        "source": "epoch",
        "citation": "14k prompt-response pairs. Based on training details it's 4M tokens, or 3M words"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Palmyra Large 20B",
    "fields": {
      "flops": {
        "value": "9.60E+22",
        "source": "epoch",
        "citation": "\"Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.\"I'm not sure if the 800B is how many tokens the model was trained on, or the size of the dataset. But the dataset linked on HuggingFace has 1T tokens, so 800B as tokens trained is more likely.20B*800B*6 = 9.6e22"
      },
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1 trillion tokens, or 750B words: https://huggingface.co/datasets/Writer/palmyra-data-index"
      },
      "releaseDate": {
        "value": "2023-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ALBERT",
    "fields": {
      "numParams": {
        "value": "18000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3300000000",
        "source": "epoch",
        "citation": "Pretraining same as for BERT - Wikipedia and BookCorpus\"For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)\""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CODEFUSION (Python)",
    "fields": {
      "flops": {
        "value": "7.92E+18",
        "source": "epoch",
        "citation": "V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP"
      },
      "numParams": {
        "value": "75000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4390400",
        "source": "epoch",
        "citation": "Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens"
      },
      "trainingTimeDays": {
        "value": "11",
        "source": "epoch",
        "citation": "\"The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.\""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 SXM2 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Convolutional Pose Machines",
    "fields": {
      "releaseDate": {
        "value": "2016-01-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "InternImage",
    "fields": {
      "numParams": {
        "value": "1080000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "427000000",
        "source": "epoch",
        "citation": "427M images"
      },
      "releaseDate": {
        "value": "2022-11-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HSO",
    "fields": {
      "flops": {
        "value": "3.45E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "345000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Chinese - English translation",
    "fields": {
      "releaseDate": {
        "value": "2018-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM+WT+Cache+IOG (WT2)",
    "fields": {
      "flops": {
        "value": "3.31E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "53000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qarasu-14B",
    "fields": {
      "numParams": {
        "value": "14000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "5000000000",
        "source": "epoch",
        "citation": "7B tokens or ~5B words"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Greedy layer-wise DNN training",
    "fields": {
      "releaseDate": {
        "value": "2006-12-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DiffDock",
    "fields": {
      "flops": {
        "value": "7.2E+19",
        "source": "epoch",
        "citation": "\"We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).\"4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686"
      },
      "numParams": {
        "value": "20240000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "432",
        "source": "epoch",
        "citation": "18 days"
      },
      "gpuType": {
        "value": "NVIDIA RTX A6000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DARK",
    "fields": {
      "flops": {
        "value": "9.7E+18",
        "source": "epoch",
        "citation": "\"Rounding up to the nearest day, if we were to re-perform DARK from nothing to having a trained DARK3 it would take 12 days when parallelized across ten V100 GPUS. Of that time, model training constitutes just over 3 days and only requires 1 GPU\"3 * 24 * 3600 * 125 teraFLOP/s * 0.3 (utilization) = 9.7e18"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "System 11",
    "fields": {
      "flops": {
        "value": "12930000000",
        "source": "epoch",
        "citation": "Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.Number of passes (Section 2.1):* \"Nearly 1,050 face examples were gathered from face databases [...]\"* \"Fifteen face examples are generated for the training set from each original image\"Training loop:1. \"initial set of nonface images by generating 1,000 random images\"2. Train (presumably on whole set)3. Run + collect false positives4. \"Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2\"\"A typical training run selects approximately 8,000 nonface images \"Selecting 8,000 nonface images implies 8000/250 = 32 loops.Assuming compute is 3 * N * D, we have* Loop 1: D = 15*1050 + 1000* Loop 2: D = 15*1050 + 1000 + 250* So on.Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.Hence compute = 3 * 6452 * 668e3 = 1.3e10."
      },
      "numParams": {
        "value": "6452",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9050",
        "source": "epoch",
        "citation": "\"A typical trainingrun selects approximately 8000 non-face images from the146,212,178 subimages that are available at all locationsand scales in the training scenery images.\"\"Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from eachoriginal image [...]\"\"Create an initial set of non-face images by generating1000 images with random pixel intensities\""
      },
      "releaseDate": {
        "value": "1996-06-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Temporal Convolutional Attention-based Network(TCAN) (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-02-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Minerva (540B)",
    "fields": {
      "flops": {
        "value": "2.74E+24",
        "source": "epoch",
        "citation": "Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hoursMinerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hoursSo fine-tuning added 8.5% more compute.Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOPhttps://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24"
      },
      "numParams": {
        "value": "540350000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "613875000000",
        "source": "epoch",
        "citation": "\"Our models were trained on a dataset of 38.5B tokens\" + PaLM"
      },
      "costDollars": {
        "value": "3267257.75",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "696",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ChatRhino",
    "fields": {
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DANet",
    "fields": {
      "releaseDate": {
        "value": "2019-04-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SparseOPT-13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-1b",
    "fields": {
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TOME",
    "fields": {
      "numParams": {
        "value": "220000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Inflection-1",
    "fields": {
      "flops": {
        "value": "1.00E+24",
        "source": "epoch",
        "citation": "<= 2.5e24They define two \"compute classes\", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)"
      },
      "gpuType": {
        "value": "NVIDIA H100 SXM5",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "bRSM + cache",
    "fields": {
      "flops": {
        "value": "213000000000000",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "2550000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PLUG",
    "fields": {
      "flops": {
        "value": "3.60E+22",
        "source": "epoch",
        "citation": "128 Nvidia A100 for 35 days"
      },
      "numParams": {
        "value": "27000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "840",
        "source": "epoch",
        "citation": "35 days"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM (Hebbian, Cache, MbPA)",
    "fields": {
      "flops": {
        "value": "2.4E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "45200000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "144",
        "source": "epoch",
        "citation": "6 days"
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA P100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-03-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "\u03bb-WASP",
    "fields": {
      "numTokens": {
        "value": "792",
        "source": "epoch",
        "citation": "\"Table 1 summarizes the results at the end of the learning curves (792 training examples for \u03bbWASP, WASP and SCISSOR, 600 for Z&C)\""
      },
      "releaseDate": {
        "value": "2007-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPipe (Amoeba)",
    "fields": {
      "numParams": {
        "value": "557000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1281167",
        "source": "epoch",
        "citation": "Table 4"
      },
      "releaseDate": {
        "value": "2018-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GRU + p-tHSM (pretrain via Brown) (WT103)",
    "fields": {
      "numParams": {
        "value": "206000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Segment Anything Model",
    "fields": {
      "flops": {
        "value": "7.80E+21",
        "source": "epoch",
        "citation": "\"SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of traininglarge scale models. The environmental impact of training the released SAM model is approximately 6963 kWh\"68*256 A100-hours = 17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)= 7.82e21max A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)"
      },
      "numParams": {
        "value": "636000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1100000000",
        "source": "epoch",
        "citation": "\"SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.\"segmentation mask is a map that identifies segments in an image"
      },
      "trainingTimeDays": {
        "value": "68",
        "source": "epoch",
        "citation": "\"SAM was trained on 256 A100 GPUS for 68 hours\""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DETR",
    "fields": {
      "flops": {
        "value": "4E+20",
        "source": "epoch",
        "citation": "\"Training the baseline model for 300 epochs on 16 V100 GPUstakes 3 days, with 4 images per GPU (hence a total batch size of 64). For thelonger schedule used to compare with Faster R-CNN we train for 500 epochswith learning rate drop after 400 epochs. This schedule adds 1.5 AP comparedto the shorter schedule.\"48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20"
      },
      "numParams": {
        "value": "60000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "123000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout (ImageNet)",
    "fields": {
      "flops": {
        "value": "2.73197E+17",
        "source": "epoch",
        "citation": "\"a single NVIDIA GTX 580 GPU... Training on ImageNet takesroughly four days with dropout and two days without.\"1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization"
      },
      "numTokens": {
        "value": "1000000",
        "source": "epoch",
        "citation": "In 2010, a subset of 1000 classeswith roughly 1000 examples per class was the basis of an object recognition competition..."
      },
      "trainingTimeDays": {
        "value": "96",
        "source": "epoch",
        "citation": "4 days with dropout; 2 days without dropout"
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 580",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-06-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LBL",
    "fields": {
      "flops": {
        "value": "501999999999999",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "2000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-06-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MS-CNN",
    "fields": {
      "releaseDate": {
        "value": "2016-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Inception-ResNet-V2",
    "fields": {
      "numParams": {
        "value": "56000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-02-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Variational RHN + WT",
    "fields": {
      "releaseDate": {
        "value": "2016-07-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT-G/14 (LiT)",
    "fields": {
      "numParams": {
        "value": "3005000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4000000000",
        "source": "epoch",
        "citation": "Largest dataset is \"4 billion image and alt-text pairs\". This is rounded down slightly; the other datasets are much smaller."
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WD+LR+M",
    "fields": {
      "releaseDate": {
        "value": "2021-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM2-8M",
    "fields": {
      "flops": {
        "value": "4.8E+19",
        "source": "epoch",
        "citation": "\"All language models were trained for 500K updates, except the 15B language model\" \"All models used 2 million tokens as batch size except the 15B model\"[Supplementary Materials]Hence: 1000B training tokens (500k steps, 2M tokens/batch)Estimate: 8M*2*1000B + 8M*4*1000B"
      },
      "numParams": {
        "value": "8000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MoE-1.1T",
    "fields": {
      "flops": {
        "value": "2.23E+22",
        "source": "epoch",
        "citation": "Reported directly in paper. Authors calculate FLOPs analytically in appendix G"
      },
      "numParams": {
        "value": "1100000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "84000000000",
        "source": "epoch",
        "citation": "112B tokens, or 84B words at 0.75 English words/token. \"We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and theEnglish subset of CC100, totalling 112B tokens\""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-12-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TinyLlama-1.1B (1T token checkpoint)",
    "fields": {
      "flops": {
        "value": "7.25E+21",
        "source": "epoch",
        "citation": "6ND approximation: 6*1.1B * 1T = 6600000000000000000000Based on reported GPU-time and utilization:flops = (num gpu) * (peak flops) * (time in seconds) * (reported utilization rate) = (16) * (312 * 10**12) * (30 * 24 * 3600) * (0.56) = 7245987840000001048576source: https://github.com/jzhang38/TinyLlama\"Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization\"and Releases Schedule from the same link"
      },
      "numParams": {
        "value": "1100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1T tokens checkpoint so around 0.75T words"
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": "1T checkpoint was released after 1 monthsource: https://github.com/jzhang38/TinyLlama"
      },
      "gpuCount": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.56",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2)",
    "fields": {
      "flops": {
        "value": "4.24E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "34000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Poro34B (700B token checkpoint)",
    "fields": {
      "flops": {
        "value": "1.53E+23",
        "source": "epoch",
        "citation": "6ND = 6*0.75T*34B= 153000000000000000000000\"Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.\""
      },
      "numParams": {
        "value": "34000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1T tokens, assuming 0.75 word per token\"Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.\""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "AMD Instinct MI250X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SqueezeNet",
    "fields": {
      "numParams": {
        "value": "1200000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BIG-G 137B",
    "fields": {
      "flops": {
        "value": "5.60E+23",
        "source": "epoch",
        "citation": "\"BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswaniet al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDAarchitectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of webdocuments, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8trillion BPE tokens using a 32k-token SentencePiece vocabulary\"Appendix:\"We use a pre-training batch size of 262k tokens for all models...\"2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)681B * 137B * 6 = 5.6e23"
      },
      "numParams": {
        "value": "137000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fairseq + UID: variance",
    "fields": {
      "releaseDate": {
        "value": "2021-05-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YouTube Video Recommendation System",
    "fields": {
      "numTokens": {
        "value": "10000000000",
        "source": "epoch",
        "citation": "\"We currently handle millions of usersand tens of billions of activity events with a total footprintof several terabytes of data\"If 10M users each watch 1000 videos, that's 10B visualizations, which matches their \"activity events\" count."
      },
      "releaseDate": {
        "value": "2010-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Grover-Mega",
    "fields": {
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Temporal Convolutional Attention-based Network(TCAN) (WT2)",
    "fields": {
      "numParams": {
        "value": "33000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hopfield network",
    "fields": {
      "numParams": {
        "value": "9900",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1982-04-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CT-MoS (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-12-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RL mapping instructions (troubleshooting)",
    "fields": {
      "numParams": {
        "value": "133140",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1327.36",
        "source": "epoch",
        "citation": "Shown at beginning of section 7Total number of documents is 128, average number of actions per document is 10.37"
      },
      "releaseDate": {
        "value": "2009-08-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Neural Architecture Search with base 8 and shared embeddings",
    "fields": {
      "flops": {
        "value": "1.05E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "54000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hanabi 4 player",
    "fields": {
      "flops": {
        "value": "4.3E+18",
        "source": "epoch",
        "citation": "14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP"
      },
      "numParams": {
        "value": "764000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "0.3357748109",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-02-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PNAS-net",
    "fields": {
      "numParams": {
        "value": "86000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Vicuna-13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "259",
        "source": "epoch",
        "citation": "$300 in 2020, adjusted for inflation using BLS.gov inflation calculator"
      },
      "releaseDate": {
        "value": "2023-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (fine-tuned with HYDRA)",
    "fields": {
      "flops": {
        "value": "1.92E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "1540000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Binarized Neural Network (MNIST)",
    "fields": {
      "numParams": {
        "value": "37000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "60k training images, 10k test in MNIST"
      },
      "releaseDate": {
        "value": "2016-03-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MOSS-Moon-003",
    "fields": {
      "flops": {
        "value": "6.67E+22",
        "source": "epoch",
        "citation": "6.67e22 including pre-training for CodeGen:\"The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.\""
      },
      "numParams": {
        "value": "16000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Feedforward NN",
    "fields": {
      "flops": {
        "value": "350000000000000",
        "source": "epoch",
        "citation": "Roughly two times the number of parameters for ops per forward pass. So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14"
      },
      "numParams": {
        "value": "7082000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "0.01292577502",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2010-05-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProgressiveGAN",
    "fields": {
      "releaseDate": {
        "value": "2017-10-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Rita-XLarge",
    "fields": {
      "flops": {
        "value": "3.40E+21",
        "source": "epoch",
        "citation": "\"The models were trained for a total training time of over 25 thousand Nvidia-V100 GPU hours\"125 teraFLOP/s (uncertain which V100 model, tensor performance varies from 112-130tFLOP/s) * 25000 * 3600 * 0.3 (utilization) = 3.4e+21\""
      },
      "numParams": {
        "value": "1200000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sparse all-MLP",
    "fields": {
      "flops": {
        "value": "6.07703E+19",
        "source": "epoch",
        "citation": "112 hours on 32 V100 GPUsassumed 0.33 util rate32*112*60*60*0.3*1.57E+13"
      },
      "numParams": {
        "value": "9400000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "75000000000",
        "source": "epoch",
        "citation": "100B tokens (Table 2) so 75B words."
      },
      "costDollars": {
        "value": "320",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "112",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Alleviated TOI 10 (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2019-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-MemoryAug (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2020-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OmniVec",
    "fields": {
      "releaseDate": {
        "value": "2023-11-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransformerXL + FWL",
    "fields": {
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OmegaPLM",
    "fields": {
      "flops": {
        "value": "1.38E+22",
        "source": "epoch",
        "citation": "\"OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days.\" \"Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.\"Assume 0.4 utilization.Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.4 * = 1.38e2"
      },
      "numParams": {
        "value": "670000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DnCNN",
    "fields": {
      "releaseDate": {
        "value": "2017-02-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLM-2B",
    "fields": {
      "releaseDate": {
        "value": "2021-03-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN Baseline",
    "fields": {
      "releaseDate": {
        "value": "2019-07-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Named Entity Recognition model",
    "fields": {
      "flops": {
        "value": "9.69408E+16",
        "source": "epoch",
        "citation": "8 hours of training for NERGeForce GTX TITAN X GPU0.33 utilization rate"
      },
      "numTokens": {
        "value": "204567",
        "source": "epoch",
        "citation": "Table 2. 204567 tokens"
      },
      "costDollars": {
        "value": "0.6258358486",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "8",
        "source": "epoch",
        "citation": "\"the model training requires about 12 hours for POS tagging and 8hours for NER\""
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX TITAN X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YOLO",
    "fields": {
      "numParams": {
        "value": "271684800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-06-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PeptideBERT",
    "fields": {
      "flops": {
        "value": "7.60E+21",
        "source": "epoch",
        "citation": "\"Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming from Table 1 we have 244 minutes11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,"
      },
      "trainingTimeDays": {
        "value": "4.067",
        "source": "epoch",
        "citation": "244 minues from Table 1"
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Geforce GTX1080 Ti",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
    "fields": {
      "flops": {
        "value": "3.09E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "33000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Maxout Networks",
    "fields": {
      "releaseDate": {
        "value": "2013-02-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensor-Transformer(1core)+PN (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-03-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLIP-2 (Q-Former)",
    "fields": {
      "flops": {
        "value": "1.20E+21",
        "source": "epoch",
        "citation": "https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33"
      },
      "numParams": {
        "value": "1480000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "200",
        "source": "epoch",
        "citation": "\"For example, usinga single 16-A100(40G) machine, our largest model withViT-g and FlanT5-XXL requires less than 6 days for the firststage and less than 3 days for the second stage.\"9 days = 216 hours"
      },
      "gpuCount": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOMZ-176B",
    "fields": {
      "numParams": {
        "value": "176000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "20000000000",
        "source": "epoch",
        "citation": "per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0"
      },
      "releaseDate": {
        "value": "2022-11-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "InternLM2-20b",
    "fields": {
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Persimmon-8B",
    "fields": {
      "numParams": {
        "value": "9300000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "552750000000",
        "source": "epoch",
        "citation": "737B tokens = 552750M words"
      },
      "releaseDate": {
        "value": "2023-09-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Segatron-XL large, M=384 + HCP",
    "fields": {
      "flops": {
        "value": "2.65E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hiero",
    "fields": {
      "numParams": {
        "value": "120000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "171400000",
        "source": "epoch",
        "citation": "[WORDS]155M words dataset for the language model plus (7.2+9.2)M words for the translation model?"
      },
      "releaseDate": {
        "value": "2005-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN-WER",
    "fields": {
      "numParams": {
        "value": "26500000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1100000",
        "source": "epoch",
        "citation": "dataset is 81 hoursAt 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)that's 81*228*60 = 1,108,080another source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/"
      },
      "releaseDate": {
        "value": "2014-06-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ConSERT",
    "fields": {
      "flops": {
        "value": "2.8E+20",
        "source": "epoch",
        "citation": "Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)Foundation model is BeRT with 2.8e+20 FLOP.So total compute is 2.8e+20."
      },
      "numParams": {
        "value": "345000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "0.1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100S PCIe 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MCDNN (MNIST)",
    "fields": {
      "flops": {
        "value": "3.72698E+15",
        "source": "epoch",
        "citation": "Num of multiply-adds per forward pass2 FLOPs/mult-add3 (fp+bp FLOPs / fp FLOPs)800 epochs60.000 training size35 networks\"Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed\""
      },
      "numParams": {
        "value": "1994300",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)"
      },
      "costDollars": {
        "value": "0.08332593349",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-02-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Network in Network",
    "fields": {
      "releaseDate": {
        "value": "2013-12-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AudioLM",
    "fields": {
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "820800000",
        "source": "epoch",
        "citation": "60k hours of English speech13680*60000 = 820800000 wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce"
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "N\u00dcWA",
    "fields": {
      "flops": {
        "value": "4.84E+21",
        "source": "epoch",
        "citation": "From AI Tracker:\"Compute cost: End of Sec 4.1: \"We pre-train on 64 A100 GPUs for two weeks\". Info sheet from NVIDIA (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) gives single precision TensorFloat 32 performance of 156 TFLOPs/s. So we get 64 x 14 x 156 = 140,000 TFLOPs/s x days.\"Multiply by seconds/day and 30% utilization"
      },
      "numParams": {
        "value": "870000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "10446.83687",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "QRNN",
    "fields": {
      "flops": {
        "value": "3.6E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "135000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOM-1.7B",
    "fields": {
      "numParams": {
        "value": "1700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPU DBNs",
    "fields": {
      "flops": {
        "value": "1E+15",
        "source": "epoch",
        "citation": "https://www.getguesstimate.com/models/19602"
      },
      "numParams": {
        "value": "100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000",
        "source": "epoch",
        "citation": "Table 2 shows the running time for processing 1 millionexamples for RBMs of varying size"
      },
      "costDollars": {
        "value": "0.05677990204",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2009-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ByT5-XXL",
    "fields": {
      "flops": {
        "value": "8.10E+22",
        "source": "epoch",
        "citation": "\"Like mT5, we set our sequencelength to 1024 (bytes rather than tokens), and trainfor 1 million steps over batches of 2^20 tokens.\"12.9 billion * 1 million * 2^20 * 6 = ~8.1e22"
      },
      "numParams": {
        "value": "12900000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "1048576",
        "source": "epoch",
        "citation": "\"Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NetTalk (dictionary)",
    "fields": {
      "flops": {
        "value": "27664065000",
        "source": "epoch",
        "citation": "18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word"
      },
      "numParams": {
        "value": "18629",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000",
        "source": "epoch",
        "citation": "\"A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus\""
      },
      "releaseDate": {
        "value": "1987-06-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "H-LSTM+wg+rcp+rcg+wp",
    "fields": {
      "numParams": {
        "value": "800000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tranception",
    "fields": {
      "flops": {
        "value": "7.24E+21",
        "source": "epoch",
        "citation": "Trained using 64 A100 GPUs for two weeks.64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)= 7.24e21"
      },
      "numParams": {
        "value": "700000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "336",
        "source": "epoch",
        "citation": "2 weeks"
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "3D city reconstruction",
    "fields": {
      "releaseDate": {
        "value": "2009-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM2-15B",
    "fields": {
      "flops": {
        "value": "7.35E+22",
        "source": "epoch",
        "citation": "from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOPfrom Arb Research (https://arbresearch.com/files/gen_bio.pdf): \"ESM-2-15B: 270000 updates x 3.2M batch size x 15 B \u201cconnections\u201d x 6. : 7.8e22 FLOPfrom the paper's Supplementary Materials: \"We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.\"60 days x 512 V100s x an imputed 30% utilization\": 1e23 FLOPGeometric mean: 7.35e22"
      },
      "numParams": {
        "value": "15000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1440",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "S4",
    "fields": {
      "flops": {
        "value": "6E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "249000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2-Small+Pixelfly",
    "fields": {
      "releaseDate": {
        "value": "2021-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM(medium)+Sememe+cell",
    "fields": {
      "releaseDate": {
        "value": "2019-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MoCo",
    "fields": {
      "numParams": {
        "value": "375000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-11-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Time-delay neural networks",
    "fields": {
      "releaseDate": {
        "value": "1989-03-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HyenaDNA",
    "fields": {
      "flops": {
        "value": "4.49E+18",
        "source": "epoch",
        "citation": "8 Nvidia A100 (80GB) GPUs, ~300 minutes (figure 3.2)Assuming 40% utilizationEstimate: 78 TFLOP/s * 8 GPUs * (300*60) s * 0.4 = 4.49e18 FLOPs"
      },
      "numParams": {
        "value": "1600000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cube-Space AutoEncoder",
    "fields": {
      "flops": {
        "value": "1.06609E+17",
        "source": "epoch",
        "citation": "(1) * (4113 * 10**9) * (24 * 3600) * (0.3) = 106608960000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = \"For each domain, we searched for 100 iterations (\u224815min/iter, 24 hours total) on a Tesla K80\"4.113 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-k80.c2616"
      },
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": "\"Finally, we tested Mandrill 15-puzzle, a significantly morechallenging 4x4 variant of the sliding tile puzzle (Figure 1).We trained the network with more hyperparameter tuning it-erations (300) and a larger training set (50000). We gener-ated l = 14, 21 instances (20 each) and ran the system (Ta-ble 2, bottom right). \""
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "\"For each domain, we searched for 100 iterations (\u224815min/iter, 24 hours total) on a Tesla K80\""
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-04-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude 2",
    "fields": {
      "flops": {
        "value": "3.87E+24",
        "source": "epoch",
        "citation": "https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY"
      },
      "releaseDate": {
        "value": "2023-07-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StableLM-3B-4E1T",
    "fields": {
      "flops": {
        "value": "6.21E+22",
        "source": "epoch",
        "citation": "\"StableLM-3B-4E1T was trained on the Stability AI cluster across 256 NVIDIA A100 40GB GPUs (AWS P4d instances). Training began on August 23, 2023, and took approximately 30 days to complete.\"256 * 30 * 24* 3600 * 312 trillion * 0.3 utilization (assumption) = 6.21e22"
      },
      "numParams": {
        "value": "2795443200",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "Trained on 1T tokens (~750B words)"
      },
      "batchSize": {
        "value": "4194304",
        "source": "epoch",
        "citation": "\"The batch size is set to 1024 (4,194,304 tokens).\""
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": "approximately 30 days"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "2-layer skip-LSTM + dropout tuning (WT2)",
    "fields": {
      "numParams": {
        "value": "5400000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DIABETES",
    "fields": {
      "numParams": {
        "value": "429409",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1991-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLOOM-560M",
    "fields": {
      "numParams": {
        "value": "560000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Go-explore",
    "fields": {
      "releaseDate": {
        "value": "2020-04-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Integer Transformer",
    "fields": {
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LMSI-Palm",
    "fields": {
      "numParams": {
        "value": "540000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NAS+ESS (23M)",
    "fields": {
      "releaseDate": {
        "value": "2020-05-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLSTM for handwriting (2)",
    "fields": {
      "numParams": {
        "value": "100881",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2007-12-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Guanaco-65B",
    "fields": {
      "numParams": {
        "value": "65000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "24",
        "source": "epoch",
        "citation": "24 hours"
      },
      "releaseDate": {
        "value": "2023-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HRA",
    "fields": {
      "releaseDate": {
        "value": "2017-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VGG16",
    "fields": {
      "flops": {
        "value": "9.25344E+18",
        "source": "epoch",
        "citation": "2.5 weeks * 4 Titan Black GPUs * 0.30 utilizationSection 3.3: \"On a system equipped withfour NVIDIA Titan Black GPUs, training a single net took 2\u20133 weeks depending on the architecture.\""
      },
      "numParams": {
        "value": "138000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1300000",
        "source": "epoch",
        "citation": "\"In this section, we present the image classification results achieved by the describedConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012\u20132014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).\""
      },
      "costDollars": {
        "value": "82.80013614",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GTX Titan Black",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Advantage Learning",
    "fields": {
      "releaseDate": {
        "value": "2015-12-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EXAONE 2.0",
    "fields": {
      "numParams": {
        "value": "300000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cascaded LNet-ANet",
    "fields": {
      "releaseDate": {
        "value": "2014-11-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Primer",
    "fields": {
      "flops": {
        "value": "7.10E+21",
        "source": "epoch",
        "citation": "From the email they claim to have use 72K TPUv4 hours for trainingThus: 72000 h * 0.1 * 275e12 FLOP/s 3600s/h = 7.1e21 FLOP"
      },
      "numParams": {
        "value": "1900000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "173284750600",
        "source": "epoch",
        "citation": "In GB - TODO convert to words\"Dataset size: 806.92 GiB\"https://www.tensorflow.org/datasets/catalog/c4This was the largest dataset that the authors used \"These benefits are robust and hold across model sizes (20Mto 1.9B parameters), across compute scales (10 to 105accelerator hours), across datasets (LM1B,C4, PG19 [22])\"802.92 GiB ~ 866.42 GB1 GB ~ 200M words"
      },
      "costDollars": {
        "value": "9690.724669",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cognitron",
    "fields": {
      "releaseDate": {
        "value": "1975-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Qwen-7B",
    "fields": {
      "flops": {
        "value": "1.01E+23",
        "source": "epoch",
        "citation": "2.4T tokens per Table 17b*2.4T*6 = 1.01e23"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "Table 1"
      },
      "releaseDate": {
        "value": "2023-09-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RedPajama-INCITE-7B-Base",
    "fields": {
      "flops": {
        "value": "4.10E+22",
        "source": "epoch",
        "citation": "Trained over 1.001 trillion tokens.6.9b * 1 trillion * 6 = 4.1e22"
      },
      "numParams": {
        "value": "6900000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "900000000000",
        "source": "epoch",
        "citation": "1.2 trillion, or 900b words at 0.75 words/token"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "\"global batch size 4M tokens\""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GOAT",
    "fields": {
      "flops": {
        "value": "7.80E+22",
        "source": "epoch",
        "citation": "[Final calculation](8 TPUs)(4.20e14 FLOP/s)(0.1 utilisation rate)(32 agents)(7.3e6 s/agent) = 7.8e22 FLOPs==========================NOTES BELOW[Hardware]- \"Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.\"- TPUv3 (half precision): 4.2e14 FLOP/s- Number of TPUs: 8- Utilisation rate: 0.1[Timesteps]- Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.- 3.9e11 / 5e4 = 8e6 s \u2192 ~93 days- 100 million steps is equivalent to 30 minutes of wall-clock time in our setup. (pg 29, fig 27)- 1e8 steps \u2192 0.5h- 3.9e11 steps \u2192 1950h \u2192 7.0e6 s \u2192 ~82 days- Both of these seem like overestimates, because:\u201cFinally, on the largest timescale (days), generational training iteratively improves population performance by bootstrapping off previous generations, whilst also iteratively updating the validation normalised percentile metric itself.\u201d (pg 16)- Suggests that the above is an overestimate of the number of days needed, else they would have said (months) or (weeks)?- Final choice (guesstimate): 85 days = 7.3e6 s[Population size]- 8 agents? (pg 21) \u2192 this is describing the case where they\u2019re not using PBT, so ignore this number- The original PBT paper uses 32 agents for one task https://arxiv.org/pdf/1711.09846.pdf (in general it uses between 10 and 80)- (Guesstimate) Average population size: 32"
      },
      "numParams": {
        "value": "3500000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "390000000000",
        "source": "epoch",
        "citation": "Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent."
      },
      "costDollars": {
        "value": "122418.967",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-07-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Xinghan Foundation Model",
    "fields": {
      "releaseDate": {
        "value": "2023-10-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "T5-3B",
    "fields": {
      "flops": {
        "value": "8.65865E+20",
        "source": "epoch",
        "citation": "Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodbHowever, this seems dubiously high.\"We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.\"\"In total, this batch size and number of steps corresponds to pre-training on 2^35 \u2248 34B tokens.\"\"To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432\"Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOPhttps://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5"
      },
      "numParams": {
        "value": "2800000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "25500000000",
        "source": "epoch",
        "citation": "\"This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but alsocomprises reasonably clean and natural English text. We dub this data set the \u201cColossal Clean Crawled Corpus\u201d (or C4 for short) and release it as part of TensorFlow Datasets\"750GB * 200M word/GB = 1.5e11\"In total, this batch size and number of steps corresponds to pre-training on 2^35 \u2248 34B tokens.\"\"Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.\"The fraction is 25.5 billion / 150 billion = 0.17 epochs."
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CPM-2",
    "fields": {
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "444100000000",
        "source": "epoch",
        "citation": "\"We pre-train our model on WuDaoCorpus (Yuan et al., 2021), which contains 2.3TB cleaned Chinese data as well as 300GB cleaned English data.\"2300*167 million + 300*200 million = 444,100,000,000 (444 billion)https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0"
      },
      "releaseDate": {
        "value": "2021-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeWhisperer",
    "fields": {
      "releaseDate": {
        "value": "2022-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pluribus",
    "fields": {
      "flops": {
        "value": "6.6E+16",
        "source": "epoch",
        "citation": "Trained in 8 days on a 64 core CPUhttps://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/\"We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.\"Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core. https://epochai.org/blog/estimating-training-compute8 days, 64 cores, 5e9 FLOP/s, 30% utilization"
      },
      "releaseDate": {
        "value": "2019-07-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KwaiYii",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Swin Transformer V2",
    "fields": {
      "flops": {
        "value": "1.10E+21",
        "source": "epoch",
        "citation": "trained on \"<0.5k\" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)A core is 123/2 teraflops500 core-days= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization~= 1.1e21"
      },
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNeXt-101 Billion-scale",
    "fields": {
      "numParams": {
        "value": "193000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SNARC",
    "fields": {
      "numParams": {
        "value": "40",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1952-01-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MMLSTM",
    "fields": {
      "flops": {
        "value": "2.32E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "75000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM2-150M",
    "fields": {
      "flops": {
        "value": "1.10E+21",
        "source": "epoch",
        "citation": "from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 1.1e21 FLOP"
      },
      "numParams": {
        "value": "150000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Ferret (13B)",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "120",
        "source": "epoch",
        "citation": "\"The training takes \u223c5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.\""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LiMoE",
    "fields": {
      "flops": {
        "value": "1.80E+22",
        "source": "epoch",
        "citation": "Section 3.2: \"The model contains 5.6B parameters in total, but only applies 675M parameters per token\"From Section A.3, \"batch size 21502 with resolution 288 and text sequence length16\". \"The model was trained for 700k steps pre-cooldown. There was one cooldown of length 125k stepsfrom the final step, and 3 of length 40k steps starting from step 650k\". Patch size 14 for images.Assume C = 6*N*D. C = 6*675e6*21.5e3*1e6*(16+(288/14)**2)/2 = 1.8e22This is broadly consistent with ViT-H/14's compute"
      },
      "numParams": {
        "value": "5600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "7600000000",
        "source": "epoch",
        "citation": "Section 3: \"Training data. By default, all models are trained on paired image-text data used in [16], consisting of 3.6B images and alt-texts scraped from the web. For large LIMoE-H/14 experiment, we also co-train with JFT-4B [17]. \""
      },
      "releaseDate": {
        "value": "2022-06-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BigChaos 2008",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\""
      },
      "releaseDate": {
        "value": "2008-11-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PolyCoder",
    "fields": {
      "flops": {
        "value": "1.10E+21",
        "source": "epoch",
        "citation": "\"We use GPT-NeoX toolkit 11 totrain the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The walltime used to train the largest 2.7B model is about 6 weeks\"8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21"
      },
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1000",
        "source": "epoch",
        "citation": "6 weeks"
      },
      "gpuType": {
        "value": "NVIDIA Quadro RTX 8000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-02-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WizardCoder-15.5B",
    "fields": {
      "numParams": {
        "value": "15500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FTW",
    "fields": {
      "flops": {
        "value": "7.26E+21",
        "source": "epoch",
        "citation": "We assume that most operations happen in the visual embedding.2* 84^2*84^2 * 32 * 3 / 1^2 = 9.5 *10^9new image size: 76 x 76 x 32ignore ReLU/additions becaue probably very little influence 2 * 76^2 * 76^2 * 10* 64 = 4 *10^10new image size: 72 x 72 x 642 * 72^2 *72^2 * 64 * 64 * 3=  6.6 * 10^11new image size: 69 x 69 x 642 * 69^2 *69^2 * 64 * 64 * 3=  5.5 * 10^11new image size: 66 x 66 x 64Linear layer: 2* ( 66*66*64)*256 = 1.4*10^8Total aprox: 1.21e+12 FLOP/forward pass"
      },
      "numParams": {
        "value": "126001330",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "21045.01655",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-07-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Code Llama-70B",
    "fields": {
      "flops": {
        "value": "1.23E+24",
        "source": "epoch",
        "citation": "Finetune compute for 70B model: 1T tokens of code *"
      },
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000000000",
        "source": "epoch",
        "citation": "Llama 70B training dataset was 2 trillion tokens. Code Llama finetuning dataset was 1 trillion tokens of code."
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. \"We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.\"Subsequent fine-tuning batch sizes are 500k-1M. \"For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default.\" "
      },
      "trainingTimeDays": {
        "value": "6480",
        "source": "epoch",
        "citation": "Assuming Code Llama 70B training continued on same hardware as Llama 2 70B."
      },
      "gpuCount": {
        "value": "400",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.435",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "S-Norm",
    "fields": {
      "numTokens": {
        "value": "1500000000",
        "source": "epoch",
        "citation": "\"530k question-document training pairs\"average question length of 14 words and document length of 2895 words, per https://www.cs.utexas.edu/~eunsol/files/papers/acl17jcwz.pdf530,000 * 2900 = ~1,500,000,000"
      },
      "releaseDate": {
        "value": "2017-10-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout-LSTM+Noise(Laplace)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlexNet",
    "fields": {
      "flops": {
        "value": "4.7E+17",
        "source": "epoch",
        "citation": "1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) = 470 PF = 0.0054 pfs-daysSource: https://openai.com/blog/ai-and-compute/"
      },
      "numParams": {
        "value": "60000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1200000",
        "source": "epoch",
        "citation": "\"ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon\u2019s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.\""
      },
      "costDollars": {
        "value": "7.999913721",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 580",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-09-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM+GraB",
    "fields": {
      "releaseDate": {
        "value": "2022-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-2.7B (finetuned on WT2)",
    "fields": {
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Yuan 1.0",
    "fields": {
      "flops": {
        "value": "3.54E+23",
        "source": "epoch",
        "citation": "Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOPhttps://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day"
      },
      "numParams": {
        "value": "245730000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000000000",
        "source": "epoch",
        "citation": "\"Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.\"1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words."
      },
      "batchSize": {
        "value": "6881280",
        "source": "epoch",
        "citation": "Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280"
      },
      "costDollars": {
        "value": "606364.7479",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "2128",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.45",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaGo Zero",
    "fields": {
      "flops": {
        "value": "3.41E+23",
        "source": "epoch",
        "citation": "source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-secondsTotal compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOPSo similar to the Cotra and Davidson estimate (within a factor of 2)."
      },
      "numParams": {
        "value": "46400244",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "5800000000",
        "source": "epoch",
        "citation": "\"Over the course of training, 29 million games of self-play were generated\"Approx 200 moves per Go game on averagehttps://homepages.cwi.nl/~aeb/go/misc/gostat.htmlThus 200 * 29e6 = 5.8e9"
      },
      "costDollars": {
        "value": "1544149.418",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "480",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v1",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-10-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hybrid H3-125M",
    "fields": {
      "numParams": {
        "value": "125000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Seq2Seq LSTM",
    "fields": {
      "flops": {
        "value": "5.6E+19",
        "source": "epoch",
        "citation": "384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOPTimes 5 independent models in ensemble => 5.6E+19 FLOPIf we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP"
      },
      "numParams": {
        "value": "1920000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "652000000",
        "source": "epoch",
        "citation": "[WORDS]\"We used the WMT\u201914 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean \u201cselected\u201dsubset from [29].\""
      },
      "costDollars": {
        "value": "79.59599485",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-09-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DrLIM",
    "fields": {
      "numParams": {
        "value": "37097",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "217470",
        "source": "epoch",
        "citation": "\"The dataset was split into 660 training images and a 312test images. The result of training on all 10989 similar pairsand 206481 dissimilar pairs is a 3-dimensional manifold inthe shape of a cylinder (see figure 8).\"206481 + 10989 = 217470"
      },
      "releaseDate": {
        "value": "2006-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "xTrimoPGLM -100B",
    "fields": {
      "flops": {
        "value": "6.00E+23",
        "source": "epoch",
        "citation": "\"xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8\u00d740G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date,xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible\"6 * 100 billion params * 1T tokens = 6e238*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24"
      },
      "numParams": {
        "value": "1820000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "3912",
        "source": "epoch",
        "citation": "163 days"
      },
      "gpuCount": {
        "value": "768",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SARA-RT-2",
    "fields": {
      "numParams": {
        "value": "5000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MEB",
    "fields": {
      "numParams": {
        "value": "135000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Zi Yue",
    "fields": {
      "releaseDate": {
        "value": "2023-07-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OtterHD-8B",
    "fields": {
      "numParams": {
        "value": "8000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "3",
        "source": "epoch",
        "citation": "3 hours"
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hybrid H3-2.7B",
    "fields": {
      "flops": {
        "value": "8.49E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Symmetric Residual Encoder-Decoder Net",
    "fields": {
      "releaseDate": {
        "value": "2016-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Diplodocus",
    "fields": {
      "releaseDate": {
        "value": "2022-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SENet (ImageNet)",
    "fields": {
      "numParams": {
        "value": "28100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-09-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ChatGPT (gpt-3.5-turbo)",
    "fields": {
      "numParams": {
        "value": "20000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-1.4b",
    "fields": {
      "numParams": {
        "value": "1400000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MoE",
    "fields": {
      "flops": {
        "value": "9.39391E+19",
        "source": "epoch",
        "citation": "12 days 64 NVIDIA K40 GPUS (see hardware data sheet for performance)0.33 util rate"
      },
      "numParams": {
        "value": "8700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "100000000000",
        "source": "epoch",
        "citation": "[WORDS]\"We constructed a similar training set consisting of shuffled unique sentences from Google\u2019s internalnews corpus, totalling roughly 100 billion words\""
      },
      "costDollars": {
        "value": "8484.354244",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "288",
        "source": "epoch",
        "citation": "12 days"
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K40t",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-01-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout (2014)",
    "fields": {
      "releaseDate": {
        "value": "2014-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Skywork-13B",
    "fields": {
      "flops": {
        "value": "2.50E+23",
        "source": "epoch",
        "citation": "\"Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.\"They note that \"we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... \". \"MFU\" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode."
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2780000000000",
        "source": "epoch",
        "citation": "The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion."
      },
      "batchSize": {
        "value": "16000000",
        "source": "epoch",
        "citation": "Table 3"
      },
      "trainingTimeDays": {
        "value": "940",
        "source": "epoch",
        "citation": "39 days"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A800",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.46",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Weblab-10B",
    "fields": {
      "flops": {
        "value": "3.60E+22",
        "source": "epoch",
        "citation": "6ND = 10B*600B * 6 = 3.6e22\" The model was trained on around 600B tokens from a mixture of the following corpora.\"See also: https://weblab.t.u-tokyo.ac.jp/en/100%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%BA%E3%83%BB%E6%97%A5%E8%8B%B12%E3%83%B6%E5%9B%BD%E8%AA%9E%E5%AF%BE%E5%BF%9C%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1/"
      },
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "600000000000",
        "source": "epoch",
        "citation": "600B tokens , assuming 1 word per token"
      },
      "releaseDate": {
        "value": "2023-08-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "data2vec (language)",
    "fields": {
      "numParams": {
        "value": "705134592",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3300000000",
        "source": "epoch",
        "citation": "Section 5.3: \"weadopt the same training setup as BERT (Devlin et al., 2019)by pre-training on the Books Corpus (Zhu et al., 2015) andEnglish Wikipedia data over 1M updates and a batch sizeof 256 sequences.\""
      },
      "releaseDate": {
        "value": "2022-01-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Feedback Transformer",
    "fields": {
      "flops": {
        "value": "4.41E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "126000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mid-level Features",
    "fields": {
      "releaseDate": {
        "value": "2010-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MAGNeT",
    "fields": {
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-13B (LoRA finetuned)",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PanGu-\u03a3",
    "fields": {
      "flops": {
        "value": "4.67E+23",
        "source": "epoch",
        "citation": "It has sparse architecture, so we can't use C=6ND.\"We develop PanGu-\u03a3 model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.\"100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOPhttps://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33"
      },
      "numParams": {
        "value": "1085000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "246750000000",
        "source": "epoch",
        "citation": "329B tokens ~= 247B words"
      },
      "batchSize": {
        "value": "524288",
        "source": "epoch",
        "citation": "\"We train PanGu-\u03a3 with global batch size of 512 with sequence length of 1024 for each sample\""
      },
      "trainingTimeDays": {
        "value": "2400",
        "source": "epoch",
        "citation": "We develop PanGu-\u03a3 model under the framework of MindSpore 5and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days."
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Huawei Ascend 910",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CT-MoS + DynamicEval (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2020-12-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNS-RNN",
    "fields": {
      "flops": {
        "value": "3.15E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "5660000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ATLAS",
    "fields": {
      "flops": {
        "value": "3.82579E+19",
        "source": "epoch",
        "citation": "flops = (8) * (123 * 10**12) * (36 * 3600) * (0.3)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from Appendix A.2: \"Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.\"so 36h for T5\"Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for BART models.\"from https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_chiptpu chip have peak flops 123 teraflopsso 8 chips have peak flops 123 * 8"
      },
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "36",
        "source": "epoch",
        "citation": "Appendix A.2: Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively."
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MnasNet-A3",
    "fields": {
      "flops": {
        "value": "1.50E+21",
        "source": "epoch",
        "citation": "\"each architecture search takes 4.5 days on 64 TPUv2 devices\"This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPUAssuming a 33% utilization rate:4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOPHowever, it is unclear if \"64 TPUv2 devices\" refers to chips or modules, so the true compute might be 1/4 of this amount."
      },
      "numParams": {
        "value": "5200000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": "\"In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. \""
      },
      "costDollars": {
        "value": "4330.997218",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "108",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LaMDA",
    "fields": {
      "flops": {
        "value": "3.55E+23",
        "source": "epoch",
        "citation": "\"The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days= 3.55E+23\"From https://arxiv.org/pdf/2201.08239.pdf p.18"
      },
      "numParams": {
        "value": "137000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1560000000000",
        "source": "epoch",
        "citation": "\"and are pre-trained on 1.56T words of public dialog data and web text\""
      },
      "batchSize": {
        "value": "256000",
        "source": "epoch",
        "citation": "\"All models were trained with 256K tokens per batch\""
      },
      "costDollars": {
        "value": "484957.2043",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1385",
        "source": "epoch",
        "citation": "57.7 days * 24"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.565",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-02-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Consistency Decoder",
    "fields": {
      "releaseDate": {
        "value": "2023-11-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BTLM-3B",
    "fields": {
      "flops": {
        "value": "9.80E+21",
        "source": "epoch",
        "citation": "2.6b params * 627b tokens * 6 = 9.8e21"
      },
      "numParams": {
        "value": "2600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "470000000000",
        "source": "epoch",
        "citation": "627B tokens, equivalent to 470B english words"
      },
      "batchSize": {
        "value": "3932160",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Cerebras CS-2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Galactica",
    "fields": {
      "flops": {
        "value": "3.24E+23",
        "source": "epoch",
        "citation": "Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23"
      },
      "numParams": {
        "value": "120000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "2000000",
        "source": "epoch",
        "citation": "Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mogrifier (d2, MC) + dynamic eval",
    "fields": {
      "releaseDate": {
        "value": "2019-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SVD in recommender systems",
    "fields": {
      "releaseDate": {
        "value": "2000-07-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RL mapping instructions (games)",
    "fields": {
      "numParams": {
        "value": "80940",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "293",
        "source": "epoch",
        "citation": "Shown at beginning of section 7Total number of documents is 50, average number of actions per document is 5.86source: https://en.wikipedia.org/wiki/Netflix_Prize"
      },
      "releaseDate": {
        "value": "2009-08-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NLM",
    "fields": {
      "flops": {
        "value": "7.36E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "515000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HyperCLOVA X",
    "fields": {
      "releaseDate": {
        "value": "2023-08-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PAR Transformer Large",
    "fields": {
      "releaseDate": {
        "value": "2020-09-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sparse Wide GPT-3 Small",
    "fields": {
      "flops": {
        "value": "8.84E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "1300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Optimized Single-layer Net",
    "fields": {
      "releaseDate": {
        "value": "2011-04-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StripedHyena-Hessian-7B",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-70m",
    "fields": {
      "numParams": {
        "value": "70000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YuLan-Chat-2 (13B)",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT-Huge/14",
    "fields": {
      "flops": {
        "value": "4.26E+21",
        "source": "epoch",
        "citation": "from Table 6"
      },
      "numParams": {
        "value": "632000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.32",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-DOC (fin) (37M)",
    "fields": {
      "releaseDate": {
        "value": "2018-08-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LRCN",
    "fields": {
      "numParams": {
        "value": "142552000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "40000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-11-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DiT-XL/2 + CADS",
    "fields": {
      "numParams": {
        "value": "675000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "2-layer-LSTM+Deep-Gradient-Compression",
    "fields": {
      "flops": {
        "value": "1.34E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "6020000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DLRM-2021",
    "fields": {
      "flops": {
        "value": "3E+20",
        "source": "epoch",
        "citation": "Figure 1https://arxiv.org/abs/2104.05158"
      },
      "numParams": {
        "value": "1000000000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "1094.917862",
        "source": "epoch",
        "citation": "https://bdtechtalks.com/2020/02/03/google-meena-chatbot-ai-language-model/"
      },
      "releaseDate": {
        "value": "2020-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CRF-RNN",
    "fields": {
      "releaseDate": {
        "value": "2015-02-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cloob",
    "fields": {
      "numTokens": {
        "value": "15000000",
        "source": "epoch",
        "citation": "[Image-text pairs]\"To be comparable to the CLIP results, we use the same subset of 15 million samples from the YFCC100M dataset\""
      },
      "releaseDate": {
        "value": "2021-10-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RHN+HSG(depth=40)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OntoProtein",
    "fields": {
      "numParams": {
        "value": "420000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CD-GraB (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2023-02-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RSM",
    "fields": {
      "releaseDate": {
        "value": "2019-05-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GANs",
    "fields": {
      "flops": {
        "value": "5.184E+17",
        "source": "epoch",
        "citation": "From https://openai.com/blog/ai-and-compute/ Appendix\"Less than 0.006 pfs-days\"(86400*10^15*0.006)Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper."
      },
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "\"We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].\"MNIST has 60k images https://en.wikipedia.org/wiki/MNIST_databaseTFD seems to have 2925 examples (?)https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdfCIFAR-10 has 60k imageshttps://www.cs.toronto.edu/~kriz/cifar.html"
      },
      "costDollars": {
        "value": "6.086988417",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deep RNN",
    "fields": {
      "releaseDate": {
        "value": "2013-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WeNet (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2019-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Hybrid H3-355M",
    "fields": {
      "numParams": {
        "value": "355000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout (CIFAR)",
    "fields": {
      "flops": {
        "value": "4.2687E+15",
        "source": "epoch",
        "citation": "\"a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes\" p171.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization"
      },
      "trainingTimeDays": {
        "value": "1.5",
        "source": "epoch",
        "citation": "90 minutes"
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 580",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2012-06-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PaLI",
    "fields": {
      "flops": {
        "value": "5.10E+22",
        "source": "epoch",
        "citation": "\"The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days\"275 teraFLOP/s * 1024 * 7 * 24 * 3600 * 0.3 (utilization assumption) = 5.1e22"
      },
      "numParams": {
        "value": "16900000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "7 days"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-09-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNeXt-50",
    "fields": {
      "numParams": {
        "value": "25000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ZymCTRL",
    "fields": {
      "flops": {
        "value": "5.05E+21",
        "source": "epoch",
        "citation": "\"We trained for 179,000 steps on 48 NVIDIA A100s 80GB for about 15,000 GPU hours\"15000  * 3600 * 312 teraFLOPS * 0.3 (utilization assumption) = 5.05e21"
      },
      "numParams": {
        "value": "738000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-175B",
    "fields": {
      "flops": {
        "value": "4.30E+23",
        "source": "epoch",
        "citation": "https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md\"As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute\""
      },
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "135000000000",
        "source": "epoch",
        "citation": "\"The training data contains 180B tokens corresponding to 800 GB of data\"1 token ~ 0.75 words"
      },
      "batchSize": {
        "value": "2000000",
        "source": "epoch",
        "citation": "Table 1"
      },
      "costDollars": {
        "value": "1654082.504",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "793.5",
        "source": "epoch",
        "citation": "4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hourshttps://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29\"As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).\""
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.47115",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MADLAD-400 10B",
    "fields": {
      "flops": {
        "value": "1.61E+22",
        "source": "epoch",
        "citation": "6ND = 10.7B * 250B = 1.6e22'MADLAD-400-10B-MT is a multilingual machine translation model based on the T5 architecture that was trained on 250 billion tokens covering over 450 languages using publicly available data. '10.7B  params from appendix A.8"
      },
      "numParams": {
        "value": "10700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3000000000000",
        "source": "epoch",
        "citation": "Assuming 1 word per token we have 3T words 'We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.'"
      },
      "releaseDate": {
        "value": "2023-09-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Grown to Prune Two-layer stacked LSTM",
    "fields": {
      "releaseDate": {
        "value": "2020-07-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ReLU (LFW)",
    "fields": {
      "releaseDate": {
        "value": "2010-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stanley (DARPA Grand Challenge 2)",
    "fields": {
      "releaseDate": {
        "value": "2006-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NTM",
    "fields": {
      "releaseDate": {
        "value": "2014-12-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Relational Memory Core",
    "fields": {
      "releaseDate": {
        "value": "2018-06-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Selfish-RNN (AWD-LSTM-MoS)",
    "fields": {
      "releaseDate": {
        "value": "2021-01-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Ankh_base",
    "fields": {
      "flops": {
        "value": "2.60E+21",
        "source": "epoch",
        "citation": "Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf"
      },
      "numParams": {
        "value": "450000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ViT-G (model soup)",
    "fields": {
      "flops": {
        "value": "3.40E+21",
        "source": "epoch",
        "citation": "This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon. Fine-tuning compute is likely minor in comparision."
      },
      "numParams": {
        "value": "1843000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN + char3-MS-vec",
    "fields": {
      "releaseDate": {
        "value": "2019-07-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL+AdamP",
    "fields": {
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TRIMELMext (247M)",
    "fields": {
      "flops": {
        "value": "3.12E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gemini Nano-2",
    "fields": {
      "numParams": {
        "value": "3250000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v5e",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNNsearch-50*",
    "fields": {
      "flops": {
        "value": "1.5552E+18",
        "source": "epoch",
        "citation": "From https://openai.com/blog/ai-and-compute/ Appendix.0.018 pfs-days(86400*10^15*0.018)252 hours in a Quadro K-6000 GPU"
      },
      "numTokens": {
        "value": "348000000",
        "source": "epoch",
        "citation": "[WORDS]\"WMT \u201914 contains the following English-French parallel corpora: Europarl (61M words), newscommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size ofthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).\""
      },
      "costDollars": {
        "value": "81.47585579",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Image-to-image cGAN",
    "fields": {
      "releaseDate": {
        "value": "2016-11-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TripletRes",
    "fields": {
      "releaseDate": {
        "value": "2019-08-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Sequence-based pattern recognition",
    "fields": {
      "releaseDate": {
        "value": "1955-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Polyglot-Ko-12.8B",
    "fields": {
      "flops": {
        "value": "1.28E+22",
        "source": "epoch",
        "citation": "trained for 167 billion tokens167b * 12.8b * 6 = 1.28e22"
      },
      "numParams": {
        "value": "12898631680",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "96000000000",
        "source": "epoch",
        "citation": "863 GB of Korean language data after processing~111m Korean words per GB, so ~95,793,000,000 or ~96B wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0"
      },
      "batchSize": {
        "value": "554817",
        "source": "epoch",
        "citation": "from HuggingFace: \"Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework.\"from the paper: \"The overall batch size was maintained through the use of gradient accumulation steps (GAS). The model was trained for a total of 301,000 steps.\"GAS is a technique to train larger batches if you have limited memory. I don't think this text says anything in particular about whether the batch sizes changed over the course of training? 167B / 301k = 554,817"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPALM + RelationLM",
    "fields": {
      "numParams": {
        "value": "124000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TCN (13M)",
    "fields": {
      "releaseDate": {
        "value": "2018-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Semantic Taxonomy Induction",
    "fields": {
      "numParams": {
        "value": "100",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "850750",
        "source": "epoch",
        "citation": "[Classification task]The labeled training set isconstructed by labeling the collected feature vectors as positive \u201cknown hypernym\u201d or negative\u201cknown non-hypernym\u201d examples using WordNet2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairswere labeled as negative training examples.800,828 + 49,922 = 850750"
      },
      "releaseDate": {
        "value": "2006-07-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Recursive sentiment autoencoder",
    "fields": {
      "releaseDate": {
        "value": "2011-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProGen2-xlarge",
    "fields": {
      "flops": {
        "value": "1.35E+22",
        "source": "epoch",
        "citation": "Estimate 1:\"350,000 steps x 1m batch size x 6.4 B \u201cconnections\u201d x 6\" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)Steps and batches from Table 1. FLOP estimate: 1.3e22Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdfFLOP estimate: 1.4e22Geometric mean = 1.35e22 FLOP"
      },
      "numParams": {
        "value": "6400000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Theseus",
    "fields": {
      "flops": {
        "value": "40",
        "source": "epoch",
        "citation": "The \"training\" consists on the mouse running around and checking each wall."
      },
      "numParams": {
        "value": "40",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "40",
        "source": "epoch",
        "citation": "Each wall Theseus bumps into is a datapoint"
      },
      "releaseDate": {
        "value": "1950-07-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "T5-11B",
    "fields": {
      "flops": {
        "value": "3.30E+22",
        "source": "epoch",
        "citation": "https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdfTable 4, 4.05e22update: 3.3e22 per FLAN paper from Google https://arxiv.org/pdf/2210.11416.pdf"
      },
      "numParams": {
        "value": "11000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "150000000000",
        "source": "epoch",
        "citation": "\"This produces a collection of text that is not onlyorders of magnitude larger than most data sets used for pre-training (about 750 GB) but alsocomprises reasonably clean and natural English text. We dub this data set the \u201cColossalClean Crawled Corpus\u201d (or C4 for short) and release it as part of TensorFlow Datasets\"750GB * 200M word/GB = 1.5e11"
      },
      "batchSize": {
        "value": "65536",
        "source": "epoch",
        "citation": "\"We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we \u201cpack\u201d multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens\""
      },
      "costDollars": {
        "value": "105686.2001",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "481.9",
        "source": "epoch",
        "citation": "4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hourshttps://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29"
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.3707",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Nemotron-3-8B",
    "fields": {
      "flops": {
        "value": "1.80E+23",
        "source": "epoch",
        "citation": "https://huggingface.co/nvidia/nemotron-3-8b-base-4k\"This model was trained on a dataset containing 3.8 Trillion tokens of text\"8 billion * 3.8 trillion * 6 = 1.8e23Also, using the hardware method: \"1,024 A100s were used for 19 days to train the model.\"19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23"
      },
      "numParams": {
        "value": "8000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "456",
        "source": "epoch",
        "citation": "19 days"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.34",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "fastText",
    "fields": {
      "releaseDate": {
        "value": "2016-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RetinaNet-R101",
    "fields": {
      "flops": {
        "value": "2.06539E+18",
        "source": "epoch",
        "citation": "\"We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.\"NVIDIA M40 GPU35*60**2*0.3*8*6.83E+12 = 2.07e18"
      },
      "numParams": {
        "value": "53000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "135000",
        "source": "epoch",
        "citation": "trainval135k split"
      },
      "trainingTimeDays": {
        "value": "35",
        "source": "epoch",
        "citation": "\"We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.\"NVIDIA M40 GPU35*60**2*0.3*8*6.83E+12 = 2.07e18"
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA M40",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BEIT-3",
    "fields": {
      "numParams": {
        "value": "1900000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-08-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "L_UL-seq",
    "fields": {
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-08-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Distilled Grandmaster",
    "fields": {
      "flops": {
        "value": "1.04E+22",
        "source": "epoch",
        "citation": "10356718320000000065536 FLOP\"Board states \ud835\udc60 are encoded as FEN strings which we convert to fixed-length strings of 77 characters where the ASCII-code of each character is one token.\" so 77 tokens for board + 1 token for action \"For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates\"so input is 78 tokens for each action-valuenumber of tokens = 1194960000000.0The model is dense transformer\" We train for 10 millionsteps, which corresponds to 2.67 epochs for a batchsize of 4096 with 15.32B data points \", but in appendix A.2 there is mention of 5.35 epochsI have used higher value from 5.35 and 2.67,Probably final they trained model for 5.35 epochs and used checkpoint from 2.67 as final model.aproximation 6ND for 5.35 epochs = 6*270e6*1194960000000.0 * 5.35 =  10356718320000000065536"
      },
      "numParams": {
        "value": "270000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "15320000000",
        "source": "epoch",
        "citation": "15.32BTraining is supervised. I count each action-value (board state, action and numeric evaluation of state from Stockfish 16) as 1 data point.\"For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates\""
      },
      "releaseDate": {
        "value": "2024-02-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-13B",
    "fields": {
      "flops": {
        "value": "4.55E+22",
        "source": "epoch",
        "citation": "1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22from paper, Llama-7B took 135,168 GPU hours using A100s312 trillion * 135,168 * 3600 * 0.3 = 4.55e22 FLOP"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM+Behaviorial-Gating",
    "fields": {
      "releaseDate": {
        "value": "2019-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DOC + Finetune\u2217 + Partial Shuffle (WT2)",
    "fields": {
      "numParams": {
        "value": "67300000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-03-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer+Recurrent Windows of Context",
    "fields": {
      "flops": {
        "value": "1.17E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "124000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Megatron-LM (2.5B)",
    "fields": {
      "releaseDate": {
        "value": "2019-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Nanbeige-16B",
    "fields": {
      "flops": {
        "value": "2.40E+23",
        "source": "epoch",
        "citation": "\"It uses 2.5T Tokens for pre-training\". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.16 billion * 2.5 trillion * 6 = 2.4e23"
      },
      "numParams": {
        "value": "16000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "retrieval-quality-kNN-LMs",
    "fields": {
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-125M (finetuned)",
    "fields": {
      "numParams": {
        "value": "125000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TRIMELMext (7M)",
    "fields": {
      "numParams": {
        "value": "7000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RefineNet",
    "fields": {
      "releaseDate": {
        "value": "2016-11-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BaGuaLu",
    "fields": {
      "numParams": {
        "value": "173900000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "13200000000",
        "source": "epoch",
        "citation": "17.5B tokens (in English, this is approximately 13.1B words, but the conversion may be different in Chinese) and 60.5M images."
      },
      "releaseDate": {
        "value": "2022-03-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mono3D++",
    "fields": {
      "flops": {
        "value": "4.85606E+18",
        "source": "epoch",
        "citation": "(4) * (6.691 * 10**12) * (168* 3600) * (0.3) = (num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = training time - about one week = 168 hours from\"It takes about one week to train the 2D bounding box net-work, and two hours for the orientation/3D scale networkon KITTI with 4 TITAN-X GPUs. The landmark detector istrained on Pascal3D. The training process for the monocu-lar depth estimation network is unsupervised using KITTIstereo-pairs, which takes around 5 to 12 hours dependingon the amount of data available. \"gpu flops - FP32 (float) 6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632"
      },
      "numTokens": {
        "value": "7481",
        "source": "epoch",
        "citation": "\"We evaluate our method on the KITTI object detection benchmark. This dataset contains 7, 481 training images \""
      },
      "trainingTimeDays": {
        "value": "168",
        "source": "epoch",
        "citation": "\"about one week\" from section 3.4 "
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GTX Titan X",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "XVERSE-65B-2",
    "fields": {
      "flops": {
        "value": "1.25E+24",
        "source": "epoch",
        "citation": "C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP"
      },
      "numParams": {
        "value": "65000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2720000000000",
        "source": "epoch",
        "citation": "Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.Assume 0.85 words per token on average for the mix of languages."
      },
      "trainingTimeDays": {
        "value": "4096",
        "source": "epoch",
        "citation": "November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours."
      },
      "releaseDate": {
        "value": "2023-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaMA-7B",
    "fields": {
      "flops": {
        "value": "4.02E+22",
        "source": "epoch",
        "citation": "1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOPfrom paper, Llama-7B took 82,432 GPU hours using A100s312 trillion * 82,432 * 3600 * 0.3 = 2.78e22 FLOP"
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1 trillion tokens * 0.75 words/token = 750 billion words"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.43",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deconvolutional Network",
    "fields": {
      "releaseDate": {
        "value": "2010-06-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama 2-13B",
    "fields": {
      "flops": {
        "value": "1.60E+23",
        "source": "epoch",
        "citation": "13 billion * 2 trillion * 6 = 1.6e23"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1500000000000",
        "source": "epoch",
        "citation": "2 trillion tokens ~= 1.5 trillion words"
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao - Wen Hui",
    "fields": {
      "flops": {
        "value": "1.16122E+20",
        "source": "epoch",
        "citation": "64 Nvidia V100 GPUs for 2.5 days64 GPUs * 2.8e13 FLOP/s /GPU * 2.5*24*60*60s* 0.3 [utilization rate]"
      },
      "numParams": {
        "value": "11300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SmooCT",
    "fields": {
      "flops": {
        "value": "6.9E+16",
        "source": "epoch",
        "citation": "\"Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization\"Assume an Intel i7 so 400e9 FLOP/s.6.9e16 = 400e9*60*60*48"
      },
      "numTokens": {
        "value": "12000000000",
        "source": "epoch",
        "citation": "\"Each three-player agentwas trained for about 12 billion episodes\"An episode seems to be a round of betting."
      },
      "trainingTimeDays": {
        "value": "48",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mitosis",
    "fields": {
      "flops": {
        "value": "1.37E+17",
        "source": "epoch",
        "citation": "\"Training each network requires one day of computation with an optimized GPUimplementation\"Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get3600*24*1.58E+12 = 1.37E+17 FLOP"
      },
      "numParams": {
        "value": "37230",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1000000",
        "source": "epoch",
        "citation": "The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.The final dataset has 1M instances\"We build the actual training set, composed by 1 million instances, which includesall mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampledfrom non-mitosis pixels by assigning to each pixel p a weight D(p).\""
      },
      "costDollars": {
        "value": "2.00443852",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-09-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ENAS",
    "fields": {
      "flops": {
        "value": "20099999999999996",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "24000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLM-130B",
    "fields": {
      "flops": {
        "value": "3.78E+23",
        "source": "epoch",
        "citation": "\"96 NVIDIA A100 (40G * 8) servers for 2 months\"312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 30% utilization = 3.778*10^23 FLOPhttps://www.wolframalpha.com/input?i=312+teraflops+*+96+*+8+*+2+months+*+30%25utilization rate - citation from the paper: \"we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.\""
      },
      "numParams": {
        "value": "130000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "1440",
        "source": "epoch",
        "citation": "see compute notes"
      },
      "gpuCount": {
        "value": "768",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.433",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-08-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM + dynamic eval",
    "fields": {
      "numParams": {
        "value": "50000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-09-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLM-10B",
    "fields": {
      "flops": {
        "value": "3.79E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "524288",
        "source": "epoch",
        "citation": "\"The models are trained on 64 V100 GPUs for 200K steps withbatch size of 1024 and maximum sequence lengthof 512\""
      },
      "releaseDate": {
        "value": "2021-03-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AFP+FPI (PTB)",
    "fields": {
      "flops": {
        "value": "227000000000000",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "2040000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adversarial + AWD-LSTM-MoS + partial shuffled",
    "fields": {
      "releaseDate": {
        "value": "2019-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Jurassic-1-Jumbo",
    "fields": {
      "flops": {
        "value": "3.70E+23",
        "source": "epoch",
        "citation": "see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit"
      },
      "numParams": {
        "value": "178000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "225000000000",
        "source": "epoch",
        "citation": "\"Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources\"1 token ~ 0.75 words"
      },
      "batchSize": {
        "value": "3200000",
        "source": "epoch",
        "citation": "\"Namely, we used a base learning rate of 1.2 \u00d7 10\u22124 and 0.6 \u00d7 10\u22124 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.\""
      },
      "costDollars": {
        "value": "805277.0088",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "mT0-13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "20000000000",
        "source": "epoch",
        "citation": "per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0"
      },
      "releaseDate": {
        "value": "2022-11-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Alleviated TOI 10 (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2019-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Word-Independent-SRNN+KN5",
    "fields": {
      "numParams": {
        "value": "5320000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-03-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AR-LDM",
    "fields": {
      "flops": {
        "value": "5.1E+20",
        "source": "epoch",
        "citation": "8 NVIDIA A100 GPUs for 8 days"
      },
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "194",
        "source": "epoch",
        "citation": "8 NVIDIA A100 GPUs for 8 days"
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Zoneout + Variational LSTM (WT2)",
    "fields": {
      "flops": {
        "value": "1.68E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "21000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "mPLUG-Owl2",
    "fields": {
      "numParams": {
        "value": "7120000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "400000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "StableLM-2-1.6B",
    "fields": {
      "flops": {
        "value": "1.92E+22",
        "source": "epoch",
        "citation": "6 * 1.6B * 2T = 19200000000000000000000"
      },
      "numParams": {
        "value": "1600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2000000000000",
        "source": "epoch",
        "citation": "\"model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.\"assuming 1 word per token "
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FinGPT-13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Show-1",
    "fields": {
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "dense-IndRNN+dynamic eval",
    "fields": {
      "numParams": {
        "value": "44100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DALL-E mega",
    "fields": {
      "flops": {
        "value": "2.28527E+20",
        "source": "epoch",
        "citation": "flops = (128) * (1230 * 10**9) * (1344 * 3600) * (0.3) = 2.3e20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact"
      },
      "trainingTimeDays": {
        "value": "1344",
        "source": "epoch",
        "citation": "from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Culturome",
    "fields": {
      "releaseDate": {
        "value": "2010-12-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Retrieval-Augmented Generator",
    "fields": {
      "numParams": {
        "value": "626000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 PCIe 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BellKor 2007",
    "fields": {
      "numTokens": {
        "value": "100480507",
        "source": "epoch",
        "citation": "The training data set consists of 100,480,507ratings"
      },
      "releaseDate": {
        "value": "2009-09-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tacotron 2",
    "fields": {
      "numTokens": {
        "value": "340000",
        "source": "epoch",
        "citation": "\"We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker.\"13,680 words/hour * 24.6 = 336528 words"
      },
      "releaseDate": {
        "value": "2017-12-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-2.7B (finetuned on PTB)",
    "fields": {
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stacked hourglass network",
    "fields": {
      "releaseDate": {
        "value": "2016-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PointNet",
    "fields": {
      "releaseDate": {
        "value": "2016-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SantaCoder",
    "fields": {
      "flops": {
        "value": "2.10E+21",
        "source": "epoch",
        "citation": "Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 \u00d7 10^21 FLOPs. The final model described in Section 6.2 uses twice the amount of compute."
      },
      "numParams": {
        "value": "1100000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "150",
        "source": "epoch",
        "citation": "Their initial training runs took 3.1 days. The final training run was run for twice as many iterations with \"all other hyper-parameters the same\" and used twice as much compute as this. So likely 6 days or ~150 hours, but they don't explicitly say whether they used the same hardware."
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Jurassic-X",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Zip CNN",
    "fields": {
      "flops": {
        "value": "43372117520",
        "source": "epoch",
        "citation": "Its a deep CNN so we assume a backward-forward ratio of 2:1\"The network was trained for 23passes through the training set (167,693 pattern presentations).\""
      },
      "numParams": {
        "value": "9760",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "7291",
        "source": "epoch",
        "citation": "The digits were writtenby many different people, using a great variety of sizes, writing styles,and instruments, with widely varying amounts of care; 7291 examplesare used for training the network and 2007 are used for testing the generalization performance"
      },
      "releaseDate": {
        "value": "1989-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Word2Vec (large)",
    "fields": {
      "flops": {
        "value": "3.888E+16",
        "source": "epoch",
        "citation": "From https://openai.com/blog/ai-and-compute/ Appendix.\"less than 0.00045 pfs days\"(86400*10^15*0.00045)"
      },
      "numParams": {
        "value": "692000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "33000000000",
        "source": "epoch",
        "citation": "\"For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K\""
      },
      "costDollars": {
        "value": "0.5481336952",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2013-10-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mogrifier (d2, MoS2, MC) + dynamic eval",
    "fields": {
      "numParams": {
        "value": "35000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Learning deep architectures",
    "fields": {
      "releaseDate": {
        "value": "2009-11-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GLM-10B-bidirectional",
    "fields": {
      "releaseDate": {
        "value": "2021-03-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GNoME for crystal discovery",
    "fields": {
      "numParams": {
        "value": "16240000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "1-layer-LSTM",
    "fields": {
      "numParams": {
        "value": "86500000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-07-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-IML (175B)",
    "fields": {
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-12-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepFace",
    "fields": {
      "releaseDate": {
        "value": "2014-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM2-650M",
    "fields": {
      "flops": {
        "value": "7.56E+21",
        "source": "epoch",
        "citation": "from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 4.4e21 FLOPfrom the paper's Supplementary Materials: \"We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.\"8 days x 512 V100s x an imputed 30% utilization\": 1.3e22 FLOPGeometric mean: 7.56e21 FLOP"
      },
      "numParams": {
        "value": "650000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "192",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NAS+ESS (156M)",
    "fields": {
      "flops": {
        "value": "2.89E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "156000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-05-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2-Medium+Pixelfly",
    "fields": {
      "flops": {
        "value": "8.34E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "203000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao 2.0",
    "fields": {
      "numParams": {
        "value": "1750000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-05-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Samuel Neural Checkers",
    "fields": {
      "flops": {
        "value": "428400000",
        "source": "epoch",
        "citation": "\"it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)\"\"The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.\"\"The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.\"source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html\"An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games\"\"Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.\""
      },
      "numParams": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "53000",
        "source": "epoch",
        "citation": "Based on number of board positionsAt the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much largernumber of positions by means of the culling techniquesdescribed. While this is still far from the number whichwould tax the listing and searching procedures used inthe program, rough estimates, based on the frequencywith which the saved boards are utilized during normalplay (these figures being tabulated automatically), indicate that a library tape containing at least 20 times thepresent number of board positions would be needed toimprove the midgame play significantly. At the presentrate of acquisition of new positions this would requirean inordinate amount of play and, consequently, ofmachine time."
      },
      "releaseDate": {
        "value": "1959-07-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TigerBot-70B",
    "fields": {
      "flops": {
        "value": "1.02E+24",
        "source": "epoch",
        "citation": "~1.02e24Tigerobo did ~2.1e23 additional pre-training. We estimated Llama 2 was trained on 8.1e23 FLOP."
      },
      "numParams": {
        "value": "70000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "from paper:\"We pretrained TigerBot models using a global batch size (GBS) of 4M tokens, while fine-tuned models with a GBS as small as 100\u2013400k tokens\"It's also based on pretrained Llama 2, which also used a batch size of 4M"
      },
      "releaseDate": {
        "value": "2023-09-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Two-stream ConvNets for action recognition",
    "fields": {
      "releaseDate": {
        "value": "2014-06-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SparseOPT-175B",
    "fields": {
      "flops": {
        "value": "1.58E+23",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "87500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MuZero VP9",
    "fields": {
      "releaseDate": {
        "value": "2022-02-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Luminous Sparse",
    "fields": {
      "numParams": {
        "value": "2600000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gradient Boosting Machine",
    "fields": {
      "releaseDate": {
        "value": "2001-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-DRILL + dynamic evaluation\u2020 (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2019-05-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "M4-50B",
    "fields": {
      "numParams": {
        "value": "50000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPALM + kNN",
    "fields": {
      "releaseDate": {
        "value": "2021-04-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LIRA",
    "fields": {
      "numParams": {
        "value": "100000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "10000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2004-07-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Agile Soccer Robot",
    "fields": {
      "trainingTimeDays": {
        "value": "240",
        "source": "epoch",
        "citation": "14+158+68 hours:\"Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-playtook 68 hours (see Appendix B for details)\""
      },
      "releaseDate": {
        "value": "2023-04-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "STeLLA",
    "fields": {
      "releaseDate": {
        "value": "1963-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Monarch-GPT-2-Small",
    "fields": {
      "releaseDate": {
        "value": "2022-04-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Firefly",
    "fields": {
      "releaseDate": {
        "value": "2023-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "2-layer skip-LSTM + dropout tuning (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PyramidNet",
    "fields": {
      "numParams": {
        "value": "26000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-09-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CICERO",
    "fields": {
      "releaseDate": {
        "value": "2022-11-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2)",
    "fields": {
      "numParams": {
        "value": "33000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-09-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeGen2.5",
    "fields": {
      "flops": {
        "value": "5.90E+22",
        "source": "epoch",
        "citation": "7B parameters, trained on 1.4T tokens7 billion * 1.4 trillion * 6 = 5.9e22"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Ankh_large",
    "fields": {
      "flops": {
        "value": "6.50E+21",
        "source": "epoch",
        "citation": "Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf"
      },
      "numParams": {
        "value": "1150000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Rotation",
    "fields": {
      "numParams": {
        "value": "86000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "wave2vec 2.0 LARGE",
    "fields": {
      "flops": {
        "value": "1.90E+21",
        "source": "epoch",
        "citation": "From surveying the authors:We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)Assume 40% utilization based on default for non-Language domain (https://epochai.org/blog/estimating-training-compute)64 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h~= 1.9E+21 FLOP"
      },
      "numParams": {
        "value": "317000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "727776000",
        "source": "epoch",
        "citation": "pg 4, section 4.1\"As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.\"53.2k h * 13,680 words/h = 727776000 words"
      },
      "costDollars": {
        "value": "1569.382269",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Xception",
    "fields": {
      "flops": {
        "value": "4.36E+20",
        "source": "epoch",
        "citation": "60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20"
      },
      "numParams": {
        "value": "22855952",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "350000000",
        "source": "epoch",
        "citation": "\"JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k\""
      },
      "costDollars": {
        "value": "1961.338254",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": "\"while the JFT experiments took over one month each.\""
      },
      "gpuCount": {
        "value": "60",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla K80",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-10-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LDA",
    "fields": {
      "releaseDate": {
        "value": "2003-02-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "E-SPA",
    "fields": {
      "numParams": {
        "value": "243000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-2.7B",
    "fields": {
      "flops": {
        "value": "6.48E+21",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ASE+ACE",
    "fields": {
      "numParams": {
        "value": "324",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1983-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "UDSMProt",
    "fields": {
      "flops": {
        "value": "8.9E+17",
        "source": "epoch",
        "citation": "170k sequences, each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.170k * 1024 * 30 * 28.3 * 6 = 8.9e17."
      },
      "numParams": {
        "value": "28303800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "6-Layer-Tensor-Transformer+AdaHessian",
    "fields": {
      "flops": {
        "value": "1.58E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "85300000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT3-6.7B + muP",
    "fields": {
      "flops": {
        "value": "1.28E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-03-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PAGnol-XL",
    "fields": {
      "flops": {
        "value": "2.592E+20",
        "source": "epoch",
        "citation": "They report their compute directly.From section 8: \"About 62k GPU-hours on the Jean Zay HPC Cluster.\" Jean Zay uses both A100 and V100 GPUs, and maybe other stuff as well?Note they explicitly call out V100 in their Appendix A.https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/"
      },
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "24000000000",
        "source": "epoch",
        "citation": "Section 4.1: 32G tokens => 32e9*0.75 = 24e9 words"
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 SXM2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-10-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PointNet++",
    "fields": {
      "releaseDate": {
        "value": "2017-06-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "genCNN + dyn eval",
    "fields": {
      "flops": {
        "value": "7.3E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "8000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2015-03-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepStack",
    "fields": {
      "flops": {
        "value": "1.44634E+19",
        "source": "epoch",
        "citation": "The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.From p.26: \"For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack\u2019s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.\"Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.Calculation:6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP."
      },
      "numParams": {
        "value": "2500000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "10000000",
        "source": "epoch",
        "citation": "\"The turn network was trained by solving 10 million randomly generated poker turngames. These turn games used randomly generated ranges, public cards, and a random potsize (10).\""
      },
      "costDollars": {
        "value": "0.0008494643653",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "218",
        "source": "epoch",
        "citation": "from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs"
      },
      "gpuCount": {
        "value": "20",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-01-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Segatron XL base, M=384",
    "fields": {
      "releaseDate": {
        "value": "2020-04-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BERT-RBP",
    "fields": {
      "flops": {
        "value": "1.4E+20",
        "source": "epoch",
        "citation": "See DNABert entry:\"Since the pre-training of DNABERT model is resource-intensive (about 25\u2009days on 8 NVIDIA 2080Ti GPUs)\"Assuming FP16 and 30% utilizationCalculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP"
      },
      "numParams": {
        "value": "110000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-07",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CapsNet (MultiMNIST)",
    "fields": {
      "numParams": {
        "value": "11360000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-10-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Scatterbrain",
    "fields": {
      "releaseDate": {
        "value": "2021-10-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fuyu-8B",
    "fields": {
      "numParams": {
        "value": "8000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Incoder-6.7B",
    "fields": {
      "flops": {
        "value": "3.00E+21",
        "source": "epoch",
        "citation": "per table 5, required 3 zettaflop (3e21) to train.also, \"INCODER-6.7B was trained on 248 V100 GPUs for 24 days\"hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time."
      },
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "576",
        "source": "epoch",
        "citation": "24"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Byte-mLSTM+emb+WN+VD",
    "fields": {
      "numParams": {
        "value": "46000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Shortformer",
    "fields": {
      "flops": {
        "value": "3.04E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "24000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Falcon-40B",
    "fields": {
      "flops": {
        "value": "2.40E+23",
        "source": "epoch",
        "citation": "C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)"
      },
      "numParams": {
        "value": "40000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1000B tokens ~= 750B words"
      },
      "batchSize": {
        "value": "2359296",
        "source": "epoch",
        "citation": "Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.\"All Falcon models are pretrained with a 2,048 sequence length\"https://arxiv.org/pdf/2311.16867.pdf"
      },
      "trainingTimeDays": {
        "value": "1440",
        "source": "epoch",
        "citation": "\"Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.\"\"Training started in December 2022 and took two months.\""
      },
      "gpuCount": {
        "value": "384",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.3864",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-03-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Imagen",
    "fields": {
      "flops": {
        "value": "1.46E+22",
        "source": "epoch",
        "citation": "256 TPU v4 chips for 64x64, for 4 days128 TPU v4 chips for 64->256, for 2 days128 TPU v4 chips for 256->1024, for 2 days256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP"
      },
      "numParams": {
        "value": "3000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "860000000",
        "source": "epoch",
        "citation": "\"We train on a combination of internal datasets, with \u2248 460Mimage-text pairs, and the publicly available Laion dataset [61], with \u2248 400M image-text pairs.\""
      },
      "trainingTimeDays": {
        "value": "96",
        "source": "epoch",
        "citation": "4 days"
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-05-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ResNet-RS",
    "fields": {
      "flops": {
        "value": "1.76E+22",
        "source": "epoch",
        "citation": "(350) * (128000000000) * (1312 * 10**5) * 3 = 17633280000000000000000(epochs) * (inference FLOP) * (dataset size) * (constant to account for backpropagation)from 4.2 \"Our training method closely matches that of EfficientNet, where we train for 350 epochs, but with a few small differences\"350 epochs from description of Table 8 in appendix C"
      },
      "numParams": {
        "value": "192000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "131200000",
        "source": "epoch",
        "citation": "1.2M + 130M = 131.2M \"In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudo-labeled images.\"\"We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Studen\" \"We use the same dataset of 130M images pseudo-labeled as Noisy Student\""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "InternLM-XComposer",
    "fields": {
      "flops": {
        "value": "3.45E+21",
        "source": "epoch",
        "citation": "flops = (128) * (312 * 10**12) * (80 * 3600) * (0.3) = 3.45e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from section A.1 we have 128xA100 used for 80 hours"
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "64900000000",
        "source": "epoch",
        "citation": "from appendix A.1  55.6B English tokens and 22.1B Chineese tokens and 1.1B images so 0.75*55.6e9+22.1e9+1.1e9 = 64900000000.0"
      },
      "trainingTimeDays": {
        "value": "80",
        "source": "epoch",
        "citation": "from appendix A.1"
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Thumbs Up?",
    "fields": {
      "numTokens": {
        "value": "2053",
        "source": "epoch",
        "citation": "yielding a corpus of 752 negative and1301 positive reviews"
      },
      "releaseDate": {
        "value": "2002-05-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stable Diffusion (LDM-KL-8-G)",
    "fields": {
      "flops": {
        "value": "5.00E+22",
        "source": "epoch",
        "citation": "\"I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP\"[1] https://twitter.com/EMostaque/status/1563870674111832066"
      },
      "numParams": {
        "value": "1450000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "400000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "585.9375",
        "source": "epoch",
        "citation": "total chip-hours divided by number of GPUs150k/256"
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SPN-4+KN5",
    "fields": {
      "flops": {
        "value": "4.4E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "5000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "InstructGPT",
    "fields": {
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "374000033207",
        "source": "epoch",
        "citation": "Table 6 - describes **number of prompts**26584 + 6623 = 33207This is added to GPT-3 dataset size."
      },
      "releaseDate": {
        "value": "2022-01-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
    "fields": {
      "flops": {
        "value": "4.74E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "38000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Spectrally Normalized GAN",
    "fields": {
      "releaseDate": {
        "value": "2018-02-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CLIP (ViT L/14@336px)",
    "fields": {
      "flops": {
        "value": "1.05E+22",
        "source": "epoch",
        "citation": "https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing"
      },
      "numParams": {
        "value": "370000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "400000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "40146.98827",
        "source": "epoch",
        "citation": "https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html"
      },
      "trainingTimeDays": {
        "value": "288",
        "source": "epoch",
        "citation": "\u201cThe largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs\u201d"
      },
      "gpuCount": {
        "value": "256",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepLabV3",
    "fields": {
      "releaseDate": {
        "value": "2017-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cutout-regularized net",
    "fields": {
      "releaseDate": {
        "value": "2017-08-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "KataGo",
    "fields": {
      "flops": {
        "value": "2.32E+19",
        "source": "epoch",
        "citation": "\"[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days\"14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP"
      },
      "numParams": {
        "value": "2500000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "241000000",
        "source": "epoch",
        "citation": "241 million training samples across 4.2 million games"
      },
      "trainingTimeDays": {
        "value": "456",
        "source": "epoch",
        "citation": "27 processors for 19 days"
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 16 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-02-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN + char2-MS-vec",
    "fields": {
      "releaseDate": {
        "value": "2019-07-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN+LSA+KN5+cache (model combination w/ linear extrapolation)",
    "fields": {
      "releaseDate": {
        "value": "2012-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FixRes ResNeXt-101 WSL",
    "fields": {
      "numParams": {
        "value": "829000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "940000000",
        "source": "epoch",
        "citation": "\"Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)\""
      },
      "releaseDate": {
        "value": "2019-06-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2016-12-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Innervator",
    "fields": {
      "flops": {
        "value": "120000000",
        "source": "epoch",
        "citation": "10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations"
      },
      "numParams": {
        "value": "10",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1989-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Character-enriched word2vec",
    "fields": {
      "releaseDate": {
        "value": "2016-07-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Zi Yue 2.0",
    "fields": {
      "releaseDate": {
        "value": "2023-07-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Anthropic LM 175B",
    "fields": {
      "numParams": {
        "value": "175000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Switch",
    "fields": {
      "flops": {
        "value": "8.22E+22",
        "source": "epoch",
        "citation": "Table 4https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf"
      },
      "numParams": {
        "value": "1600000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "432000000000",
        "source": "epoch",
        "citation": "\"In our protocol we pre-train with 220 (1,048,576) tokensper batch for 550k steps amounting to 576B total tokens.\"1 token ~ 0.75 words"
      },
      "costDollars": {
        "value": "149825.6036",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "648",
        "source": "epoch",
        "citation": "see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.28",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)",
    "fields": {
      "releaseDate": {
        "value": "2018-09-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MPT-7B",
    "fields": {
      "flops": {
        "value": "4.20E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Back-propagation",
    "fields": {
      "flops": {
        "value": "124416000",
        "source": "epoch",
        "citation": "We assume that the number of mult-adds per pass is equal to the number of parameters.\"We trained the network for 1500 sweeps\"There are 12*12 possible pairs of people, so we assume that is the dataset size"
      },
      "numParams": {
        "value": "144",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "144",
        "source": "epoch",
        "citation": "There are 12*12 possible pairs of people, so we assume that is the dataset size"
      },
      "releaseDate": {
        "value": "1986-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Swift",
    "fields": {
      "flops": {
        "value": "5.337E+16",
        "source": "epoch",
        "citation": "Policies are trained for a total of 1\u2009\u00d7\u2009108 environment interactions, which takes 50\u2009min on a workstation (i9 12900K, RTX 3090, 32\u2009GB RAM DDR5). Fine-tuning is performed for 2\u2009\u00d7\u2009107 environment interactions.35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP"
      },
      "numParams": {
        "value": "21124",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "0.833",
        "source": "epoch",
        "citation": "50 minutes (training details, page 8)"
      },
      "gpuType": {
        "value": "NVIDIA GeForce RTX 3090",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-08-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeciLM 6B",
    "fields": {
      "numParams": {
        "value": "5700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaVA",
    "fields": {
      "flops": {
        "value": "4.85222E+19",
        "source": "epoch",
        "citation": "8*312e12*(10+8)*3600*0.3 = 4.852224e+19num gpus * peak flops * time *assumed utilization rate \"We train all models with 8\u00d7 A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\" so 18 hours of time, 8 A100,"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "10",
        "source": "epoch",
        "citation": "\"We train all models with 8\u00d7 A100s. Pretraining on CC-595K completes within 4 hours. Finetuningon Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\""
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BLSTM for handwriting (1)",
    "fields": {
      "releaseDate": {
        "value": "2007-09-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "True-Regularization+Finetune",
    "fields": {
      "numParams": {
        "value": "7000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-Neo-125M",
    "fields": {
      "releaseDate": {
        "value": "2021-03-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ESM1-85M",
    "fields": {
      "flops": {
        "value": "5.6E+19",
        "source": "epoch",
        "citation": "Information: 128 NVIDIA V100 GPUs [Pre-training details]840k steps [See Table S2: Hyperparameters]131,072 tokens per batch [\"We trained with 131,072 tokens per batch (128 gpus x 1024 tokens).\" - Pre-training details]Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 85.1e6 parameters = 5.6e19 FLOP"
      },
      "numParams": {
        "value": "85100000",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer",
    "fields": {
      "flops": {
        "value": "7.42452E+18",
        "source": "epoch",
        "citation": "\"The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.\"source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.htmlNVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performancesource: https://www.nvidia.com/en-gb/data-center/tesla-p100/We assume 0.33 utilization performance, in line with OpenAI's \"AI and compute\" articlesource: https://openai.com/blog/ai-and-compute/"
      },
      "numParams": {
        "value": "213000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1400000000",
        "source": "epoch",
        "citation": "\"We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary \"In total, this is 40.5 million sentence-pairs. Assuming each sentence pair is 15-20 words in each language, this is 1.2-1.6 billion words."
      },
      "costDollars": {
        "value": "111.1661088",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "84",
        "source": "epoch",
        "citation": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on thebottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps(3.5 days)."
      },
      "gpuCount": {
        "value": "8",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA P100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-06-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deep LSTM for video classification",
    "fields": {
      "releaseDate": {
        "value": "2015-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GShard (dense)",
    "fields": {
      "flops": {
        "value": "1.28E+22",
        "source": "epoch",
        "citation": "\"The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3core-years\"Assume 30% utilization. 2 TPU v3 cores = 1 TPU v3 chip.TPU v3 performance is 123 teraFLOPS per chip2048 TPU cores * (1 chip / 2 cores) * 123 TFLOPS/chip * 0.30 = 1.28e22 FLOPhttps://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+22+years+*+0.30Meanwhile their best dense model was trained on 235.5 TPU v3 core-years or 1.3702e23 FLOPhttps://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+235.5+years+*+0.30Effective model FLOPs utilization could have been lower since this model has very high training compute compared to parameter count (2.3B). (Compare to Chinchilla-optimal?)"
      },
      "numParams": {
        "value": "600000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "260000000000",
        "source": "epoch",
        "citation": "\"We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training\"Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words."
      },
      "batchSize": {
        "value": "4000000",
        "source": "epoch",
        "citation": "Table 3, bolded row is best model"
      },
      "trainingTimeDays": {
        "value": "1008",
        "source": "epoch",
        "citation": "6 weeks = 1008 hours"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeBERTa",
    "fields": {
      "flops": {
        "value": "6.00E+21",
        "source": "epoch",
        "citation": "From section 5.1.1: \"We use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K batch size and 1M steps takes about 20 days.\" This specifically refers to the largest models referred to in the paper, and smaller models are described elsewhere, but I'm assuming the large models are what we care about here. Apparently there are multiple types of GPUs referred to as V100s. I'm guessing these are NVIDIA Tesla SMX2s."
      },
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "15600000000",
        "source": "epoch",
        "citation": "\" DeBERTa is pretrained on 78G training data\"1GB ~ 200M words"
      },
      "trainingTimeDays": {
        "value": "240",
        "source": "epoch",
        "citation": "20 days"
      },
      "gpuCount": {
        "value": "96",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Eve",
    "fields": {
      "releaseDate": {
        "value": "2021-10-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BERT-Large",
    "fields": {
      "flops": {
        "value": "2.85E+20",
        "source": "epoch",
        "citation": "more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing"
      },
      "numParams": {
        "value": "340000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3300000000",
        "source": "epoch",
        "citation": "\"For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)\""
      },
      "costDollars": {
        "value": "999.9345742",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "96",
        "source": "epoch",
        "citation": "from appendix A.2: \"Training of BERTLARGE was performedon 16 Cloud TPUs (64 TPU chips total). Each pre-training took 4 days to complete.\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v2",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.29",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-10-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Compress-LSTM (4.6M)",
    "fields": {
      "releaseDate": {
        "value": "2019-02-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NLLB",
    "fields": {
      "flops": {
        "value": "1.75E+22",
        "source": "epoch",
        "citation": "Section 8.8:\" To train NLLB-200, a cumulativeof 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB\"See also Table 48Section 8.2.4 states they use FP16NVIDIA Datasheet states 312TFLOPS for FP16https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdfAssuming 0.3 utilization:312e12*3600*51968*0.3Also:\"Our final model is a Transformerencoder-decoder model in which we replace the Feed Forward Network (FFN) layer inevery 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoderlayers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described inSection 6.1.1. We share the embedding weights of the encoder input embedding, decoderinput embedding and decoder output embedding layers. We use an overall dropout of 0.3,attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parametersand FLOPs similar to that of a 3.3B dense model.\""
      },
      "numParams": {
        "value": "54500000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "360000000000",
        "source": "epoch",
        "citation": "[WORDS]Section 8.2.2: \"As we prepare to train on the final 202 language dataset comprising of over 18B sentencepairs and 2440 language directions\"18B sentences * 20 words/sentence"
      },
      "costDollars": {
        "value": "39175.63826",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-4 Turbo",
    "fields": {
      "releaseDate": {
        "value": "2023-11-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Tensorized Transformer (core-2)",
    "fields": {
      "releaseDate": {
        "value": "2019-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fast R-CNN",
    "fields": {
      "releaseDate": {
        "value": "2015-04-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VALL-E",
    "fields": {
      "flops": {
        "value": "1.01E+19",
        "source": "epoch",
        "citation": "\"The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustictokens per GPU for 800k steps\"353M * 800k * 6k * 6 = 1.01e1916 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?"
      },
      "numParams": {
        "value": "353000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "820800000",
        "source": "epoch",
        "citation": "60k hours~13,680 words/hour * 60,000 = 820800000 wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LTM",
    "fields": {
      "releaseDate": {
        "value": "2019-04-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wu Dao - Wen Lan",
    "fields": {
      "flops": {
        "value": "7.20E+21",
        "source": "epoch",
        "citation": "128 Nvidia A100 GPUs for 7 days128 GPUs * 3.1e14 FLOP/s /GPU * 7*24*60*60s* 0.3 [utilization rate]"
      },
      "numParams": {
        "value": "1000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-3.5 (text-davinci-003)",
    "fields": {
      "flops": {
        "value": "2.58E+24",
        "source": "epoch",
        "citation": "https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SRN-Encoded Grammatical Structures",
    "fields": {
      "numTokens": {
        "value": "177805",
        "source": "epoch",
        "citation": "4 training sets of 10k sentences each. Total number of words calculated by multiplying 10k and the avg. number of words per sentence in the training set."
      },
      "releaseDate": {
        "value": "1991-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-6.7B",
    "fields": {
      "numParams": {
        "value": "6700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Stable Beluga 1",
    "fields": {
      "numParams": {
        "value": "65200000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-07-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-Large+Behaviorial-Gating",
    "fields": {
      "numParams": {
        "value": "67000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "HOGWILD!",
    "fields": {
      "releaseDate": {
        "value": "2011-11-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "NASv3 (CIFAR-10)",
    "fields": {
      "flops": {
        "value": "2.20E+21",
        "source": "epoch",
        "citation": "50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass = 1.9e6 PF = 22 pfs-dayssource: https://openai.com/blog/ai-and-compute/"
      },
      "numParams": {
        "value": "37400000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "13069.34581",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "800",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-11-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adaptive Subgrad",
    "fields": {
      "releaseDate": {
        "value": "2011-10-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GAN-Advancer",
    "fields": {
      "releaseDate": {
        "value": "2016-12-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AWD-LSTM + dynamic eval (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2017-09-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YaLM",
    "fields": {
      "flops": {
        "value": "2.20E+23",
        "source": "epoch",
        "citation": "\"It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.\""
      },
      "numParams": {
        "value": "100000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "300000000000",
        "source": "epoch",
        "citation": "1.7TB of data 300B tokens \u2013 from github https://github.com/yandex/YaLM-100BI've assumed that 1 token correspond to 1 word in russian language."
      },
      "trainingTimeDays": {
        "value": "1560",
        "source": "epoch",
        "citation": "65 days"
      },
      "gpuCount": {
        "value": "800",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PAPA",
    "fields": {
      "releaseDate": {
        "value": "1961-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RaptorX-Contact",
    "fields": {
      "releaseDate": {
        "value": "2019-05-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Unified-IO",
    "fields": {
      "flops": {
        "value": "3.50E+21",
        "source": "epoch",
        "citation": "1M steps, batch size 1024. Sequence length may be 128-256:\"We use a maximum of 256 and 128 text tokens for inputs and outputs respectively, and a maximumlength of 576 (i.e. 24 \u00d7 24 patch encoding from a 384 \u00d7 384 image) for image inputs and 256 (i.e.16 \u00d7 16 latent codes from a 256 \u00d7 256 image) for image outputs6 * 1 million * 1024 * 128 * 2.9 billion = 2.3e216 * 1 million * 1024 * 256 * 2.9 billion = 4.6e21average is 3.5e21No hardware details."
      },
      "numParams": {
        "value": "2925000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CPM-Large",
    "fields": {
      "flops": {
        "value": "1.80E+21",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb"
      },
      "numParams": {
        "value": "2600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "16700000000",
        "source": "epoch",
        "citation": "\"language model, with 2.6 billion parameters and 100GB Chinese training data.\"We use the conversion factor 1GB ~ 167M words"
      },
      "costDollars": {
        "value": "6569.507171",
        "source": "epoch",
        "citation": "https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9."
      },
      "trainingTimeDays": {
        "value": "336",
        "source": "epoch",
        "citation": "\"It takes two weeks to train our largest model using 64 NVIDIA V100.\""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeepNash",
    "fields": {
      "releaseDate": {
        "value": "2022-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Llama-2-Chinese 13B",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-66B",
    "fields": {
      "flops": {
        "value": "1.10E+23",
        "source": "epoch",
        "citation": "OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 \u2217 2e6 \u2217 66e9 \u2217 6 = 1.1e23 FLOP"
      },
      "numParams": {
        "value": "66000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RBM Image Classifier",
    "fields": {
      "numParams": {
        "value": "80000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2009-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Spatial Pyramid Matching",
    "fields": {
      "releaseDate": {
        "value": "2006-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multi-scale Dilated CNN",
    "fields": {
      "releaseDate": {
        "value": "2015-11-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WeNet (Penn Treebank)",
    "fields": {
      "numParams": {
        "value": "23000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TF-LM-discourse LSTM (WT2)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RoseTTAFold All-Atom (RFAA)",
    "fields": {
      "releaseDate": {
        "value": "2023-10-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Flan-PaLM 540B",
    "fields": {
      "numParams": {
        "value": "540000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "37",
        "source": "epoch",
        "citation": "\"we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4TPU chips for 37 hours)\""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ADAM (CIFAR-10)",
    "fields": {
      "flops": {
        "value": "6.048E+16",
        "source": "epoch",
        "citation": "From https://openai.com/blog/ai-and-compute/ Appendixless than 0.0007 pfs-days (86400*10^15*0.0007)"
      },
      "costDollars": {
        "value": "0.6042303565",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2014-12-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GloVe (6B)",
    "fields": {
      "numParams": {
        "value": "120000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "6000000000",
        "source": "epoch",
        "citation": "\"We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]\""
      },
      "releaseDate": {
        "value": "2014-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Granite 13B",
    "fields": {
      "flops": {
        "value": "2.44E+23",
        "source": "epoch",
        "citation": "Estimate using hardware:\"Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.Granite.13b.v2 was trained on the same infrastructure for anadditional 1152 hours with 120 TFLOPS, bringing the total to2208 hours\"Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.256 * 2208 * 3600 * 120 TFLOPS = 2.44e23Using 6ND:\"The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.\"\"The granite.13b.v1 base model is trained for 300K iterations,with a batch size of 4M tokens, for a total of 1.25 trillion5 tokens. The granite.13b.v2 base model continued pre-trainingon top of the granite.13b.v1 checkpoint for an additional 300Kiterations and a total of 2.5 trillion tokens.\"2.5T * 13B * 6 = 1.95e23"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1875000000000",
        "source": "epoch",
        "citation": "2.5T tokens, 1.875T words at 0.75 words/token"
      },
      "trainingTimeDays": {
        "value": "2208",
        "source": "epoch",
        "citation": "\"Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for anadditional 1152 hours with 120 TFLOPS, bringing the total to2208 hours\""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "WeLM",
    "fields": {
      "flops": {
        "value": "6.21E+21",
        "source": "epoch",
        "citation": ">>> num_gpu = 128 >>> flops = 7797 * 10**10 >>> time = 24 * 24 * 3600 >>> utilization = 0.3 >>> num_gpu * flops * time * utilization 6.2084579328e+21 >>> int(num_gpu * flops * time * utilization) 6208457932800000000000 from citations:\"The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 days\u201d, \"All models are trained with FP16 mixed precision.\"from https://www.techpowerup.com/gpu-specs/a100-sxm4-40-gb.c3506 A100 have  77.97 TFLOPS for FP16 (half), "
      },
      "numParams": {
        "value": "10000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "262000000000",
        "source": "epoch",
        "citation": "from the paper \"After all the above filtering process, our corpus contains 262B tokens\u201dData is mostly in Chinese language - we assume that 1 token correspond to 1 word"
      },
      "trainingTimeDays": {
        "value": "576",
        "source": "epoch",
        "citation": "\"The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 days\u201d, "
      },
      "gpuCount": {
        "value": "128",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MADALINE III",
    "fields": {
      "releaseDate": {
        "value": "1990-09-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-350M",
    "fields": {
      "numParams": {
        "value": "350000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SimpleNet",
    "fields": {
      "numParams": {
        "value": "5480000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA GeForce GTX 980",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-08-22",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "RNN for speech",
    "fields": {
      "flops": {
        "value": "226690156032",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "numParams": {
        "value": "7512",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "14096",
        "source": "epoch",
        "citation": "The data base was divided into two parts: a training set and an open test set. These two sets consisted of 28 191 and 7051 syllables,respectively.Of the top 10,000 Chinese words, 15% have 1 syllable, 78% have 2 syllables, and 7% have more than two syllables. Assuming 2 syllables per word, the training set is around 14100 words."
      },
      "releaseDate": {
        "value": "1998-05-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL-ptb",
    "fields": {
      "releaseDate": {
        "value": "2019-01-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MultiBand Diffusion",
    "fields": {
      "flops": {
        "value": "2.6E+19",
        "source": "epoch",
        "citation": "\"It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.\"125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number)4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19"
      },
      "trainingTimeDays": {
        "value": "48",
        "source": "epoch",
        "citation": "around 2 days"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Wide & Deep",
    "fields": {
      "releaseDate": {
        "value": "2016-06-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mistral 7B",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "UniPi",
    "fields": {
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-01-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CLIP (ResNet-50)",
    "fields": {
      "numParams": {
        "value": "88600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "400000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-01-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ISS",
    "fields": {
      "flops": {
        "value": "3.4E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "11100000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-09-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Adversarial Joint Adaptation Network (ResNet)",
    "fields": {
      "numParams": {
        "value": "60000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "4652",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Flamingo",
    "fields": {
      "flops": {
        "value": "2.70E+23",
        "source": "epoch",
        "citation": "1536 TPU v4 chips for 15 days. Assuming 50% utilization:C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.50 = 2.7*10^23 FLOPAll training and evaluationwas performed on TPUv4 instances. The largest model containing 80 billion parameters is trained onQUSV chips for 15 days and sharded across 16 devices.All trained parameters and optimizer accumulators are storedand updated in float32; all activations and gradients are computed in bfloat16 after downcastingof parameters from float32 to bfloat16"
      },
      "numParams": {
        "value": "80000000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "360",
        "source": "epoch",
        "citation": "1536 TPU v4 chips for 15 days"
      },
      "gpuCount": {
        "value": "1536",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CodeT5-base",
    "fields": {
      "flops": {
        "value": "1.56E+21",
        "source": "epoch",
        "citation": "\"We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively\"16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21"
      },
      "numParams": {
        "value": "220000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "288",
        "source": "epoch",
        "citation": "\"The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively\""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PanGu-\u03b1",
    "fields": {
      "flops": {
        "value": "5.83E+22",
        "source": "epoch",
        "citation": "source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Huawei/PanGu-%CE%B1.json"
      },
      "numParams": {
        "value": "207000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "200000000000",
        "source": "epoch",
        "citation": "\"The composition of our corpus and the processing steps adopted to each data source is shown in Table 3.2.Based on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and 13B) and large (200B) models, respectively\"1 TB = 1000 GB1 GB ~ 200M words"
      },
      "costDollars": {
        "value": "97802.05832",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GRU + p-tHSM (pretrain via Brown) (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2017-08-19",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ST-MoE",
    "fields": {
      "numParams": {
        "value": "269000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1342000000000",
        "source": "epoch",
        "citation": "1790B tokens, or 1342B words at 0.75 words/token"
      },
      "releaseDate": {
        "value": "2022-02-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Engin-Base (NE)",
    "fields": {
      "releaseDate": {
        "value": "2021-12-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM+Noise(Beta)",
    "fields": {
      "flops": {
        "value": "1.27E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "51000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Differentiable neural computer",
    "fields": {
      "releaseDate": {
        "value": "2016-10-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaGo Lee",
    "fields": {
      "flops": {
        "value": "1.90E+21",
        "source": "epoch",
        "citation": "This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.The architecture used was pretty much the same as AlphaGo Fan, but it was \"trained for longer\" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked belowAlphaGo Fan: https://www.nature.com/articles/nature16961AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"
      },
      "numTokens": {
        "value": "29400000",
        "source": "epoch",
        "citation": "We trained the policy network p\u03c3 to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games."
      },
      "costDollars": {
        "value": "14041.8044",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2016-01-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Mask R-CNN",
    "fields": {
      "releaseDate": {
        "value": "2017-03-30",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Dropout-LSTM+Noise(Bernoulli) (WT2)",
    "fields": {
      "flops": {
        "value": "1.27E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "51000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-05-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EMDR",
    "fields": {
      "flops": {
        "value": "1.24E+21",
        "source": "epoch",
        "citation": "\"We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete,while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we alsoperform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.\"1 week + 25+25+8 hours * 16 A100s= ~230 * 16 A100-hours= 230 * 16 * 3600 * 312 trillion * 0.3 = 1.24e21"
      },
      "numParams": {
        "value": "440000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "230",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-06-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Discriminator Guidance",
    "fields": {
      "flops": {
        "value": "2.157E+20",
        "source": "epoch",
        "citation": "481 hours * 312 TFLOPS (A100) * 40% utilization"
      },
      "trainingTimeDays": {
        "value": "481",
        "source": "epoch",
        "citation": "Table 6"
      },
      "gpuType": {
        "value": "NVIDIA A100 PCIe",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MusicGen",
    "fields": {
      "numParams": {
        "value": "3300000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-06-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL Large",
    "fields": {
      "flops": {
        "value": "1.09E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "257000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Claude Instant",
    "fields": {
      "releaseDate": {
        "value": "2023-08-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Probabilistic modeling for object recognition",
    "fields": {
      "numTokens": {
        "value": "120472",
        "source": "epoch",
        "citation": "Section 5.1: \"We formed training sets from 991 faces images and 1,552non-face images.\"\"For each face image we generated120 synthetic variations\""
      },
      "releaseDate": {
        "value": "1998-06-23",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MT-DNN",
    "fields": {
      "numParams": {
        "value": "330000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-01-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DEQ-Transformer (Medium, Adaptive Embedding)",
    "fields": {
      "flops": {
        "value": "8.16E+17",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "110000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Random Decision Forests",
    "fields": {
      "numTokens": {
        "value": "60000",
        "source": "epoch",
        "citation": "The images are from the 1992 NIST (National Institute of Standards and Technology) Competition"
      },
      "releaseDate": {
        "value": "1995-08-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Meena",
    "fields": {
      "flops": {
        "value": "1.12E+23",
        "source": "epoch",
        "citation": "https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdfTable 4"
      },
      "numParams": {
        "value": "2600000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "40000000000",
        "source": "epoch",
        "citation": "\"The final Meena dataset contains 341GB of text(40B words)\"Converting from GB to words yields 6.8e10, which is in the same OOM"
      },
      "batchSize": {
        "value": "82655",
        "source": "epoch",
        "citation": "61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc"
      },
      "costDollars": {
        "value": "263099.9403",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "720",
        "source": "epoch",
        "citation": "We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)"
      },
      "gpuCount": {
        "value": "1024",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.3439",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-01-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ImageBind",
    "fields": {
      "numParams": {
        "value": "932000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA V100,NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Decaying Fast Weights Transformer",
    "fields": {
      "flops": {
        "value": "7.9E+19",
        "source": "epoch",
        "citation": "Pre-trained model is Transformer (Adaptive Input Embeddings) which was 7.3e19. This is from 8 * 67 V100-hours. fine-tuning:\"Training was performed on a single NVIDIA A40 GPU for 35 hours\"35h, 1 GPU, 149.7e12, 30% = 5.7e18 FLOP\"5.7e18 + 7.3e19 is 7.9e19"
      },
      "numParams": {
        "value": "242000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-10-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Baichuan2-13B",
    "fields": {
      "flops": {
        "value": "2.03E+23",
        "source": "epoch",
        "citation": "They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.13b * 2.6t * 6 = 2.03e23"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2300000000000",
        "source": "epoch",
        "citation": "2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words1.3T English tokens * (0.75 words/token) = 0.975T English wordstotal: 2.275T, or ~2.3T"
      },
      "releaseDate": {
        "value": "2023-09-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Pythia-12b",
    "fields": {
      "flops": {
        "value": "2.16E+22",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "12000000000",
        "source": "epoch",
        "citation": ""
      },
      "batchSize": {
        "value": "2097152",
        "source": "epoch",
        "citation": "\"The most notable divergence from standard training procedures is that we use a much larger batch size than what is standard for training small language models... we use a batch size of 1024 samples with a sequence length of 2048 (2,097,152 tokens) for all models\""
      },
      "releaseDate": {
        "value": "2023-04-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TF-LM-discourse LSTM (PTB)",
    "fields": {
      "releaseDate": {
        "value": "2018-05-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT-2 (1.5B, Curriculum Learning 45K)",
    "fields": {
      "flops": {
        "value": "6E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "1500000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Samsung Gauss Image",
    "fields": {
      "releaseDate": {
        "value": "2023-11-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Samuel Neural Checkers II",
    "fields": {
      "numParams": {
        "value": "40",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "1967-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Verbatim Memory Transformer (117M)",
    "fields": {
      "releaseDate": {
        "value": "2022-10-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BlueLM 13B",
    "fields": {
      "flops": {
        "value": "1.09E+23",
        "source": "epoch",
        "citation": "C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOPhttps://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion(assuming 1 epoch)"
      },
      "numParams": {
        "value": "7000000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1950000000000",
        "source": "epoch",
        "citation": "\"Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data\" from GitHub"
      },
      "releaseDate": {
        "value": "2023-10-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GPT2+CoreLM+Fine-Tuning",
    "fields": {
      "flops": {
        "value": "3.17E+16",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "132000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-11-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Transformer-XL + RMT",
    "fields": {
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-07-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "S + I-Attention (3)",
    "fields": {
      "releaseDate": {
        "value": "2018-06-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LSTM-Medium+Behaviorial-Gating",
    "fields": {
      "releaseDate": {
        "value": "2019-08-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PaLI-X",
    "fields": {
      "numParams": {
        "value": "55000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1400000000",
        "source": "epoch",
        "citation": "1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data"
      },
      "releaseDate": {
        "value": "2023-05-29",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Statistical Shape Constellations",
    "fields": {
      "releaseDate": {
        "value": "2003-01-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "SRU++ Base",
    "fields": {
      "releaseDate": {
        "value": "2021-02-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TransformerXL+RelationLM",
    "fields": {
      "flops": {
        "value": "3.20E+21",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "124000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-01-24",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-2.7B",
    "fields": {
      "numParams": {
        "value": "2700000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "EfficientDet",
    "fields": {
      "numParams": {
        "value": "77000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-07-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AlphaX-1",
    "fields": {
      "flops": {
        "value": "7.6E+18",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "579000000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "24.10161745",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Geforce GTX1080 Ti",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-10-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PNASNet-5",
    "fields": {
      "flops": {
        "value": "6.62904E+19",
        "source": "epoch",
        "citation": "8 times less compute than Zoph (2018), which used 500 p100s for 4 days.(From Imagenet paper-data, Besiroglu et al., forthcoming) "
      },
      "numTokens": {
        "value": "1280000",
        "source": "epoch",
        "citation": ""
      },
      "costDollars": {
        "value": "991.4815111",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-12-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ProteinGAN",
    "fields": {
      "flops": {
        "value": "4.3E+18",
        "source": "epoch",
        "citation": "2.5 million steps, batch size 64, 210 hours on NVIDIA Tesla P100 system"
      },
      "numParams": {
        "value": "60000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "210",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA P100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-03-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ChatGLM3",
    "fields": {
      "flops": {
        "value": "1.09E+24",
        "source": "epoch",
        "citation": "Highly speculative.Assume 1 epoch on 1.4T tokens.6 FLOP/token/param * 1.4T tokens * 130B paramshttps://www.wolframalpha.com/input?i=6*130+billion*1.4+trillion"
      },
      "numParams": {
        "value": "130000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1050000000000",
        "source": "epoch",
        "citation": "The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.Sources:https://chatglm.cn/https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.mdhttps://www.zhipuai.cn/en/news/76"
      },
      "releaseDate": {
        "value": "2023-10-27",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Optimized Multi-Scale Edge Detection",
    "fields": {
      "releaseDate": {
        "value": "1986-11-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CURL",
    "fields": {
      "numParams": {
        "value": "907264",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-04-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ObjectNet",
    "fields": {
      "flops": {
        "value": "1.94E+19",
        "source": "epoch",
        "citation": "3-5 days of training (say, 4.5), 50 teraFLOP/second at 50% utilization rate (reported) = 1.94E19"
      },
      "numParams": {
        "value": "38000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": "In total, 95,824 images were collected from 5,982 workers out of which 50,000 images were retainedafter validation and included in the dataset"
      },
      "costDollars": {
        "value": "50.78535972",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-09-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "AmoebaNet-A (F=190)",
    "fields": {
      "numParams": {
        "value": "87000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OPT-125M (finetuned on PTB)",
    "fields": {
      "numParams": {
        "value": "125000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-06-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GRUs",
    "fields": {
      "releaseDate": {
        "value": "2014-06-03",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Multi-cell LSTM",
    "fields": {
      "flops": {
        "value": "2.01E+15",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "7200000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-11-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cohere Embed",
    "fields": {
      "releaseDate": {
        "value": "2023-11-02",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Peephole LSTM",
    "fields": {
      "numParams": {
        "value": "17",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "64970000",
        "source": "epoch",
        "citation": "See Table 2"
      },
      "releaseDate": {
        "value": "2000-07-26",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "OpenAI TI7 DOTA 1v1",
    "fields": {
      "flops": {
        "value": "604609522259200200000",
        "source": "epoch",
        "citation": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/."
      },
      "costDollars": {
        "value": "2873.986592",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2017-08-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "MSA Transformer",
    "fields": {
      "flops": {
        "value": "5.49E+21",
        "source": "epoch",
        "citation": "Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp"
      },
      "numParams": {
        "value": "100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "26000000",
        "source": "epoch",
        "citation": "\"Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.\""
      },
      "gpuCount": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-02-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Fusion in Encoder",
    "fields": {
      "flops": {
        "value": "1.3E+20",
        "source": "epoch",
        "citation": "\"The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.\"2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20"
      },
      "numParams": {
        "value": "330000000",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "48",
        "source": "epoch",
        "citation": "2 days"
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 80 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-11-18",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PhraseCond",
    "fields": {
      "numTokens": {
        "value": "4000000",
        "source": "epoch",
        "citation": "size of SQuAD 1.1"
      },
      "releaseDate": {
        "value": "2017-10-28",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ALBERT-xxlarge",
    "fields": {
      "flops": {
        "value": "2.39E+21",
        "source": "epoch",
        "citation": "32 hours of training512 TPU V3s0.33 utilization rate"
      },
      "numParams": {
        "value": "235000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "3300000000",
        "source": "epoch",
        "citation": "Pretraining same as for BERT - Wikipedia and BookCorpus\"For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)\""
      },
      "costDollars": {
        "value": "5924.431295",
        "source": "epoch",
        "citation": ""
      },
      "trainingTimeDays": {
        "value": "32",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "512",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-02-09",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "iGPT-XL",
    "fields": {
      "flops": {
        "value": "3.30E+22",
        "source": "epoch",
        "citation": "Taken from herehttps://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening"
      },
      "numParams": {
        "value": "6801000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "9600000",
        "source": "epoch",
        "citation": "\"We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.\"https://image-net.org/challenges/LSVRC/2012/\"The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.\""
      },
      "costDollars": {
        "value": "120440.9648",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA Tesla V100 DGXS 32 GB",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-06-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LLaVA + LVIS-INSTRUCT4V",
    "fields": {
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-11-13",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "YOLOX-X",
    "fields": {
      "numParams": {
        "value": "99100000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2500000",
        "source": "epoch",
        "citation": "2.5 million image-label pairs, per Coco paper https://arxiv.org/abs/1405.0312"
      },
      "gpuType": {
        "value": "NVIDIA V100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-08-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TinyLlama-1.1B (3T token checkpoint)",
    "fields": {
      "flops": {
        "value": "2.17E+22",
        "source": "epoch",
        "citation": "6ND approximation: 6*1.1B * 3T = 19800000000000000000000Extrapolation from the 1T checkpoint:flops = (16) * (312 * 10**12) * (3 * 30 * 24 * 3600) * (0.56) = 7245987840000001048576(num gpu) * (peak flops) * (time in seconds) * (reported utilization rate)source: https://github.com/jzhang38/TinyLlama\"Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization\"and Releases Schedule from the same link"
      },
      "numParams": {
        "value": "1100000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "750000000000",
        "source": "epoch",
        "citation": "1T tokens checkpoint so around 0.75T words"
      },
      "trainingTimeDays": {
        "value": "2160",
        "source": "epoch",
        "citation": "1T checkpoint was released after 1 month. Assume the 3T checkpoint took 3 months.source: https://github.com/jzhang38/TinyLlama"
      },
      "gpuCount": {
        "value": "16",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100 SXM4 40 GB",
        "source": "epoch",
        "citation": ""
      },
      "gpuUtilization": {
        "value": "0.56",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-10-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BatchNorm",
    "fields": {
      "releaseDate": {
        "value": "2015-06-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DBN for NLP",
    "fields": {
      "numParams": {
        "value": "1021535",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "178000",
        "source": "epoch",
        "citation": "The training data has 27K automatically transcribed utterances amounting to 178K words."
      },
      "releaseDate": {
        "value": "2014-02-11",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BAAI/bge-reranker-large",
    "fields": {
      "numParams": {
        "value": "560000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-09-14",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "PaLM 2",
    "fields": {
      "flops": {
        "value": "7.34E+24",
        "source": "epoch",
        "citation": "Compute Requirements \"Not reported.\"Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP."
      },
      "numParams": {
        "value": "340000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "2700000000000",
        "source": "epoch",
        "citation": "\"The pre-training corpus is significantly larger than the corpus used to train PaLM\" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html"
      },
      "gpuType": {
        "value": "Google TPU v4",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-05-10",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Local Transformer",
    "fields": {
      "releaseDate": {
        "value": "2020-03-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Gopher (7.1B)",
    "fields": {
      "releaseDate": {
        "value": "2021-12-08",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "IMPALA",
    "fields": {
      "flops": {
        "value": "1.68E+20",
        "source": "epoch",
        "citation": "Source: Ajeya Cotra and Tom Davidson, https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389"
      },
      "numParams": {
        "value": "1600000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "240000000000",
        "source": "epoch",
        "citation": "From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the \"dataset size\""
      },
      "trainingTimeDays": {
        "value": "100",
        "source": "epoch",
        "citation": "Maximum training time for IMPALA is 100 hours according to Figure 6. This seems to refer to the 1 GPU model. The 8 GPU model looks to have been trained about 1/8 as long."
      },
      "gpuCount": {
        "value": "1",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA P100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-05",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Cerebras-GPT-13B",
    "fields": {
      "flops": {
        "value": "2.30E+22",
        "source": "epoch",
        "citation": "2.3e22, per table 2"
      },
      "numParams": {
        "value": "13000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "278000000000",
        "source": "epoch",
        "citation": "371B tokens, or 278B words"
      },
      "batchSize": {
        "value": "2210000",
        "source": "epoch",
        "citation": "\"For the 13B parameter model, we train with a batch size of 720 sequences of length 2048 tokens for the first 84B tokens. At that point, we observed the gap between validation and train loss growing, indicating that the gradient noise was growing, so we increased the batch size to 1080 sequences for the rest of training.\"batch sizes ramp from 1.47M to 2.21M"
      },
      "gpuType": {
        "value": "Cerebras CS-2",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-04-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "TSN",
    "fields": {
      "releaseDate": {
        "value": "2016-09-17",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Photo-Geometric Autoencoder",
    "fields": {
      "releaseDate": {
        "value": "2019-11-25",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Deep Boltzmann Machines",
    "fields": {
      "releaseDate": {
        "value": "2009-04-16",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "GBERT-Large",
    "fields": {
      "flops": {
        "value": "2.24E+21",
        "source": "epoch",
        "citation": "flops = (64) * (123* 10**12) * (11 * 24 * 3600) * (0.3) = 2.24e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1it was trained for 11 days from Table 2"
      },
      "numParams": {
        "value": "335000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "27287800000",
        "source": "epoch",
        "citation": "163.4GB from Table 1 in the paperassuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0"
      },
      "trainingTimeDays": {
        "value": "264",
        "source": "epoch",
        "citation": "11 days from Table 2"
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-10-21",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "BERT-Large-CAS (PTB+WT2+WT103)",
    "fields": {
      "flops": {
        "value": "5.21E+20",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "395000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2019-04-20",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "CNN Best Practices",
    "fields": {
      "numTokens": {
        "value": "50000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2003-08-06",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ERNIE-Doc (247M)",
    "fields": {
      "flops": {
        "value": "2.91E+19",
        "source": "epoch",
        "citation": ""
      },
      "numParams": {
        "value": "247000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2020-12-31",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "Midjourney V1",
    "fields": {
      "releaseDate": {
        "value": "2022-02-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "LaMemo",
    "fields": {
      "numParams": {
        "value": "151000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2022-04-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DeciCoder-6B",
    "fields": {
      "numParams": {
        "value": "6000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2024-01-15",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "ELMo",
    "fields": {
      "numParams": {
        "value": "94000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2018-02-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "tsuzumi 7B",
    "fields": {
      "numParams": {
        "value": "7000000000",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2023-12-01",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "VD-LSTM+REAL Small",
    "fields": {
      "releaseDate": {
        "value": "2016-11-04",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "DLRM-12T",
    "fields": {
      "numParams": {
        "value": "12000000000000",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "NVIDIA A100",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-04-12",
        "source": "epoch",
        "citation": ""
      }
    }
  },
  {
    "model_name": "FLAN 137B",
    "fields": {
      "flops": {
        "value": "4.90E+22",
        "source": "epoch",
        "citation": "From section 2.4: \"60 hours on a TPUv3 with 128 cores.\" I assume that \"128 cores\" = 128 TPUv3s. Which took less than 2% of total time (see environmental considerations section)"
      },
      "numParams": {
        "value": "137000000000",
        "source": "epoch",
        "citation": ""
      },
      "numTokens": {
        "value": "1870000000000",
        "source": "epoch",
        "citation": "\"Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).\"2.49e12 tokens ~= 1.87e12 words"
      },
      "trainingTimeDays": {
        "value": "60",
        "source": "epoch",
        "citation": ""
      },
      "gpuCount": {
        "value": "64",
        "source": "epoch",
        "citation": ""
      },
      "gpuType": {
        "value": "Google TPU v3",
        "source": "epoch",
        "citation": ""
      },
      "releaseDate": {
        "value": "2021-09-03",
        "source": "epoch",
        "citation": ""
      }
    }
  }
]
