model_name,attribute,value,citation,is_from_epoch_ai
Sparse digit recognition SVM,releaseDate,2008-11-19,,True
Karakuri LM,numParams,70000000000.0,,True
Karakuri LM,releaseDate,2024-01-26,,True
DOT(S)-RNN,numParams,6160000.0,,True
DOT(S)-RNN,releaseDate,2013-12-20,,True
KEPLER,flops,124000000000000000000,"From author communication""About 128 GPU-days using Nvidia V100 (16GB). ""precision: float16V100 GPU for float16: 28000000000000 (2.8E+13)0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h= 1.24E+20",True
KEPLER,numParams,110000000.0,,True
KEPLER,numTokens,3300000000.0,"For BookCorpus + English Wikipedia: 800M + 2500MFor Wikidata5M: 20614279See table 1. Contains ""entities"", ""relations"", and ""triplets""",True
KEPLER,costDollars,437.967144712369,,True
KEPLER,releaseDate,2020-11-23,,True
DOC + Finetune∗ + Partial Shuffle (PTB),releaseDate,2019-03-11,,True
MV-RNN,numParams,3510255.0,,True
MV-RNN,releaseDate,2012-07-12,,True
n-gram LM,releaseDate,1997-07-01,,True
Wu Dao - Wen Yuan,flops,650280960000000000000,64 Nvidia V100 GPUs for two weeks64 GPUs * 2.8e13 FLOP/s /GPU * 14*24*60*60s * 0.3 [utilization rate],True
Wu Dao - Wen Yuan,numParams,2600000000.0,,True
Wu Dao - Wen Yuan,releaseDate,2021-01-11,,True
iGPT-L,flops,8.91e+21,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.[1] https://openai.com/blog/image-gpt/[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf[3] https://openai.com/blog/ai-and-compute/",True
iGPT-L,numParams,1362000000.0,,True
iGPT-L,numTokens,9600000.0,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""https://image-net.org/challenges/LSVRC/2012/""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""",True
iGPT-L,costDollars,32482.563232834,,True
iGPT-L,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
iGPT-L,releaseDate,2020-06-17,,True
VD-LSTM+REAL Large,flops,21300000000000000,,True
VD-LSTM+REAL Large,numParams,51000000.0,,True
VD-LSTM+REAL Large,releaseDate,2016-11-04,,True
DeepLab (2017),releaseDate,2017-04-27,,True
NEC LLM 13B,numParams,13000000000.0,,True
NEC LLM 13B,releaseDate,2023-07-06,,True
Fuyu-Heavy,numParams,100000000000.0,,True
Fuyu-Heavy,releaseDate,2024-01-24,,True
Deeply-recursive ConvNet,releaseDate,2016-11-11,,True
GCNN-14,releaseDate,2016-12-23,,True
ContextNet,numParams,112700000.0,,True
ContextNet,releaseDate,2020-05-07,,True
Claude,releaseDate,2023-03-14,,True
Kohonen network,numParams,4096.0,,True
Kohonen network,releaseDate,1981-07-25,,True
ResNet-110 (CIFAR-10),numParams,1700000.0,,True
ResNet-110 (CIFAR-10),releaseDate,2015-12-10,,True
GPT,flops,17578125000000000000,"COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""",True
GPT,numParams,117000000.0,,True
GPT,numTokens,1000000000.0,"""BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).""https://paperswithcode.com/dataset/bookcorpusBookCorpus seems to have about 5000MB of contentsource: https://huggingface.co/datasets/bookcorpusopenAssuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.So approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens",True
GPT,costDollars,68.7197473734963,,True
GPT,trainingTimeDays,720.0,"""1 month on 8 GPUs."" from the reference link",True
GPT,gpuCount,8.0,,True
GPT,gpuType,NVIDIA Quadro P600,,True
GPT,releaseDate,2018-06-01,,True
AlexNet + coordinating filters,flops,1557140125176000000,"The paper reimplements ResNet-20, AlexNet and GoogLeNet. The largest from these models is AlexNet. (data for these models taken form this db)training of AlexNet taken 470000000000000000.00 FLOPstraining of GoogLeNet taken 1557140125176000000.00 FLOPsmax(1557140125176000000, 470000000000000000) = 1557140125176000000source ""The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. """,True
AlexNet + coordinating filters,numParams,60000000.0,,True
AlexNet + coordinating filters,numTokens,1200000.0,size of ImageNet,True
AlexNet + coordinating filters,releaseDate,2017-03-28,,True
DeepSpeech2 (English),flops,26000000000000000000,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop = 26,000 PF = 0.30 pfs-daysSee also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",True
DeepSpeech2 (English),numParams,38000000.0,,True
DeepSpeech2 (English),numTokens,163339200.0,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""11,940 * 13,680 = 163339200",True
DeepSpeech2 (English),costDollars,150.778479610371,,True
DeepSpeech2 (English),gpuType,NVIDIA GTX Titan X,,True
DeepSpeech2 (English),gpuUtilization,0.45,,True
DeepSpeech2 (English),releaseDate,2015-12-08,,True
OneLLM,numParams,7000000000.0,,True
OneLLM,gpuCount,16.0,,True
OneLLM,gpuType,NVIDIA A100,,True
OneLLM,releaseDate,2023-12-06,,True
LSTM(large)+Sememe+cell,flops,24000000000000000,,True
LSTM(large)+Sememe+cell,numParams,48000000.0,,True
LSTM(large)+Sememe+cell,releaseDate,2019-10-20,,True
Adaptive Inputs + LayerDrop,numParams,423000000.00000006,,True
Adaptive Inputs + LayerDrop,releaseDate,2019-09-25,,True
Japanese-LM-3.6B,numParams,3600000000.0,,True
Japanese-LM-3.6B,numTokens,72000000000.0,"650GB per huggingfaceour guide says 111M Japanese words per GB, which would be ~72B wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",True
Japanese-LM-3.6B,gpuType,NVIDIA A100,,True
Japanese-LM-3.6B,releaseDate,2023-08-14,,True
Neural cache model (size=2000) (300M),numParams,300000000.0,,True
Neural cache model (size=2000) (300M),releaseDate,2016-12-13,,True
GPT-SW3,flops,1.3e+21,"From section 4: ""Training was performed on GPU resources from the Berzelius Superpod, which is currently the fastest supercomputer in Sweden, equipped with 60 Nvidia DGXA100 servers, each of which consists of 8 Nvidia A100GPUs with 320 GB Total GPU memory. Our trainingprocess took 2.5 days utilizing 16 of the DGX A100servers (in total 128 GPUs).""2.5*24*60**2 * 128 * 1.56E+14 * 0.3 = 1.3e21",True
GPT-SW3,numParams,3500000000.0,,True
GPT-SW3,numTokens,16700000000.0,"100GB Swedish corpus, assume Swedish has similar 167M words per GB as German.100*167e6 = 1.67e10",True
GPT-SW3,trainingTimeDays,60.0,"""Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs).""",True
GPT-SW3,gpuType,NVIDIA A100,,True
GPT-SW3,releaseDate,2022-06-25,,True
Make-A-Video,releaseDate,2022-09-29,,True
aLSTM(depth-2)+RecurrentPolicy (WT2),flops,75900000000000000,,True
aLSTM(depth-2)+RecurrentPolicy (WT2),numParams,32000000.0,,True
aLSTM(depth-2)+RecurrentPolicy (WT2),releaseDate,2018-05-22,,True
MENACE,releaseDate,1963-11-01,,True
GShard (600B),flops,1.33e+22,https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdfTable 4,True
GShard (600B),numParams,600000000000.0,,True
GShard (600B),numTokens,260000000000.0,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",True
GShard (600B),costDollars,27609.807429926,,True
GShard (600B),releaseDate,2020-06-30,,True
Bayesian Starcraft,numParams,13125.0,,True
Bayesian Starcraft,releaseDate,2011-08-31,,True
Conformer,numParams,118800000.0,,True
Conformer,releaseDate,2020-05-16,,True
Phi-2,flops,2.27e+22,"2.7B params, trained on 1.4T tokens2.7 billion * 1.4 trillion * 6 = 2.27e2296*14 A100-days14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22",True
Phi-2,numParams,2700000000.0,,True
Phi-2,trainingTimeDays,336.0,14 days,True
Phi-2,gpuType,NVIDIA A100,,True
Phi-2,releaseDate,2023-12-12,,True
Stable Beluga 2,numParams,70000000000.0,,True
Stable Beluga 2,releaseDate,2023-07-20,,True
LLaMA-33B,flops,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,True
LLaMA-33B,numParams,32500000000.0,,True
LLaMA-33B,batchSize,4000000.0,,True
LLaMA-33B,releaseDate,2023-02-27,,True
LLaMA-33B,batchSize,4000000.0,,True
Megatron-LM (355M),flops,335000000000000000000,"355M is a GPT-2-based model (Table 2).""For GPT-2 models, all training is performed with sequencesof 1024 subword units at a batch size of 512 for 300k iterations"" I interpret the above as 1024*512*300k = 157B training tokens (or things that require a forward+backward pass). 6 * 1024*512*300,000 * 355,000,000  = 3.35e20",True
Megatron-LM (355M),numParams,355000000.0,,True
Megatron-LM (355M),numTokens,34800000000.0,"""The resulting aggregatecorpus contains 174 GB of deduplicated text.""assuming 200M tokens per GB",True
Megatron-LM (355M),releaseDate,2019-09-17,,True
SEER,flops,4.42e+21,Numbers from section 3.2512 GPUs * 0.1 * 8days * 24h/day * 3600s/h * 125 TFLOP/s,True
SEER,numParams,1300000000.0,,True
SEER,numTokens,1000000000.0,"""Overall, we trainon 1B images for a total of 122K iterations.""",True
SEER,costDollars,16058.7953061202,,True
SEER,trainingTimeDays,192.0,8 days,True
SEER,gpuCount,512.0,,True
SEER,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
SEER,releaseDate,2021-07-29,,True
GGNN,gpuType,NVIDIA A100 SXM4 80 GB,,True
GGNN,releaseDate,2023-08-05,,True
CTC-Trained LSTM,numParams,114662.0,,True
CTC-Trained LSTM,numTokens,41620.0,"4162 utterances, guesstimated avg 10 words per utterance",True
CTC-Trained LSTM,releaseDate,2006-06-25,,True
GLM-10B-unidirectional,releaseDate,2021-03-18,,True
ABAB,releaseDate,2023-08-30,,True
Megatron-LM (8.3B),flops,9.1e+21,"source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodbother estimates:8.3B is a GPT-2-based model (Table 2). ""For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations"" I interpret the above as 1024*512*300k = 157B training tokens 6 * 157 billion * 8.3 billion  = 7.8e21Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)2.1 days per epoch, ~4.4 epochs2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22These are both close to the akronomicon estimate",True
Megatron-LM (8.3B),numParams,8300000000.0,,True
Megatron-LM (8.3B),numTokens,34800000000.0,"""The resulting aggregatecorpus contains 174 GB of deduplicated text.""",True
Megatron-LM (8.3B),costDollars,33212.5084740213,,True
Megatron-LM (8.3B),trainingTimeDays,327.0,Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUsAssume total compute is 9.1e21 FLOP.Then training time is 327 hours.https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29,True
Megatron-LM (8.3B),gpuCount,512.0,,True
Megatron-LM (8.3B),gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
Megatron-LM (8.3B),gpuUtilization,0.1162,,True
Megatron-LM (8.3B),releaseDate,2019-09-17,,True
Pattern recognition and reading by machine,numParams,2625.0,,True
Pattern recognition and reading by machine,releaseDate,1959-12-01,,True
ProGen,flops,370000000000000000000,"Our model was implemented in TensorFlow (Abadi et al.,2016) and trained with a global batch size of 64 distributedacross 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)4.00E+12*256*60**2*24*14*0.3 = 3.7e20",True
ProGen,numParams,1200000000.0,,True
ProGen,costDollars,623.75143437183,,True
ProGen,releaseDate,2020-03-13,,True
Adaptive Input Transformer + RD,flops,82000000000000000000,,True
Adaptive Input Transformer + RD,numParams,247000000.00000003,,True
Adaptive Input Transformer + RD,releaseDate,2021-06-28,,True
Refact-1.6B,flops,1.152e+22,"6ND = 6 * 1.6B * 1.2T = 11520000000000000000000 = 1.152e22citation ""model trained for 1.2T tokens. ""alternativeflops = (64) * (27770 * 10**9) * (28 * 24 * 3600) * (0.3) = 1.2898787328e+21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)Precision: bfloat16GPUs 64 NVidia A5000Training time 28 days27.77 TFLOPS - peak flop from https://www.techpowerup.com/gpu-specs/rtx-a5000.c3748",True
Refact-1.6B,numParams,1600000000.0,,True
Refact-1.6B,numTokens,1200000000000.0,"1.2T from ""The text to code proportion was 50:50, model trained for 1.2T tokens. """,True
Refact-1.6B,trainingTimeDays,672.0,"from ""Model Stats""",True
Refact-1.6B,gpuCount,6.0,,True
Refact-1.6B,gpuType,NVIDIA RTX A5000,,True
Refact-1.6B,releaseDate,2023-08-29,,True
aLSTM(depth-2)+RecurrentPolicy (PTB),releaseDate,2018-05-22,,True
Selfish-RNN (SNT-ASGD) Stacked LSTMs,flops,14000000000000000,,True
Selfish-RNN (SNT-ASGD) Stacked LSTMs,numParams,25200000.0,,True
Selfish-RNN (SNT-ASGD) Stacked LSTMs,releaseDate,2021-01-22,,True
ADP-FAIRSEQ+NGRAMRES,numParams,247000000.00000003,,True
ADP-FAIRSEQ+NGRAMRES,releaseDate,2022-10-26,,True
Punish/Reward,numParams,21.0,,True
Punish/Reward,releaseDate,1973-09-01,,True
RFA-GATE-Gaussian-Stateful Big,flops,7140000000000000000,,True
RFA-GATE-Gaussian-Stateful Big,numParams,242000000.0,,True
RFA-GATE-Gaussian-Stateful Big,releaseDate,2021-03-03,,True
DiffStk-MRNN,flops,282000000000000,,True
DiffStk-MRNN,numParams,1010000.0,,True
DiffStk-MRNN,releaseDate,2020-04-04,,True
DNCON2,flops,95000000000000000,"""Our training was conducted on Tesla K20 Nvidia GPUs each having 5 GB of GPU memory, on which, training one model took around 12 h.""""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""Assumptions:peakFLOP rate 3.52e12FLOP/s (from: https://www.techpowerup.com/gpu-specs/tesla-k20c.c564)30% utilization rate1 GPUEstimate 1: ""training one model took around 12h"" => unclear how many GPUs(12 *3600) s * 3.52e12 FLOP/s * 0.3 = 4.5e16 FLOPEstimate 2: ""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""(1600 epochs * 2 min/epoch * 60 s/min) * 3.52e12 FLOP/s * 0.3 =  2e17 FLOPGeometric mean: 9.5e16",True
DNCON2,numTokens,1426.0,"""Our raw feature files for all 1426 training proteins""",True
DNCON2,trainingTimeDays,12.0,"""training one model took around 12 h""",True
DNCON2,releaseDate,2018-05-01,,True
RNN+LDA+KN5+cache,numParams,9000000.0,,True
RNN+LDA+KN5+cache,releaseDate,2012-12-01,,True
Big-Little Net,numParams,77360000.0,,True
Big-Little Net,numTokens,1280000.0,,True
Big-Little Net,gpuType,NVIDIA Tesla K80,,True
Big-Little Net,releaseDate,2018-07-10,,True
Hyena-3-slim,numParams,125000000.0,,True
Hyena-3-slim,releaseDate,2023-02-21,,True
ERNIE-ViLG,numParams,10000000000.0,,True
ERNIE-ViLG,numTokens,145000000.0,"To explore the landscape of large-scale pre-training for bidirectional text-image generation,we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",True
ERNIE-ViLG,releaseDate,2021-12-31,,True
Meta Pseudo Labels,flops,4.79e+22,"From communication with author:22671 TPU days on specific hardware.Which hardware did you use and in which configuration?2048 cores of TPU v3.Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)So the compute estimate is:1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP",True
Meta Pseudo Labels,numParams,480000000.0,,True
Meta Pseudo Labels,numTokens,130000000.0,"Section 4Datasets. For this experiment, we use the entire ImageNettraining set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, andthen is filtered down to 130 million images by Noisy Studentusing confidence thresholds and up-sampling [77]. We usethe same 130 million images as Noisy Student",True
Meta Pseudo Labels,costDollars,369462.818207485,,True
Meta Pseudo Labels,trainingTimeDays,264.0,"11 days from section 4:""We train the model for 1 million steps in total,which takes about 11 days for EfficientNet-L2 and 10 daysfor EfficientNet-B6-Wide. """"Specifically, our training process runs on a cluster of 2,048TPUv3 cores. """,True
Meta Pseudo Labels,gpuCount,1024.0,,True
Meta Pseudo Labels,gpuType,Google TPU v3,,True
Meta Pseudo Labels,releaseDate,2021-03-01,,True
RT-1,numParams,35000000.0,,True
RT-1,releaseDate,2022-12-13,,True
True-Regularization+Finetune+Dynamic-Eval,numParams,7000000.0,,True
True-Regularization+Finetune+Dynamic-Eval,releaseDate,2019-04-08,,True
M6-10B,numParams,10000000000.0,,True
M6-10B,numTokens,1900000000000.0,"""1.9TB images and 292GB texts""TODO: figure out what to do for multimodal pretraining datasets",True
M6-10B,releaseDate,2021-03-01,,True
4 layer QRNN + dynamic evaluation,releaseDate,2019-06-10,,True
WizardLM 70B,numParams,70000000000.0,,True
WizardLM 70B,releaseDate,2023-04-24,,True
AlphaFold-Multimer,flops,4.35e+21,"Section: 2.5. Training Regimen""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""Assuming: FP16 and utilization 0.4Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs",True
AlphaFold-Multimer,numTokens,147328.0,See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018,True
AlphaFold-Multimer,trainingTimeDays,384.0,"Section: 2.5. Training Regimen""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""",True
AlphaFold-Multimer,gpuCount,64.0,,True
AlphaFold-Multimer,gpuType,Google TPU v3,,True
AlphaFold-Multimer,releaseDate,2021-10-04,,True
PolySphere-1,numParams,14000000000.0,,True
PolySphere-1,releaseDate,2023-06-08,,True
FMMformer (2-kernel fast weight + Band20),flops,430000000000000000,,True
FMMformer (2-kernel fast weight + Band20),numParams,40000000.0,,True
FMMformer (2-kernel fast weight + Band20),releaseDate,2021-08-05,,True
Sandstorm (DARPA Grand Challenge I),releaseDate,2004-06-14,,True
Lyra-Fr 10B,numParams,10000000000.0,,True
Lyra-Fr 10B,releaseDate,2023-12-15,,True
EDSR,releaseDate,2017-06-10,,True
LLaMA-65B,flops,5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOPCompared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29",True
LLaMA-65B,numParams,65200000000.0,,True
LLaMA-65B,numTokens,1050000000000.0,1.4 trillion tokens * 0.75 words/token = 1.05 trillion words,True
LLaMA-65B,batchSize,4000000.0,,True
LLaMA-65B,costDollars,1179384.75,"1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.$1391674 / 1.18 = $1179385 in 2020 USD.",True
LLaMA-65B,trainingTimeDays,500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",True
LLaMA-65B,gpuCount,2048.0,,True
LLaMA-65B,gpuType,NVIDIA A100,,True
LLaMA-65B,gpuUtilization,0.4746,,True
LLaMA-65B,releaseDate,2023-02-24,,True
LLaMA-65B,batchSize,4000000.0,,True
OpenAI Five Rerun,flops,1.3e+22,THIS CALCULATION IS FOR RERUNsource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,True
OpenAI Five Rerun,numParams,159000000.0,,True
OpenAI Five Rerun,numTokens,53084160000.0,54k iterations (Fig 7)with a batch size of 983040 (Table 2),True
OpenAI Five Rerun,costDollars,32217.1266906017,,True
OpenAI Five Rerun,gpuCount,512.0,,True
OpenAI Five Rerun,releaseDate,2019-12-13,,True
NEAT in neuroevolution,releaseDate,2002-06-01,,True
LSTM (WT103),releaseDate,2016-12-13,,True
Falcon 180B,flops,3.76e+24,"43,500 petaflop-days per Table 1 of the paper43500 * 1e15 * 24 * 3600 = 3.76e24C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",True
Falcon 180B,numParams,180000000000.0,,True
Falcon 180B,numTokens,2625000000000.0,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,True
Falcon 180B,batchSize,4194304.0,,True
Falcon 180B,trainingTimeDays,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",True
Falcon 180B,gpuCount,4096.0,,True
Falcon 180B,gpuType,NVIDIA A100 SXM4 40 GB,,True
Falcon 180B,gpuUtilization,0.1876,,True
Falcon 180B,releaseDate,2023-09-06,,True
Falcon 180B,batchSize,4194304.0,"from paper (https://arxiv.org/pdf/2311.16867.pdf):Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.""All Falcon models are pretrained with a 2,048 sequence length""2048*2048 = 4194304",True
CALM,gpuType,NVIDIA A100,,True
CALM,releaseDate,2023-08-06,,True
Mistral Medium,releaseDate,2023-12-11,,True
Japanese StableLM Base Alpha 7B,flops,3.15e+22,"7b params, 750b tokens7b * 750b * 6 = 3.15e22",True
Japanese StableLM Base Alpha 7B,numParams,7000000000.0,,True
Japanese StableLM Base Alpha 7B,releaseDate,2023-08-10,,True
TrellisNet-MoS (1.4x larger),releaseDate,2018-10-15,,True
BloombergGPT,flops,2.36e+23,"2.36e23 per Table 4(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",True
BloombergGPT,numParams,50558868480.0,,True
BloombergGPT,numTokens,532000000000.0,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",True
BloombergGPT,batchSize,4200000.0,,True
BloombergGPT,trainingTimeDays,1270.0,"""~53 days""",True
BloombergGPT,gpuCount,512.0,,True
BloombergGPT,gpuType,NVIDIA A100,,True
BloombergGPT,gpuUtilization,0.32,,True
BloombergGPT,releaseDate,2023-03-30,,True
BloombergGPT,batchSize,4200000.0,"""in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.""",True
ERNIE-GEN (large),numParams,340000000.0,,True
ERNIE-GEN (large),releaseDate,2020-08-06,,True
TransfoRNN(d=1024)(2-layer) (WT2),numParams,97600000.0,,True
TransfoRNN(d=1024)(2-layer) (WT2),releaseDate,2021-04-04,,True
SenseTime SenseNova,releaseDate,2023-04-10,,True
CD-GraB (WT103),releaseDate,2023-02-02,,True
base LM+GNN+kNN,flops,7300000000000000000,,True
base LM+GNN+kNN,numParams,274000000.0,,True
base LM+GNN+kNN,releaseDate,2021-10-17,,True
TranceptEve,releaseDate,2022-12-10,,True
Mesh-TensorFlow Transformer 4.9B (language modelling),flops,161740800000000000000,flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 45TFLOPs per chips,True
Mesh-TensorFlow Transformer 4.9B (language modelling),numParams,4900000000.0,,True
Mesh-TensorFlow Transformer 4.9B (language modelling),numTokens,4750000000.0,"from section 9.1 Wikipedia and one-billion-word language modeling benchmark.there were around 5B tokens from wikipedia - assuming 0,75 words per token we have 3750000000.0 words from wikipedia and 1B words from the benchmarkand citation from section 9.1: 'We have included random samples from these models in Appendix C. On the languagemodel_wiki_noref_v128k_l1k dataset from the Tensor2Tensor library1, consisting of over 5 billion tokens of text from Wikipedia, perplexity continued to improve significantly with a model size of 5 billion parameters.'",True
Mesh-TensorFlow Transformer 4.9B (language modelling),trainingTimeDays,13.0,"from section 9.1 ""For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.""",True
Mesh-TensorFlow Transformer 4.9B (language modelling),gpuCount,256.0,,True
Mesh-TensorFlow Transformer 4.9B (language modelling),gpuType,Google TPU v2,,True
Mesh-TensorFlow Transformer 4.9B (language modelling),releaseDate,2018-11-05,,True
FAST,releaseDate,2006-05-07,,True
DeepSeek Coder 33B,flops,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",True
DeepSeek Coder 33B,numParams,33000000000.0,,True
DeepSeek Coder 33B,releaseDate,2023-11-02,,True
Transformer + Average Attention Network,releaseDate,2019-01-01,,True
MegaSyn,releaseDate,2022-03-07,,True
M6-100B,numParams,100000000000.0,,True
M6-100B,numTokens,1900000000000.0,"""1.9TB images and 292GB texts""TODO: figure out what to do for multimodal pretraining datasets",True
M6-100B,releaseDate,2021-03-01,,True
OpenChat 3.5-7B,numParams,7000000000.0,,True
OpenChat 3.5-7B,releaseDate,2023-11-01,,True
UniRep,flops,22000000000000000000,"""Training was performed using data parallelism on four Nvidia K80 GPUs (mLSTM-1,900) or two Nvidia K-40s (4× mLSTM-256, 4× mLSTM-64). The mLSTM-1,900 model was trained for ~770,000 weight updates, or ~3.5 weeks wall clock time, corresponding to ~1 epoch."" [Methods - Unsupervised training dataset]Assuming 30% utilization rate and single-precision performanceEstimate: 3.5 weeks * 7 days/week * 24 hours/day * 60 min/hour * 60 sec/min * 4 GPUs *8.73e12 FLOP/sec * 0.3",True
UniRep,numParams,18200000.0,,True
UniRep,trainingTimeDays,588.0,,True
UniRep,gpuCount,4.0,,True
UniRep,gpuType,NVIDIA Tesla K80,,True
UniRep,releaseDate,2019-03-26,,True
SemVec,releaseDate,2013-06-09,,True
Semantic Hashing,numParams,2600000.0,,True
Semantic Hashing,numTokens,310521.0,Section 4.1,True
Semantic Hashing,releaseDate,2008-12-10,,True
LongT5,numParams,3000000000.0,,True
LongT5,numTokens,562500000000.0,"size of C4, from https://huggingface.co/datasets/c4 , C4 dataset is a collection of about 750GB of English-language text, so around 0.75 * 750e9 = 562500000000 words",True
LongT5,gpuCount,128.0,,True
LongT5,gpuType,Google TPU v3,,True
LongT5,releaseDate,2021-12-15,,True
BigBiGAN,numParams,86000000.0,,True
BigBiGAN,releaseDate,2019-07-04,,True
Lyria,releaseDate,2023-11-16,,True
TransformerXL-LayerFusion-CA,releaseDate,2020-07-29,,True
WaveNet,releaseDate,2016-09-12,,True
CT-MoS + DynamicEval (WT2),flops,562000000000000000,,True
CT-MoS + DynamicEval (WT2),numParams,45000000.0,,True
CT-MoS + DynamicEval (WT2),releaseDate,2020-12-25,,True
Transformer LM + MinSen,releaseDate,2021-11-29,,True
Player of Games,releaseDate,2021-12-06,,True
GPT-Neo-125M(finetuned),releaseDate,2021-03-21,,True
RQ-Transformer (LSUN-cat dataset),flops,2911334400000000000,"flops = (4) * (3120 * 10**9) * (9*24 * 3600) * (0.3) = 2911334400000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""we provide details for LSUN-cat with largest compute",True
RQ-Transformer (LSUN-cat dataset),numParams,612000000.0,,True
RQ-Transformer (LSUN-cat dataset),numTokens,1657266.0,size of LSUN-cat,True
RQ-Transformer (LSUN-cat dataset),trainingTimeDays,216.0,"9 days for LSUN-cat""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,True
RQ-Transformer (LSUN-cat dataset),gpuCount,4.0,,True
RQ-Transformer (LSUN-cat dataset),gpuType,NVIDIA A100,,True
RQ-Transformer (LSUN-cat dataset),releaseDate,2022-03-03,,True
ProteinDT,releaseDate,2023-02-09,,True
MADALINE II,releaseDate,1988-07-24,,True
Otter,numParams,1300000000.0,,True
Otter,gpuCount,4.0,,True
Otter,gpuType,NVIDIA GeForce RTX 3090,,True
Otter,releaseDate,2023-05-05,,True
Deeply-supervised nets,numTokens,870000.0,60000+50000+60000+600000,True
Deeply-supervised nets,releaseDate,2014-09-18,,True
GPT2-LayerFusion-WS,releaseDate,2020-07-29,,True
data2vec (vision),numParams,705134592.0,,True
data2vec (vision),numTokens,1281167.0,"Section 5.1: ""we pretrain data2vec on the images of the ImageNet-1K trainingset""",True
data2vec (vision),releaseDate,2022-01-20,,True
BlenderBot 3,flops,4.3e+23,(taken from OPT-175 base),True
BlenderBot 3,numParams,175000000000.0,,True
BlenderBot 3,batchSize,262144.0,,True
BlenderBot 3,gpuCount,128.0,,True
BlenderBot 3,gpuType,NVIDIA A100 SXM4 40 GB,,True
BlenderBot 3,releaseDate,2022-08-10,,True
BlenderBot 3,batchSize,262144.0,"Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.""The 175B model was trained with a batch size of 2^18""2^18 = 262144",True
4 layer Densely Connected LSTM,releaseDate,2017-07-19,,True
PLaMo-13B,flops,1.17e+23,6ND = 6*13e9*1.5e12=1.17e+23from https://huggingface.co/pfnet/plamo-13b#model-details,True
PLaMo-13B,numParams,13000000000.0,,True
PLaMo-13B,numTokens,1170000000000.0,0.75*1.32T + 0.18T = 11700000000000.75 words per token for English1 for Japanese ,True
PLaMo-13B,releaseDate,2023-09-28,,True
StableLM-Base-Alpha-7B,flops,4.5e+22,"""StableLM-Base-Alpha-7B-v2 is pre-trained using a multi-stage context length extension schedule following similar work (Nijkamp et al. 2023); first pre-training at a context length of 2048 for 1 trillion tokens, then fine-tuning at a context length of 4096 for another 100B tokens""6890209280 params * 1.1 trillion tokens * 6 = 4.5e22alternatively: ""StableLM-Base-Alpha-7B-v2 was trained on the Stability AI cluster - occupying 384 NVIDIA A100 40GB GPUs across AWS P4d instances. Training took approximately 16.33 days to complete across both stages.""312 teraflops * 384 * 16.33 * 24 * 3600 * 0.3 = 5.07e22",True
StableLM-Base-Alpha-7B,numParams,6890209280.0,,True
StableLM-Base-Alpha-7B,numTokens,750000000000.0,1 trillion tokens,True
StableLM-Base-Alpha-7B,trainingTimeDays,392.0,16.33 days,True
StableLM-Base-Alpha-7B,gpuType,NVIDIA A100 SXM4 40 GB,,True
StableLM-Base-Alpha-7B,releaseDate,2023-08-05,,True
R-CNN (T-net),numParams,69003872.0,,True
R-CNN (T-net),releaseDate,2013-11-11,,True
Fraternal dropout + AWD-LSTM 3-layer (WT2),flops,98500000000000000,,True
Fraternal dropout + AWD-LSTM 3-layer (WT2),numParams,34000000.0,,True
Fraternal dropout + AWD-LSTM 3-layer (WT2),releaseDate,2017-10-31,,True
KoGPT,numParams,6166502400.0,,True
KoGPT,releaseDate,2021-11-12,,True
DARTS (second order),releaseDate,2018-06-24,,True
Deep rectifier networks,releaseDate,2011-04-13,,True
HyperCLOVA,flops,1.476e+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOPCalculation using GPU time corroborates this:- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.- Assume the default of 30% utilization rate for large language models.1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",True
HyperCLOVA,numParams,82000000000.0,,True
HyperCLOVA,numTokens,190000000000.0,"""We introduce HyperCLOVA, a large-scaleKorean in-context learning-based LM withnearly 100B parameters, by constructing alarge Korean-centric corpus of 560B tokens.""Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",True
HyperCLOVA,costDollars,103802.314398667,,True
HyperCLOVA,trainingTimeDays,643.2,see compute notes,True
HyperCLOVA,gpuCount,1024.0,,True
HyperCLOVA,gpuType,NVIDIA A100,,True
HyperCLOVA,gpuUtilization,0.2,,True
HyperCLOVA,releaseDate,2021-09-10,,True
Base LM + kNN LM + Continuous Cache,flops,7300000000000000000,,True
Base LM + kNN LM + Continuous Cache,numParams,247000000.00000003,,True
Base LM + kNN LM + Continuous Cache,releaseDate,2019-11-01,,True
LongNet,flops,4.86e+21,"2.7B params * 300B tokens * 6 = 4.86e21Note: not sure if there are very long sequences in the training data that would affect this calculation. Per paper, complexity of their attention mechanism scales linearly with sequence length.",True
LongNet,numParams,2700000000.0,,True
LongNet,releaseDate,2023-07-05,,True
Cross-Lingual POS Tagger,releaseDate,2011-06-19,,True
Local Binary Patterns for facial recognition,releaseDate,2006-12-01,,True
Constituency-Tree LSTM,numParams,205190.0,,True
Constituency-Tree LSTM,releaseDate,2015-02-28,,True
ALVINN,flops,81187041441.20895,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
ALVINN,numParams,3994.0,,True
ALVINN,numTokens,1200.0,"""Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels""",True
ALVINN,releaseDate,1989-12-01,,True
Subformer (96M),releaseDate,2021-01-01,,True
SparseOPT-30B,numParams,30000000000.0,,True
SparseOPT-30B,releaseDate,2023-01-02,,True
Transformer-XL + AutoDropout (PTB),releaseDate,2021-01-05,,True
Netflix Recommender System,releaseDate,2015-12-01,,True
QMoE: compressed 1T model,numParams,1600000000000.0,,True
QMoE: compressed 1T model,releaseDate,2023-10-25,,True
AWD-LSTM-DOC (fin) (23M),releaseDate,2018-08-30,,True
LSTM+Adam+Lookahead,numParams,7190000.0,,True
LSTM+Adam+Lookahead,releaseDate,2019-07-19,,True
DINOv2,flops,7.41851136e+21,table 1422016 * 3600 * 312 * 10 ** 12 * 3//10 = 7.41851136e+21gpu hours in seconds * flops of A100 * assumed utilization  rate,True
DINOv2,numParams,1140000000.0,,True
DINOv2,numTokens,142000000.0,new dataset  - named LVD142M Table 15,True
DINOv2,gpuType,NVIDIA A100 SXM4 40 GB,,True
DINOv2,releaseDate,2023-04-14,,True
SRGAN,releaseDate,2017-05-25,,True
ULM-FiT,releaseDate,2018-01-18,,True
Claude 3 Opus,releaseDate,2024-03-04,,True
MobileNet,numParams,4200000.0,,True
MobileNet,releaseDate,2017-04-17,,True
Joint Probability Machine Translation,numTokens,1073480.0,"[WORDS]""To evaluate our system, we trained [...] our jointprobability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132unique tokens)""",True
Joint Probability Machine Translation,releaseDate,2002-06-01,,True
Subformer (83M),releaseDate,2021-01-01,,True
T2R + Pretrain,releaseDate,2021-03-24,,True
BLOOM-1B,numParams,1000000000.0,,True
BLOOM-1B,releaseDate,2022-07-05,,True
Tensorized Transformer (151M),releaseDate,2019-06-24,,True
AlphaGo Master,flops,1.5e+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: https://deepmind.com/blog/article/alphago-zero-starting-scratchSince AGZ reaches the ELO of AlphaGo Master in about 20 days (half of the total training time), I estimate the compute to be around half that of AGZ. I round this down to 1.5e23, and I expect this to only be accurate within an OOM.",True
AlphaGo Master,costDollars,852748.081042578,,True
AlphaGo Master,gpuType,Google TPU v1,,True
AlphaGo Master,releaseDate,2017-01-01,,True
XGLM-7.5B,flops,4.347592704e+22,""" The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""so 312e12 * 256 * 3*7*24*3600 *0.3 = 4.347592704e+22alternative:6ND = 6*7.5e9*500e9 = 2.25e22 - we have 7.5B params and 500B tokens from ""we train four multilingual generative language models (up to 7.5 billion parameters),XGLM’s, and present a comprehensive study ofmultilingual zero- and in-context few-shot learning.We train the models using a large-scale corpus of500B tokens that comprises 30 diverse languages""",True
XGLM-7.5B,numParams,7500000000.0,,True
XGLM-7.5B,numTokens,565367040000.0,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""so 311.6e3 * 3*7*24*3600 = 565367040000 words- around 1.13 words per token",True
XGLM-7.5B,trainingTimeDays,504.0,"appendix A : ""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",True
XGLM-7.5B,gpuCount,256.0,,True
XGLM-7.5B,gpuType,NVIDIA A100,,True
XGLM-7.5B,releaseDate,2021-12-20,,True
Transformer (Adaptive Input Embeddings),flops,73000000000000000000,Table 2. 8 * 67 GPU-hours. GPU is V100125 teraflop/s * 8 * 67 * 3600 * 0.3 = ~7.3e19,True
Transformer (Adaptive Input Embeddings),numParams,247000000.00000003,,True
Transformer (Adaptive Input Embeddings),trainingTimeDays,67.0,,True
Transformer (Adaptive Input Embeddings),gpuCount,8.0,,True
Transformer (Adaptive Input Embeddings),gpuType,NVIDIA V100,,True
Transformer (Adaptive Input Embeddings),releaseDate,2018-09-28,,True
AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),releaseDate,2018-08-14,,True
Graph-based structural reasoning,releaseDate,1970-09-01,,True
ProteinLM,flops,1.6e+22,"""We pretrained two large models on a 480 GPUs (TeslaV100-32GB) cluster for about three weeks""21 * 24* 3600 * 480 * 125 teraFLOP/s * 0.3 (utilization) * 0.5 (two models) = 1.6e22",True
ProteinLM,numParams,3000000000.0,,True
ProteinLM,trainingTimeDays,252.0,,True
ProteinLM,gpuCount,48.0,,True
ProteinLM,gpuType,NVIDIA V100,,True
ProteinLM,releaseDate,2021-08-17,,True
Reading Twice for NLU,releaseDate,2017-06-08,,True
Hide and Seek,flops,1150000000000000000,"1.6e6 * 2 * 120e9 * 3 ~= 1.15e18 FLOP. We assume the single convolution on the lidar input is negligible, and the rest of the model (which consists of MLPs, self-attention and LSTM) has roughly 1 multiply-add per parameter. The following source has a similar estimate but does not adjust for the full number of episodes: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",True
Hide and Seek,numParams,1600000.0,,True
Hide and Seek,numTokens,120000000000.0,"""The default model, which uses a batch size of 64,000 and 1.6 million parameters, requires 132.3 million episodes (31.7 billion frames) over 34 hours of training to reach stage 4 of the skill progression, i.e. ramp defense."" But the full number of episodes, e.g. Figure 1, is 500 million. 500 / 132.3 * 31.7B ~= 120B.",True
Hide and Seek,costDollars,0.79509660347948,,True
Hide and Seek,releaseDate,2019-09-17,,True
Verbatim Memory Transformer (108M),releaseDate,2022-10-24,,True
Genetic algorithm,releaseDate,1954-07-02,,True
XLM-RoBERTa,numParams,550000000.0,,True
XLM-RoBERTa,numTokens,125250000000.0,size of CC100 - copied from other rows,True
XLM-RoBERTa,gpuCount,500.0,,True
XLM-RoBERTa,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
XLM-RoBERTa,releaseDate,2019-11-05,,True
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)",releaseDate,2017-11-10,,True
WizardLM-7B,flops,4.02e+22,"""We use pre-trained LLaMA 7B [4] to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 ×10−5, a maximum number of tokens 2048, and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs""Llama-7b was ~4e22. 8*70 V100-hours is ~2e20, so fine-tuning was <1% of base training.",True
WizardLM-7B,numParams,6700000000.0,,True
WizardLM-7B,trainingTimeDays,70.0,,True
WizardLM-7B,gpuCount,8.0,,True
WizardLM-7B,gpuType,NVIDIA V100,,True
WizardLM-7B,releaseDate,2023-04-24,,True
BLOOM-3B,numParams,3000000000.0,,True
BLOOM-3B,releaseDate,2022-07-05,,True
DDPM-IP (CelebA),flops,350000000000000000000,"""We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). Inmore detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32×32for 34 days. For LSUN tower 64×64, CelebA 64×64 and FFHQ 128×128, we used 16 GPUs to train the models for 3 days,5 days and 4 days, respectively""5*16 V100-days for CelebA.5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",True
DDPM-IP (CelebA),numParams,295000000.0,,True
DDPM-IP (CelebA),numTokens,203000.0,,True
DDPM-IP (CelebA),trainingTimeDays,120.0,5 days,True
DDPM-IP (CelebA),gpuType,NVIDIA V100,,True
DDPM-IP (CelebA),releaseDate,2023-01-27,,True
GLIDE,numParams,3500000000.0,,True
GLIDE,numTokens,250000000.0,"Section 4:""We train our model on the same dataset as DALL-E (Rameshet al., 2021)""This paper used 250M image-text pairshttps://arxiv.org/pdf/2102.12092.pdf",True
GLIDE,releaseDate,2021-12-20,,True
Stable Diffusion XL,numParams,3400000000.0,,True
Stable Diffusion XL,releaseDate,2023-07-04,,True
Naive Bayes,releaseDate,1974-09-01,,True
Routing Transformer,numParams,79500000.0,,True
Routing Transformer,releaseDate,2020-03-12,,True
OpenFlamingo,numParams,9000000000.0,,True
OpenFlamingo,numTokens,180000000.0,"""OpenFlamingo models were trained for 60M interleaved (MMC4) examples1 and 120M LAION-2B examples.""",True
OpenFlamingo,gpuCount,64.0,,True
OpenFlamingo,gpuType,NVIDIA A100 SXM4 80 GB,,True
OpenFlamingo,releaseDate,2023-08-02,,True
CoCa,flops,7.3e+22,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22",True
CoCa,numParams,2100000000.0,,True
CoCa,numTokens,4800000000.0,"JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images",True
CoCa,trainingTimeDays,120.0,5 days,True
CoCa,gpuCount,2048.0,,True
CoCa,gpuType,Google TPU v4,,True
CoCa,releaseDate,2022-06-14,,True
MADALINE I,releaseDate,1962-07-01,,True
TeleChat,releaseDate,2023-07-07,,True
RQ-Transformer (1.4B params ImageNet dataset),flops,2911334400000000000,"flops = (8) * (3120 * 10**9) * (4.5*24 * 3600) * (0.3) = 2911334400000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""we provide details for LSUN-cat with largest compute",True
RQ-Transformer (1.4B params ImageNet dataset),numParams,1388000000.0,,True
RQ-Transformer (1.4B params ImageNet dataset),numTokens,1200000.0,size of ImageNet,True
RQ-Transformer (1.4B params ImageNet dataset),trainingTimeDays,108.0,"4.5  days for ImageNet for 1.4B model""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,True
RQ-Transformer (1.4B params ImageNet dataset),gpuCount,8.0,,True
RQ-Transformer (1.4B params ImageNet dataset),gpuType,NVIDIA A100,,True
RQ-Transformer (1.4B params ImageNet dataset),releaseDate,2022-03-03,,True
Trajectory-pooled conv nets,numParams,9106245.0,,True
Trajectory-pooled conv nets,releaseDate,2015-05-19,,True
Unsupervised High-level Feature Learner,flops,600000000000000000,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",True
Unsupervised High-level Feature Learner,numParams,1000000000.0,,True
Unsupervised High-level Feature Learner,numTokens,10000000.0,10 million 200x200 images extracted from Youtube videos,True
Unsupervised High-level Feature Learner,trainingTimeDays,72.0,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,True
Unsupervised High-level Feature Learner,releaseDate,2012-07-12,,True
GPT-Neo-1.3B (finetuned),releaseDate,2021-03-21,,True
UL2,flops,1.2e+23,Trained on 1T tokens20B * 1T * 6 = 1.2e23 ,True
UL2,numParams,20000000000.0,,True
UL2,numTokens,750000000000.0,"1T tokens, assuming 0.75 words per token we have 0.75T words",True
UL2,batchSize,65536.0,,True
UL2,trainingTimeDays,744.0,around 31 days from 'Pre-training took approximately slight more than one month for about 1 trilliontokens.' from section 5.1so around 31*24 = 744,True
UL2,gpuCount,512.0,,True
UL2,gpuType,Google TPU v4,,True
UL2,gpuUtilization,0.318,,True
UL2,releaseDate,2022-05-10,,True
UL2,batchSize,65536.0,"""We pre-train all models for 500K steps with a batch size of 128 and a sequence length of 512 inputs and 512 targets using the C4 corpus. The total approximate tokens seen during pre-training is approximately 32 billion tokens.""500k*128*512 ~= 32B128*512=65,536",True
SVM for face detection,numTokens,50000.0,"Section 1: ""The problem that we have to solve involves training a classifierto discriminate between face and non-face patterns, using adata set of 50,000points. """,True
SVM for face detection,releaseDate,1997-06-17,,True
Compressive Transformers for Long-Range Sequence Modelling,flops,160000000000000000000,,True
Compressive Transformers for Long-Range Sequence Modelling,releaseDate,2019-11-13,,True
DNABERT,flops,140000000000000000000,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""Assuming FP16 and 30% utilizationCalculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",True
DNABERT,numParams,110000000.0,,True
DNABERT,trainingTimeDays,600.0,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""",True
DNABERT,gpuType,NVIDIA GeForce RTX 2080 Ti,,True
DNABERT,releaseDate,2021-08-15,,True
360 Smart Brain,releaseDate,2023-09-04,,True
TriNet,releaseDate,2017-11-21,,True
"GPT-2 (117M, SLW 110K)",releaseDate,2021-08-13,,True
Unsupervised Scale-Invariant Learning,numParams,451.0,,True
Unsupervised Scale-Invariant Learning,numTokens,3500.0,"See Table 2 and Figure 1.There are 7 datasets, each with 200-800 of pictures. I pick 500 as the avg number of pictures",True
Unsupervised Scale-Invariant Learning,releaseDate,2003-06-18,,True
RL for helicopter flight,releaseDate,2006-03-09,,True
STT Conformer-Transducer XL,numParams,600000000.0,,True
STT Conformer-Transducer XL,releaseDate,2022-04-12,,True
DiT-XL/2 + Discriminator Guidance,gpuType,NVIDIA A100,,True
DiT-XL/2 + Discriminator Guidance,releaseDate,2022-11-28,,True
WeNet (WT2),numParams,33000000.0,,True
WeNet (WT2),releaseDate,2019-04-08,,True
BOXES,releaseDate,1968-07-01,,True
Big-Little Net (speech),flops,429004800000000000,980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000,True
Big-Little Net (speech),numParams,3320000.0,,True
Big-Little Net (speech),numTokens,27360000.0,"""We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers""2000h * 13680 words per hour = 27360000https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",True
Big-Little Net (speech),releaseDate,2018-07-10,,True
Fisher-Boost,releaseDate,2010-09-05,,True
HuBERT,flops,5.54e+21,GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/sNumbers from Section IV part C0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU,True
HuBERT,numParams,1000000000.0,,True
HuBERT,numTokens,820800000.0,"""When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.""1h ~ 13,680 words13,680 * 60,000 = 820800000",True
HuBERT,costDollars,8632.10664479602,,True
HuBERT,releaseDate,2021-07-27,,True
Spatiotemporal fusion ConvNet,numTokens,97200.0,"[SECONDS OF VIDEO]They use UCF101, whose paper says""We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data""",True
Spatiotemporal fusion ConvNet,releaseDate,2016-06-01,,True
Support Vector Machines,numParams,100000000.0,,True
Support Vector Machines,numTokens,60000.0,"Section 6.2: ""The large database consists of 60,000 training and 10,000 test patterns""",True
Support Vector Machines,releaseDate,1995-09-01,,True
ESM2-35M,flops,210000000000000000000,"""All language models were trained for 500K updates, except the 15B language model"" ""All models used 2 million tokens as batch size except the 15B model""[Supplementary Materials]Hence: 1000B training tokens (500k steps, 2M tokens/batch)Estimate: 35M*2*1000B + 35M*4*1000B",True
ESM2-35M,numParams,35000000.0,,True
ESM2-35M,releaseDate,2022-07-21,,True
Rational DQN Average,numParams,1683456.0,,True
Rational DQN Average,releaseDate,2021-02-18,,True
DeepLabV3+,releaseDate,2018-02-07,,True
Projected GAN,gpuType,"NVIDIA V100,NVIDIA Quadro RTX 6000",,True
Projected GAN,releaseDate,2021-11-01,,True
AlphaFold 2,flops,2.99e+21,"123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOPhttps://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4""Training regimen"" section: ""We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPUcore, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.""",True
AlphaFold 2,numTokens,530000.0,"3 different types of input data to the network:(1) Amino acid sequence(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins(3) Template structures (3D atom coordinates of homologous structures), where availableTraining data is processed into the following two datasets that are sampled with different probabilities. Supplementary Material, Section 1.2.4. Training data:""With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank""Supplementary Material, Section 1.3 Self-distillation dataset:""This gives a final dataset of 355,993 sequences"". An initial model was used to predict structures for these sequences.PDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures172788Therefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]",True
AlphaFold 2,trainingTimeDays,264.0,7 days pretrain and 4 days finetune,True
AlphaFold 2,gpuType,Google TPU v3,,True
AlphaFold 2,releaseDate,2020-11-30,,True
mini-GPT-2+Active-AdamW,releaseDate,2023-01-24,,True
Agent57,releaseDate,2020-03-30,,True
Claude 2.1,releaseDate,2023-11-21,,True
SpecAugment,releaseDate,2019-04-18,,True
Japanese-GPT-1B,numParams,1300000000.0,,True
Japanese-GPT-1B,releaseDate,2022-01-19,,True
XLM,numParams,665000000.0,,True
XLM,releaseDate,2019-06-01,,True
PDP model for serial order,releaseDate,1986-01-05,,True
Stacked-LSTM+Pruning,numParams,6160000.0,,True
Stacked-LSTM+Pruning,releaseDate,2019-06-17,,True
Linear Transformer (small),releaseDate,2021-02-22,,True
PULI GPTrio,flops,5.8e+21,8 A100s for three months8 * 312 trillion * 24 * 3600 * 90 * 0.3 (utilization assumption) = 5.8e21,True
PULI GPTrio,numParams,6700000000.0,,True
PULI GPTrio,numTokens,214000000000.0,"adding up column in Table 2.might be slightly off because it's counting non-Chinese tokens in the Chinese data, rather than non-Chinese words, but close.",True
PULI GPTrio,trainingTimeDays,2200.0,3 months,True
PULI GPTrio,gpuType,NVIDIA A100,,True
PULI GPTrio,releaseDate,2023-08-23,,True
Robot Parkour,numParams,500000.0,,True
Robot Parkour,gpuType,NVIDIA GeForce RTX 3090,,True
Robot Parkour,releaseDate,2023-09-12,,True
Transformer Large + HCP,flops,6060000000000000000,,True
Transformer Large + HCP,numParams,257000000.0,,True
Transformer Large + HCP,releaseDate,2022-03-21,,True
BiLSTM for Speech,flops,24124575958774.88,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
BiLSTM for Speech,numParams,152061.0,,True
BiLSTM for Speech,numTokens,36960.0,https://catalog.ldc.upenn.edu/LDC93s1One sample utterance has around 10 words3696 utterances * 10 words = around 37k words,True
BiLSTM for Speech,releaseDate,2005-08-01,,True
RetNet,flops,4.02e+21,C = 6ND = 6 * 6.7 billion * 100 billion,True
RetNet,numParams,6700000000.0,,True
RetNet,numTokens,75000000000.0,,True
RetNet,batchSize,4000000.0,,True
RetNet,releaseDate,2023-07-17,,True
RetNet,batchSize,4000000.0,4M,True
Claude 3 Haiku,releaseDate,2024-03-04,,True
AWD-LSTM + DeFINE,releaseDate,2019-11-27,,True
Denoising Diffusion Probabilistic Models (LSUN Bedroom),flops,380000000000000000000,"Numbers in Appendix B10.6h for the CIFAR model (batch size 128, 21 step/s)2.2 step/s for the LSUN model, 1.15M steps so 702.8 hoursThis is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores)https://cloud.google.com/tpu/docs/regions-zones1.25E14 FLOP/s * (8 cores / 2 cores/chip) * 702.8h * 3600s/h * 0.3 = 3.8e20",True
Denoising Diffusion Probabilistic Models (LSUN Bedroom),numParams,256000000.0,,True
Denoising Diffusion Probabilistic Models (LSUN Bedroom),numTokens,3033042.0,"""We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.""""The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.""https://paperswithcode.com/dataset/celeba-hqLSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.https://www.tensorflow.org/datasets/catalog/lsun",True
Denoising Diffusion Probabilistic Models (LSUN Bedroom),costDollars,2.60189327507192,,True
Denoising Diffusion Probabilistic Models (LSUN Bedroom),gpuType,Google TPU v3,,True
Denoising Diffusion Probabilistic Models (LSUN Bedroom),releaseDate,2021-06-11,,True
IBM-5,numParams,1658364.0,,True
IBM-5,numTokens,53358600.0,"""They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.""",True
IBM-5,releaseDate,1993-06-15,,True
RNNLM + Dynamic KL Regularization,releaseDate,2018-01-01,,True
Florence,flops,4.831e+22,"""The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.""512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP",True
Florence,numParams,893000000.0,,True
Florence,numTokens,900000000.0,,True
Florence,trainingTimeDays,240.0,10 days on 512 A100 40GB,True
Florence,gpuCount,512.0,,True
Florence,gpuType,NVIDIA A100 SXM4 40 GB,,True
Florence,releaseDate,2021-11-22,,True
ADM,flops,500000000000000000000,"For LSUN horse, they report 116 V100-days in pre-training.125 teraFLOP/s * 116 * 24 * 3600 * 0.4 = 5e20",True
ADM,gpuType,NVIDIA V100,,True
ADM,releaseDate,2021-05-11,,True
Gopher (280B),flops,6.31e+23,Table A266.31E+08 Train PFLOPs,True
Gopher (280B),numParams,280000000000.0,,True
Gopher (280B),numTokens,225000000000.0,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""1 token ~ 0.75 words",True
Gopher (280B),batchSize,6000000.0,,True
Gopher (280B),costDollars,891638.804314709,,True
Gopher (280B),trainingTimeDays,920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",True
Gopher (280B),gpuCount,4096.0,,True
Gopher (280B),gpuType,Google TPU v3,,True
Gopher (280B),gpuUtilization,0.378,,True
Gopher (280B),releaseDate,2021-12-08,,True
Gopher (280B),batchSize,6000000.0,"Table 1. ""Furthermore, we increase Gopher’s batch size from three to six million tokens per batch during training""",True
RETRO-7B,numParams,7500000000.0,,True
RETRO-7B,numTokens,315000000000.0,"""we train for 419,430,400,000 training tokens"" ~= 315B words.",True
RETRO-7B,releaseDate,2022-02-07,,True
Gemini Ultra,flops,5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",True
Gemini Ultra,trainingTimeDays,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",True
Gemini Ultra,gpuCount,55000.0,,True
Gemini Ultra,gpuType,Google TPU v4,,True
Gemini Ultra,releaseDate,2023-12-06,,True
TransE,flops,1340928000000000000,"8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)8 hours 0.33 util rate",True
TransE,numParams,942000000.0,,True
TransE,numTokens,17000000.0,"""it can be successfully trained on a large scale data set with 1Mentities, 25k relationships and more than 17M training samples""",True
TransE,costDollars,17.579616171054,,True
TransE,releaseDate,2013-12-05,,True
LSTM-MemoryAug (PTB),releaseDate,2020-09-29,,True
All-attention network + adaptive span,flops,46000000000000000000,,True
All-attention network + adaptive span,numParams,133000000.0,,True
All-attention network + adaptive span,releaseDate,2019-07-02,,True
Qwen-VL-Max,releaseDate,2024-01-25,,True
Delta RNN (+ full context),flops,1100000000000000100,,True
Delta RNN (+ full context),numParams,44600000.0,,True
Delta RNN (+ full context),releaseDate,2021-06-11,,True
Hybrid H3-1.3B,numParams,1300000000.0,,True
Hybrid H3-1.3B,releaseDate,2022-12-28,,True
Gato,flops,5.44e+21,256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.5 utilization = 5.44e21 FLOPs,True
Gato,numParams,1180000000.0,,True
Gato,costDollars,6781.08229491861,,True
Gato,trainingTimeDays,96.0,4 days,True
Gato,gpuCount,256.0,,True
Gato,gpuType,Google TPU v3,,True
Gato,releaseDate,2022-05-12,,True
SRU++ Large only 2 attention layers (k=5),releaseDate,2021-02-24,,True
NoPos,flops,161000000000000000000,,True
NoPos,numParams,1300000000.0,,True
NoPos,releaseDate,2022-03-30,,True
GPT3-6.7B (rerun of original),flops,1.2e+22,,True
GPT3-6.7B (rerun of original),numParams,6700000000.0,,True
GPT3-6.7B (rerun of original),releaseDate,2020-05-28,,True
Taiyi-Stable Diffusion,flops,5.1e+22,"Fine-tuning: 32 NVIDIA A100 GPUs for 100 hours32 * 312e12 * 30% * 100 * 60 * 60 = 1.078272e+21 FLOPBase model: Stable Diffusion, 5e+22 FLOP",True
Taiyi-Stable Diffusion,numParams,1000000000.0,,True
Taiyi-Stable Diffusion,trainingTimeDays,100.0,32 NVIDIA A100 GPUs for 100 hours,True
Taiyi-Stable Diffusion,gpuCount,32.0,,True
Taiyi-Stable Diffusion,gpuType,NVIDIA A100,,True
Taiyi-Stable Diffusion,releaseDate,2022-10-31,,True
ResNet-1001,numParams,10200000.0,,True
ResNet-1001,releaseDate,2016-09-17,,True
Flan T5-XXL + BLIP-2,flops,1.2e+21,"fine-tuned from Flan-T5 XXL (11B) and ViT-gfine-tuning compute:""using a single 16-A100(40G) machine, our largest model withViT-g and FlanT5-XXL requires less than 6 days for the firststage and less than 3 days for the second stage.""16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21",True
Flan T5-XXL + BLIP-2,numParams,12100000000.0,,True
Flan T5-XXL + BLIP-2,trainingTimeDays,200.0,"""less than 6 days for the firststage and less than 3 days for the second stage""9*24 is 216, rounding down a bit is 200 hours",True
Flan T5-XXL + BLIP-2,gpuType,NVIDIA A100 SXM4 40 GB,,True
Flan T5-XXL + BLIP-2,releaseDate,2023-01-30,,True
Code Llama-7B,flops,1.1e+23,"2.5e22 finetune compute + 8.4e22 base compute for Llama 2-7B, for ~1.1e23 compute overall",True
Code Llama-7B,numParams,7000000000.0,,True
Code Llama-7B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Code Llama-7B,releaseDate,2023-08-14,,True
AlphaCode,flops,1.568160000001e+23,Figure 7 (a) shows a maximum training compute budget of approx 20000 TPU-days per model.20000 days * 275 TFLOPS * 0.33 utilization = 1.6e23 FLOPhttps://www.wolframalpha.com/input?i=20000+*+275+teraFLOPS+*+1+day+*+0.33,True
AlphaCode,numParams,41100000000.0,,True
AlphaCode,batchSize,4718592.0,,True
AlphaCode,gpuCount,3750.0,,True
AlphaCode,gpuType,"Google TPU v4,Google TPU v4i",,True
AlphaCode,releaseDate,2022-02-02,,True
AlphaCode,batchSize,4718592.0,"2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592trained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup",True
AWD-FWM (WT2),flops,739000000000000000,,True
AWD-FWM (WT2),numParams,37000000.0,,True
AWD-FWM (WT2),releaseDate,2020-11-16,,True
NASNet-A,numParams,89000000.0,,True
NASNet-A,releaseDate,2017-07-21,,True
OPT-30B,numParams,30000000000.0,,True
OPT-30B,releaseDate,2022-06-21,,True
Goat-7B,numParams,7000000000.0,,True
Goat-7B,gpuType,NVIDIA A10 PCIe,,True
Goat-7B,releaseDate,2023-05-23,,True
VideoMAE V2,flops,9.7e+21,"finetuned on ViT-g (smaller than ViT-G with 1B params)""It takes more than two weeks to pre-train a ViT-g model with VideoMAEon 64 A100 GPUs""64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21",True
VideoMAE V2,numParams,1000000000.0,,True
VideoMAE V2,trainingTimeDays,330.0,2 weeks,True
VideoMAE V2,gpuCount,64.0,,True
VideoMAE V2,gpuType,NVIDIA A100 SXM4 80 GB,,True
VideoMAE V2,releaseDate,2023-03-29,,True
GPT-2 (117M),releaseDate,2019-02-14,,True
Transformer + Simple Recurrent Unit,flops,11000000000000000000,"""We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtainedusing 8 GPUs in parallel, which provide a large effective batch size during training. To approximatethe setup, we update the model parameters every 5×5120 tokens and use 16,000 warm-up stepsfollowing OpenNMT suggestions. We train eachmodel for 40 epochs (250,000 steps), and perform3 independent trials for each model configuration.A single run takes about 3.5 days with a Tesla V100 GPU.""125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19",True
Transformer + Simple Recurrent Unit,numParams,90000000.0,,True
Transformer + Simple Recurrent Unit,gpuCount,8.0,,True
Transformer + Simple Recurrent Unit,gpuType,NVIDIA V100,,True
Transformer + Simple Recurrent Unit,releaseDate,2018-09-17,,True
ContextNet + Noisy Student,flops,5.4e+21,"""Five generations of models numbered 0 to 4, are trained,where the baseline model is taken to be the generation-zeromodel. The baseline ContextNet model has the same encoderas CN-2, but has a one-layer RNN decoder with dimension 640.Meanwhile, CN-w with w=1.25, 1.75, 2.25 and 2.5 have beenset to be the ASR model from generation 1 through 4. Eachgeneration is trained on 128 Google Cloud TPU chips for 1-3days""Roughly assuming each generation is an average of 2 days. The TPU version is likely v3 given this is a 2020 paper.we get 128 * 10 * 24 * 3600 * 123 tflops * 0.4  (assumed utilization) = 5.4e21",True
ContextNet + Noisy Student,trainingTimeDays,240.0,roughly 10 days,True
ContextNet + Noisy Student,gpuType,Google TPU v3,,True
ContextNet + Noisy Student,releaseDate,2020-01-19,,True
"GCRN-M1, dropout",flops,3040000000000000,,True
"GCRN-M1, dropout",numParams,42000000.0,,True
"GCRN-M1, dropout",releaseDate,2016-12-22,,True
Megatron-BERT,flops,6.027e+22,"A source: https://lair.lighton.ai/akronomicon/ claims 5.7e22The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdfParam-based calculation:6ND = 6*3.9e9*2e6*1024*1024 = 4.8e22 FLOPTime-based calculation:The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.BERT: batch size 1024, 2e6 iterations total.So we should expect 4B => 1.0 days per epoch (69e3*512 examples)=> 2e6*1024/(69e3*512) = 58 days trainingOn 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.C=15.1 PFLOPS * 58 days = 7.6e22 FLOP.The param and time calculations seem more trustworthy. Geometric mean is 6.027e22 FLOP",True
Megatron-BERT,numParams,3900000000.0,,True
Megatron-BERT,numTokens,34800000000.0,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""",True
Megatron-BERT,batchSize,524288.0,,True
Megatron-BERT,costDollars,208034.393738375,,True
Megatron-BERT,trainingTimeDays,1392.0,"The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations.BERT: batch size 1024, 2e6 iterations total.So we should expect 4B => 1.0 days per epoch (69e3*512 examples)=> 2e6*1024/(69e3*512) = 58 days training",True
Megatron-BERT,gpuCount,512.0,,True
Megatron-BERT,gpuType,NVIDIA Tesla V100S PCIe 32 GB,,True
Megatron-BERT,gpuUtilization,0.2269,,True
Megatron-BERT,releaseDate,2019-09-17,,True
Megatron-BERT,batchSize,524288.0,"""we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearlyover 2 million iterations. Other training parameters are keptthe same as (Devlin et al., 2018).""in Devlin et al (BERT), sequences are 512 tokens",True
Multipop Adaptive Continuous Stack (PTB),releaseDate,2018-02-15,,True
Once for All,flops,1.78428096e+21,4.2k V100-hours (table 1)0.33 utilization rate,True
Once for All,numParams,7700000.0,,True
Once for All,costDollars,13000.0,from Table 1,True
Once for All,gpuType,NVIDIA V100,,True
Once for All,releaseDate,2020-04-29,,True
Conv-DBN,releaseDate,2009-06-14,,True
N-gram+Cache,releaseDate,2014-12-24,,True
TransformerXL + PowerSGD + L-Greco,flops,414000000000000000,,True
TransformerXL + PowerSGD + L-Greco,releaseDate,2022-10-31,,True
MedBERT,flops,8545824000000000000,"flops = (1) * (2826 * 10**10) * (24*7 * 3600) * (0.5) = 8545824000000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)I assume higher utilization rate, because only 1 GPU is used.Citation from the text:""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11",True
MedBERT,numParams,17000000.0,,True
MedBERT,trainingTimeDays,168.0,"""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11",True
MedBERT,gpuCount,1.0,,True
MedBERT,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
MedBERT,releaseDate,2021-05-20,,True
GraphCast,releaseDate,2023-11-14,,True
MGK 4 heads (medium),flops,6670000000000000000,,True
MGK 4 heads (medium),numParams,90000000.0,,True
MGK 4 heads (medium),releaseDate,2021-10-16,,True
DeepConPred2,numTokens,3443.0,"""Finally, our training set contained 3443 protein domains""",True
DeepConPred2,releaseDate,2018-10-11,,True
Instruct-GPT + Mind's Eye,numParams,176500000000.0,,True
Instruct-GPT + Mind's Eye,gpuType,Google TPU v3,,True
Instruct-GPT + Mind's Eye,releaseDate,2022-10-11,,True
DeBERTaV3-large + KEAR,numParams,418000000.0,,True
DeBERTaV3-large + KEAR,releaseDate,2021-12-06,,True
LSTM+NeuralCache,flops,1020000000000000,,True
LSTM+NeuralCache,numParams,2100000.0,,True
LSTM+NeuralCache,releaseDate,2018-09-24,,True
TSLM+MoS (PTB),releaseDate,2019-01-31,,True
AFP+FPI (WT2),releaseDate,2021-06-04,,True
LeNet-5,flops,2810937600000,"""[LeNet5] contains 390408 connections"" = multiply-addsMNIST - 60,000 data points20 epochs",True
LeNet-5,numParams,60000.0,,True
LeNet-5,numTokens,60000.0,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",True
LeNet-5,releaseDate,1998-11-01,,True
Max-Margin Markov Networks,numTokens,600.0,"The data set is divided into 10 folds of ∼ 600 training and ∼ 5500 testing examples.The accuracy results, ... are averages over the 10 folds",True
Max-Margin Markov Networks,releaseDate,2004-03-01,,True
RGC+ASQ (WT2),numParams,209000000.0,,True
RGC+ASQ (WT2),releaseDate,2018-08-13,,True
Volcano 13B,numParams,13000000000.0,,True
Volcano 13B,trainingTimeDays,30.0,,True
Volcano 13B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Volcano 13B,releaseDate,2023-11-13,,True
Qwen-14B,flops,2.5e+23,3T tokens per Table 114b*3T*6 = 2.5e23,True
Qwen-14B,numParams,14000000000.0,,True
Qwen-14B,batchSize,4000000.0,,True
Qwen-14B,releaseDate,2023-09-28,,True
Qwen-14B,batchSize,4000000.0,Table 1,True
ViT-Base/32,numParams,86000000.0,,True
ViT-Base/32,releaseDate,2020-10-22,,True
XGLM,numParams,7500000000.0,,True
XGLM,numTokens,1740000000.0,,True
XGLM,releaseDate,2021-12-20,,True
VD-RHN,flops,3570000000000000,,True
VD-RHN,numParams,32000000.0,,True
VD-RHN,releaseDate,2016-07-12,,True
Large regularized LSTM,flops,91000000000000000,,True
Large regularized LSTM,numParams,66000000.0,,True
Large regularized LSTM,releaseDate,2014-09-08,,True
ShuffleNet v2,numParams,2280000.0,,True
ShuffleNet v2,releaseDate,2018-06-30,,True
Cross-lingual alignment,flops,2560000000000000000,From author communication:Precision: float32Hardware: 4 GPU  NVIDIA 1080TiNVIDIA 1080Ti: 1.06E+13Compute: 7 GPU-days0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h= 2.56E+18,True
Cross-lingual alignment,costDollars,7.83251862333082,,True
Cross-lingual alignment,releaseDate,2019-04-04,,True
XuanYuan 2.0,numParams,176200000000.0,,True
XuanYuan 2.0,gpuType,NVIDIA A100 SXM4 80 GB,,True
XuanYuan 2.0,releaseDate,2023-05-19,,True
XGen-7B,flops,8.02e+22,"270,336 TPUv4-hours per the carbon emissions section. They must mean chip-hours, not core-hours, because they multiply by 192W, which is the max power consumption of a TPU-v4 chip.https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 270336 * 275 teraflop/s * 3600 * 0.3 = 8.02e22 FLOPalso, using 6ND:1484 billion tokens * 6.7 billion * 6 = 5.97e22",True
XGen-7B,numParams,6700000000.0,,True
XGen-7B,numTokens,1113000000000.0,1484B tokens per Table 2. 1113B words at 0.75 words/token,True
XGen-7B,batchSize,1048576.0,,True
XGen-7B,gpuType,Google TPU v4,,True
XGen-7B,releaseDate,2023-09-07,,True
XGen-7B,batchSize,1048576.0,"""batch size of 128, and a sequence length of 8,192 tokens""",True
RHN(depth=40),releaseDate,2018-05-23,,True
Youtube recommendation model,releaseDate,2016-09-15,,True
Cohere Command,numParams,52000000000.0,,True
Cohere Command,gpuType,Google TPU v4,,True
Cohere Command,releaseDate,2023-03-01,,True
Mogrifier RLSTM (PTB),releaseDate,2022-11-03,,True
Tongyi Qianwen (Qwen) 2.0,releaseDate,2023-10-31,,True
RGC+ASQ (PTB),releaseDate,2018-08-13,,True
GPT-2 (762M),releaseDate,2019-02-14,,True
SimCSE,releaseDate,2022-05-18,,True
OpenLLaMA-13B,flops,7.8e+22,13b * 1T * 6 = 7.8e22,True
OpenLLaMA-13B,numParams,13000000000.0,,True
OpenLLaMA-13B,numTokens,750000000000.0,"1T tokens, or ~750B words",True
OpenLLaMA-13B,gpuType,Google TPU v4,,True
OpenLLaMA-13B,releaseDate,2023-05-01,,True
gpt-sw3-40b,flops,7.68e+22,"aproximation 6ND = 6*320E9*40e9 = 7.68e22""GPT-SW3 has been trained on a dataset containing 320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code.""",True
gpt-sw3-40b,numParams,40000000000.0,,True
gpt-sw3-40b,releaseDate,2023-03-01,,True
GPT2-Large+LHOPT,flops,1.6e+21,,True
GPT2-Large+LHOPT,numParams,760000000.0,,True
GPT2-Large+LHOPT,releaseDate,2021-06-02,,True
Stockmark-13B,flops,1.716e+22,"6ND = 6*13B*220B = 17160000000000000000000""Stockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens. This model is developed by Stockmark Inc.""",True
Stockmark-13B,numParams,13200000000.0,,True
Stockmark-13B,numTokens,220000000000.0,220B tokens so -  assuming 1 word per token - 220B words,True
Stockmark-13B,releaseDate,2023-10-23,,True
Spark 3.0,releaseDate,2023-10-24,,True
Char-CNN-BiLSTM,releaseDate,2019-06-13,,True
BellKor 2009,numTokens,100480507.0,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",True
BellKor 2009,releaseDate,2009-08-01,,True
ProBERTa,flops,9720000000000000000,"""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18",True
ProBERTa,numParams,44000000.0,,True
ProBERTa,trainingTimeDays,18.0,,True
ProBERTa,gpuType,NVIDIA V100,,True
ProBERTa,releaseDate,2020-09-01,,True
Refined Part Pooling,releaseDate,2018-01-09,,True
NMST+GPT-2,flops,120000000000000000000,,True
NMST+GPT-2,numParams,124000000.0,,True
NMST+GPT-2,releaseDate,2022-10-03,,True
ResNet-50 Billion-scale,numParams,25000000.0,,True
ResNet-50 Billion-scale,releaseDate,2019-05-02,,True
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",flops,437000000000000000,,True
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",numParams,35000000.0,,True
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",releaseDate,2017-11-10,,True
Whisper,flops,4.65e+22,See figure 9,True
Whisper,numParams,1550000000.0,,True
Whisper,numTokens,9302400000.0,"""When scaled to 680,000 hours of multilingual and multitasksupervision, the resulting models generalize wellto standard benchmarks and are often competitivewith prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""13,680 words/h * 680,000h = 9,302,400,000 words",True
Whisper,releaseDate,2022-09-21,,True
BLUUMI,numParams,176000000000.0,,True
BLUUMI,numTokens,38000000000.0,38B tokens,True
BLUUMI,gpuType,AMD Instinct MI250X,,True
BLUUMI,releaseDate,2023-11-03,,True
Libratus,flops,551000000000000000000,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).The first system has 752 nodes a 2CPUs a 14cores each.source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/1. 12M core hours for 196 cores2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores2.1 That's 42.5 GFLOPS per core.3. Running this for 12M h3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs4. Assuming 30% utilization 1.823e21 * 0.3→ 5.51e20 FLOPs",True
Libratus,costDollars,6253.48592764557,,True
Libratus,releaseDate,2017-01-01,,True
RT-1 + AutoRT,numParams,35000000.0,,True
RT-1 + AutoRT,releaseDate,2024-01-04,,True
Low-Cost Collaborative Network,numTokens,1280000.0,,True
Low-Cost Collaborative Network,releaseDate,2017-05-15,,True
RNN+LDA,releaseDate,2012-12-01,,True
Fuzzy NN,flops,1403117760,1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,True
Fuzzy NN,numParams,1166.0,,True
Fuzzy NN,numTokens,436.0,"""The above-mentioned algorithm was tested on a set of 871 Indian Telugu vowel sounds"" and 50% of the dataset was used. 871*0.5 ~= 436",True
Fuzzy NN,releaseDate,1992-09-01,,True
Jurassic-2 Jumbo,numParams,178000000000.0,,True
Jurassic-2 Jumbo,releaseDate,2023-03-01,,True
"MicroNet (Adaptive, Cache)",numParams,8300000.000000001,,True
"MicroNet (Adaptive, Cache)",releaseDate,2020-01-01,,True
WGAN-GP,releaseDate,2017-03-31,,True
M6-T,flops,5.5e+21,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,True
M6-T,numParams,1000000000000.0,,True
M6-T,numTokens,1900000000000.0,Images,True
M6-T,gpuCount,480.0,,True
M6-T,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
M6-T,releaseDate,2021-03-05,,True
Qwen-72B,flops,1.3e+24,"72 billion params, 3 trillion tokens72b * 3T * 6 = 1.3e24",True
Qwen-72B,numParams,72000000000.0,,True
Qwen-72B,batchSize,4000000.0,,True
Qwen-72B,releaseDate,2023-11-30,,True
Qwen-72B,batchSize,4000000.0,Table 1 https://arxiv.org/abs/2309.16609(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper),True
Claude 3 Sonnet,releaseDate,2024-03-04,,True
Neural cache model (size=2000),releaseDate,2016-12-13,,True
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),numParams,35000000.0,,True
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),releaseDate,2018-08-14,,True
Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),numParams,182000000.0,,True
Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),releaseDate,2022-10-24,,True
TCN (P-MNIST),numParams,42000.0,,True
TCN (P-MNIST),releaseDate,2018-02-15,,True
6-Act Tether,numParams,5000000.0,,True
6-Act Tether,releaseDate,2021-08-03,,True
German ELECTRA Large,flops,1.42829568e+21,flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2,True
German ELECTRA Large,numParams,335000000.0,,True
German ELECTRA Large,numTokens,27287800000.0,163.4GB from Table 1 in the paperassuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0,True
German ELECTRA Large,trainingTimeDays,168.0,7 days from Table 2,True
German ELECTRA Large,gpuCount,64.0,,True
German ELECTRA Large,gpuType,Google TPU v3,,True
German ELECTRA Large,releaseDate,2020-10-21,,True
Stack RNN,numParams,2010000.0,,True
Stack RNN,releaseDate,2015-03-03,,True
Image Classification with the Fisher Vector: Theory and Practice,flops,90842400000000,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003",True
Image Classification with the Fisher Vector: Theory and Practice,costDollars,0.0014095745328933,,True
Image Classification with the Fisher Vector: Theory and Practice,trainingTimeDays,2.0,,True
Image Classification with the Fisher Vector: Theory and Practice,releaseDate,2013-06-12,,True
Boss (DARPA Urban Challenge),releaseDate,2008-07-23,,True
StarCoder 2 3B,flops,5.94e+22,estimation is given in Table 6 ,True
StarCoder 2 3B,numParams,3000000000.0,,True
StarCoder 2 3B,numTokens,3100000000000.0,"from Table 7, 31T tokens ",True
StarCoder 2 3B,gpuType,NVIDIA A100 SXM4 80 GB,,True
StarCoder 2 3B,releaseDate,2024-02-29,,True
DD-PPO,flops,780000000000000000000,"""Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days – 180 GPU-days of training""125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20",True
DD-PPO,trainingTimeDays,66.0,2.75 days,True
DD-PPO,gpuCount,64.0,,True
DD-PPO,gpuType,NVIDIA V100,,True
DD-PPO,releaseDate,2019-12-19,,True
CodeT5-large,flops,2.72e+21,"""We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days""16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21",True
CodeT5-large,numParams,770000000.0,,True
CodeT5-large,trainingTimeDays,504.0,21 days,True
CodeT5-large,gpuType,NVIDIA A100,,True
CodeT5-large,releaseDate,2022-07-05,,True
Walking Minotaur robot,releaseDate,2019-06-19,,True
Statement Curriculum Learning,numTokens,275000000000.0,"Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB.",True
Statement Curriculum Learning,releaseDate,2022-03-02,,True
Selfish-RNN (ON-LSTM),releaseDate,2021-01-22,,True
rTop-k(distributed setting),flops,14600000000000000,,True
rTop-k(distributed setting),numParams,69000000.0,,True
rTop-k(distributed setting),releaseDate,2020-05-21,,True
TransfoRNN(d=1024)(2-layer) (PTB),releaseDate,2021-04-04,,True
IDEFICS,flops,1.1593580544e+23,"flops = 512 * 312e12 * 28*24*3600 * 0.3(num gpus) * (peak perforemence) * (time in seconds) * (assumed utilization rate)""The IDEFICS models were trained on an AWS SageMaker cluster with 8x80GB A100 GPUs nodes and EFA network.    IDEFICS-80B took ~28 days of training on 64 nodes (512 GPUs).""",True
IDEFICS,numParams,80000000000.0,,True
IDEFICS,releaseDate,2023-08-22,,True
Qwen-VL,numParams,9600000000.0,,True
Qwen-VL,numTokens,1400000000.0,1.4B text-image pairs,True
Qwen-VL,releaseDate,2023-08-24,,True
ShuffleNet v1,numParams,2430000.0,,True
ShuffleNet v1,releaseDate,2017-07-03,,True
RetinaNet-R50,numParams,34000000.0,,True
RetinaNet-R50,releaseDate,2017-08-07,,True
Orca 2-13B,flops,4.6e+22,4.55e22 base compute from Llama-13 + 8.6e20 finetune compute ~= 4.6e22 ,True
Orca 2-13B,numParams,13000000000.0,,True
Orca 2-13B,trainingTimeDays,80.0,"17+40+23 hours""We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16.For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch,~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continuetraining on ~1.8 million GPT-4 data for 4 epochs""",True
Orca 2-13B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Orca 2-13B,releaseDate,2023-11-21,,True
DEQ-TrellisNet,releaseDate,2019-09-03,,True
Transformer-XL + AutoDropout (WT2),numParams,35000000.0,,True
Transformer-XL + AutoDropout (WT2),releaseDate,2021-01-05,,True
GenSLM,flops,1.42e+21,See Table 3Overall ZettaFlops 1.42,True
GenSLM,numParams,25000000000.0,,True
GenSLM,releaseDate,2022-10-11,,True
Multiresolution CNN,numParams,126125568.0,,True
Multiresolution CNN,numTokens,50000000.0,"""We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.""So 5e+7 datapoints and 10 epochs.",True
Multiresolution CNN,releaseDate,2014-06-23,,True
VLM-4,releaseDate,2022-04-12,,True
NeuMF (Pinterest),releaseDate,2017-08-16,,True
SPPNet,flops,3411072000000000000,"""All networks in this paper can betrained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",True
SPPNet,numTokens,1280000.0,"Section 3.1: ""We train the networks on the 1000-category trainingset of ImageNet 2012.""",True
SPPNet,costDollars,65.0710704458551,,True
SPPNet,trainingTimeDays,672.0,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",True
SPPNet,gpuType,NVIDIA GeForce GTX TITAN,,True
SPPNet,releaseDate,2014-06-18,,True
TSLM+MoS (WT2),numParams,9120000.0,,True
TSLM+MoS (WT2),releaseDate,2019-01-31,,True
SPN-4,releaseDate,2014-01-01,,True
B2T connection (16L),flops,28000000000000000000,192*7 GPU (P100) hours per Table 619 TFLOP/s 19 trillion * 192 * 7 * 3600 * 0.3 = 2.76e19,True
B2T connection (16L),numParams,247000000.00000003,,True
B2T connection (16L),releaseDate,2022-06-01,,True
ResNet-200,trainingTimeDays,500.0,"""about 3 weeks""",True
ResNet-200,releaseDate,2016-09-17,,True
LLaMA-7B (LoRA finetuned),numParams,7000000000.0,,True
LLaMA-7B (LoRA finetuned),releaseDate,2023-05-23,,True
ProtT5-XXL-BFD,flops,3.7e+22,"FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, ",True
ProtT5-XXL-BFD,numParams,11000000000.0,,True
ProtT5-XXL-BFD,gpuCount,512.0,,True
ProtT5-XXL-BFD,gpuType,Google TPU v3,,True
ProtT5-XXL-BFD,releaseDate,2021-05-04,,True
Image generation,flops,475200000000000,"From https://openai.com/blog/ai-and-compute/ Appendix""less than 0.0000055 pfs-days""(86400*10^15*0.0000055)",True
Image generation,numTokens,60000.0,"""We trained generative models of images from the MNIST and Frey Face datasets""MNIST has 60k imageshttps://en.wikipedia.org/wiki/MNIST_databaseFrey Face has 2k imageshttps://cs.nyu.edu/~roweis/data.html",True
Image generation,costDollars,0.0064162387616529,,True
Image generation,releaseDate,2013-12-20,,True
Sandwich Transformer,flops,158000000000000000000,,True
Sandwich Transformer,numParams,209000000.0,,True
Sandwich Transformer,releaseDate,2019-11-10,,True
MatrixFac for Recommenders,numTokens,100480507.0,,True
MatrixFac for Recommenders,releaseDate,2009-08-07,,True
(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),releaseDate,2018-08-30,,True
SeamlessM4T,numParams,2300000000.0,,True
SeamlessM4T,gpuType,NVIDIA V100,,True
SeamlessM4T,releaseDate,2023-12-08,,True
Multilingual-E5-large,numParams,560000000.0,,True
Multilingual-E5-large,releaseDate,2023-06-30,,True
DQN,flops,2300000000000000,"Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connectedFirst layer: 20*20*3*16*8*8 = 1.23M add-multipliesSecond layer: 9*9*16*32*4*4 = 0.66M add-multipliesThird layer: 9*9*32*256 = 0.66M add-mutlipliesTotal ~ 2.55M add-multiplies2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass= 2.3 PF = 2.7e-5 pfs-days",True
DQN,numParams,836096.0,,True
DQN,costDollars,0.0403704382939037,,True
DQN,releaseDate,2013-12-19,,True
HMM Word Alignment,numTokens,442316.0,[WORDS]Table 1.I take the sum of all words. Maybe it would be better to use only the sum of English or German words?,True
HMM Word Alignment,releaseDate,1996-08-05,,True
Maximum Entropy Models for machine translation,numTokens,519523.0,[WORDS]Table 1,True
Maximum Entropy Models for machine translation,releaseDate,2002-07-06,,True
DiffQ Transformer (16L),flops,3360000000000000000,,True
DiffQ Transformer (16L),numParams,257000000.0,,True
DiffQ Transformer (16L),releaseDate,2021-04-20,,True
DensePhrases,flops,2099520000000000000, flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)model of GPU from appendix B (Titan Xp)number of GPUs from table in appendix Aflops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948,True
DensePhrases,numTokens,58000000.0,"from appendix D ""The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.""assuming 40 words per question we get around ~ 58M",True
DensePhrases,trainingTimeDays,20.0,appendix A row 3,True
DensePhrases,gpuCount,8.0,,True
DensePhrases,gpuType,NVIDIA TITAN Xp,,True
DensePhrases,releaseDate,2020-12-23,,True
PolyNet,flops,64000000000000000000,"Section 5: ""ResNet-500 [has] similar computationcosts to our Very Deep PolyNet"".ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.560k iterations, batch size 512:Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",True
PolyNet,numParams,92000000.0,,True
PolyNet,numTokens,1280000.0,,True
PolyNet,gpuCount,32.0,,True
PolyNet,gpuType,NVIDIA GTX Titan X,,True
PolyNet,releaseDate,2016-11-17,,True
VD-LSTM+REAL Medium,releaseDate,2016-11-04,,True
LSTM-Char-Large,flops,2650000000000000,,True
LSTM-Char-Large,numParams,19000000.0,,True
LSTM-Char-Large,releaseDate,2015-08-26,,True
MGK 8 heads (small),releaseDate,2021-10-16,,True
AlphaGeometry,numParams,151000000.0,,True
AlphaGeometry,gpuType,Google TPU v3,,True
AlphaGeometry,releaseDate,2024-01-17,,True
ResNet-152 (ImageNet),flops,12100000000000000000,(11.4 *10^9) mult-adds per forward pass2 FLOPS/ mult-add3.5 for forward & backward pass1.2 * 10^6 examples in dataset128 epochsSource:x,True
ResNet-152 (ImageNet),numParams,60000000.0,,True
ResNet-152 (ImageNet),numTokens,1280000.0,"""We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images""",True
ResNet-152 (ImageNet),costDollars,92.031433516962,,True
ResNet-152 (ImageNet),releaseDate,2015-12-10,,True
Domain Adaptation,numParams,15260.0,,True
Domain Adaptation,numTokens,4652.0,Dataset introduced in 'Adapting Visual Category Models to NewDomains',True
Domain Adaptation,releaseDate,2011-11-06,,True
XLMR-XXL,numParams,10700000000.0,,True
XLMR-XXL,numTokens,125250000000.0,"""We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.""1 token ~ 0.7 words",True
XLMR-XXL,releaseDate,2021-08-17,,True
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),releaseDate,2017-08-29,,True
SACHS,numParams,178.0,,True
SACHS,numTokens,5400.0,"I think? "" The truncated singlecell data set (420 data points) shows a large(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). ""Seems potentially wrong by maybe 20%. Might need to add 1200.",True
SACHS,releaseDate,2005-04-22,,True
Wu Dao Aquila,numParams,33000000000.0,,True
Wu Dao Aquila,gpuType,NVIDIA A100,,True
Wu Dao Aquila,releaseDate,2023-06-10,,True
Kosmos-2,flops,250026393600000000000,"flops = (256) * (2826* 10**10) * (24 * 3600) * (0.4) = 2.500263936e+20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)peak flop from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957citation from text:""We train the model on 256 V100 GPUs and the training takes approximately one day to complete""""We train KOSMOS-2 for 60k steps, equivalent to around 25 billion tokens""alternative:6ND = 6*25B*1.6B = 2.4e20",True
Kosmos-2,numParams,1600000000.0,,True
Kosmos-2,trainingTimeDays,24.0,""" We train the model on 256 V100 GPUs and the training takes approximately one day to complete""",True
Kosmos-2,gpuCount,256.0,,True
Kosmos-2,gpuType,NVIDIA V100,,True
Kosmos-2,releaseDate,2023-06-26,,True
PaLM-SayCan,numParams,540000000000.0,,True
PaLM-SayCan,trainingTimeDays,100.0,The RL model is trained using 16 TPUv3 chips and for about 100 hours,True
PaLM-SayCan,gpuType,Google TPU v3,,True
PaLM-SayCan,releaseDate,2022-08-16,,True
Dolly 2.0-12b,numParams,12000000000.0,,True
Dolly 2.0-12b,releaseDate,2023-04-12,,True
SRU++ Large,flops,11000000000000000000,,True
SRU++ Large,numParams,234000000.0,,True
SRU++ Large,releaseDate,2021-02-24,,True
SimCLR,numParams,375000000.0,,True
SimCLR,gpuType,Google TPU v3,,True
SimCLR,releaseDate,2020-02-13,,True
Polarity Classifier,numTokens,11112.0,"Section 3.3 reveals there are 11,112 sentences. Since this is phrase-level sentiment analysis sentences seem like the best unit",True
Polarity Classifier,releaseDate,2009-09-01,,True
OPT-2.7B (finetuned on PTB),numParams,2700000000.0,,True
OPT-2.7B (finetuned on PTB),releaseDate,2022-06-21,,True
Decoupled weight decay regularization,flops,2470000000000000000,From author communicationPer image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs = 2.47E+18 FLOPs,True
Decoupled weight decay regularization,numParams,36500000.0,,True
Decoupled weight decay regularization,numTokens,50000.0,,True
Decoupled weight decay regularization,costDollars,8.07288264431363,,True
Decoupled weight decay regularization,releaseDate,2019-01-04,,True
PG-SWGAN,releaseDate,2019-06-15,,True
Pandemonium (morse),flops,600000000,"The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",True
Pandemonium (morse),releaseDate,1959-02-01,,True
Pythia-2.8b,numParams,2800000000.0,,True
Pythia-2.8b,releaseDate,2023-04-03,,True
Theseus 6/768,numParams,66000000.0,,True
Theseus 6/768,releaseDate,2020-02-07,,True
Transformer-C,flops,21000000000000000,,True
Transformer-C,numParams,148000000.0,,True
Transformer-C,releaseDate,2021-04-08,,True
DeepNet,numParams,3200000000.0,,True
DeepNet,numTokens,12000000000.0,""" The final data consists of 102 languages, 1932 directions, and12B sentence pairs.""",True
DeepNet,releaseDate,2022-03-01,,True
Gated HORNN (3rd order),numParams,8970000.0,,True
Gated HORNN (3rd order),releaseDate,2016-04-30,,True
Helpful Harmless preference model,numParams,52000000000.0,,True
Helpful Harmless preference model,releaseDate,2023-04-12,,True
Spatially-Sparse CNN,releaseDate,2014-09-23,,True
TFE SVM,releaseDate,2006-02-02,,True
Llama 2-70B,flops,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",True
Llama 2-70B,numParams,70000000000.0,,True
Llama 2-70B,numTokens,1500000000000.0,2 trillion tokens ~= 1.5 trillion words,True
Llama 2-70B,batchSize,4000000.0,,True
Llama 2-70B,costDollars,1620000.0,A100 cost in 2023: $1.10/hourTraining time: 1720320 A100 GPU-hoursInflation adjustment: $1.000 2020 = $1.145 2023,True
Llama 2-70B,trainingTimeDays,2160.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods.",True
Llama 2-70B,gpuCount,1000.0,,True
Llama 2-70B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Llama 2-70B,gpuUtilization,0.435,,True
Llama 2-70B,releaseDate,2023-07-18,,True
Llama 2-70B,batchSize,4000000.0,,True
AWD-LSTM-MoS+Noisin+dynamic evaluation,releaseDate,2018-05-03,,True
Fine-tuned-AWD-LSTM-DOC(fin),flops,1920000000000000,,True
Fine-tuned-AWD-LSTM-DOC(fin),numParams,23000000.0,,True
Fine-tuned-AWD-LSTM-DOC(fin),releaseDate,2018-11-12,,True
InstructBLIP,numParams,13000000000.0,,True
InstructBLIP,trainingTimeDays,36.0,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",True
InstructBLIP,gpuCount,16.0,,True
InstructBLIP,gpuType,NVIDIA A100 SXM4 40 GB,,True
InstructBLIP,releaseDate,2023-05-11,,True
Progressive LRD,flops,62000000000000000000,,True
Progressive LRD,numParams,31000000.0,,True
Progressive LRD,releaseDate,2022-10-12,,True
Selfish-RNN (SNT-ASGD)RHNs,releaseDate,2021-01-22,,True
Context-dependent RNN,releaseDate,2012-07-27,,True
MathGPT,releaseDate,2023-08-24,,True
Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),releaseDate,2020-09-29,,True
Transformer-XL + SIS,flops,10400000000000000000,,True
Transformer-XL + SIS,numParams,246000000.0,,True
Transformer-XL + SIS,releaseDate,2021-05-03,,True
Tensor-Transformer(1core)+PN (WT103),flops,1580000000000000000,,True
Tensor-Transformer(1core)+PN (WT103),numParams,85300000.0,,True
Tensor-Transformer(1core)+PN (WT103),releaseDate,2020-03-17,,True
SparseOPT-66B,numParams,66000000000.0,,True
SparseOPT-66B,releaseDate,2023-01-02,,True
CodeFuse-13B,flops,3.3e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, witha Hardware FLOPs Utilization (HFU) of approximately 60%. Thetraining process took approximately 40 days to complete""512 * 312 trillion * 40 * 24 * 3600 * 0.6 = 3.3e23Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens",True
CodeFuse-13B,numParams,13000000000.0,,True
CodeFuse-13B,numTokens,1000000000000.0,"1T tokens, mostly code but some Chinese/English",True
CodeFuse-13B,batchSize,16777216.0,,True
CodeFuse-13B,trainingTimeDays,960.0,~40 days,True
CodeFuse-13B,gpuType,NVIDIA A100 SXM4 80 GB,,True
CodeFuse-13B,releaseDate,2023-10-10,,True
CodeFuse-13B,batchSize,16777216.0,"4096 batch size, 4096 sequence length",True
GLEE,releaseDate,1968-07-01,,True
Bayesian automated hyperparameter tuning,releaseDate,2012-12-02,,True
DenseNet-264,numParams,34000000.0,,True
DenseNet-264,releaseDate,2016-08-25,,True
4 layer QRNN (h=2500),flops,240000000000000000,,True
4 layer QRNN (h=2500),numParams,26000000.0,,True
4 layer QRNN (h=2500),releaseDate,2018-03-22,,True
Detic,flops,23439974400000000000,28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization ratefor Swin-B model from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.',True
Detic,numParams,88000000.0,,True
Detic,numTokens,16900000.0,14M + 1.5M + 1.2M + 100K + 100K = 16900000.0table above section 5.1,True
Detic,trainingTimeDays,24.0,from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.',True
Detic,gpuCount,32.0,,True
Detic,gpuType,NVIDIA V100,,True
Detic,releaseDate,2022-01-07,,True
Transformer-XL+AdamGapAware(GA),numParams,257000000.0,,True
Transformer-XL+AdamGapAware(GA),releaseDate,2019-09-24,,True
SimCLRv2,numParams,795000000.0,,True
SimCLRv2,numTokens,1280000.0,,True
SimCLRv2,releaseDate,2020-10-26,,True
mBART-50,flops,3.281596416e+21,"flops = (256) * (2826 * 10**10) * (2.5 * 7 * 24 * 3600) * (0.3) = 3.281596416e+21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)""mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs""V100 have peak flop 28.26 TFLOPS  from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957",True
mBART-50,numParams,610000000.0,,True
mBART-50,trainingTimeDays,420.0,"""mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs""",True
mBART-50,gpuCount,256.0,,True
mBART-50,gpuType,NVIDIA V100,,True
mBART-50,releaseDate,2020-08-02,,True
Attend-Infer-Repeat,flops,64488960000000000,(peak FLOPs for GPU - 1244 GFLOPs) times (training time - 3600 * 48 second) * (0.3 assumed utilization rate) ,True
Attend-Infer-Repeat,numParams,82130304.0,,True
Attend-Infer-Repeat,numTokens,60000.0,60000 MNIST images,True
Attend-Infer-Repeat,trainingTimeDays,48.0,"48 hours for MNIST model, 72 hours for 3D scenes model",True
Attend-Infer-Repeat,gpuType,NVIDIA Quadro K4000,,True
Attend-Infer-Repeat,releaseDate,2016-08-12,,True
TransformerXL + spectrum control,flops,459999999999999940,,True
TransformerXL + spectrum control,numParams,151000000.0,,True
TransformerXL + spectrum control,releaseDate,2020-03-11,,True
GSM,numTokens,4000000.0,"from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairsdownload-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=downloadwc -w on train-v.1.1 returns 4017471 words so around 4M words",True
GSM,releaseDate,2017-07-30,,True
Starling-LM-7B-alpha,numParams,7000000000.0,,True
Starling-LM-7B-alpha,gpuCount,8.0,,True
Starling-LM-7B-alpha,gpuType,NVIDIA A100,,True
Starling-LM-7B-alpha,releaseDate,2023-11-25,,True
EnCodec,gpuType,NVIDIA A100,,True
EnCodec,releaseDate,2022-10-24,,True
Multi-cause Binary Clustering,releaseDate,1995-01-01,,True
DiT-XL/2,flops,600000000000000000000,"~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. 3M iterations with a batch size of 256.",True
DiT-XL/2,numParams,675000000.0,,True
DiT-XL/2,gpuType,Google TPU v3,,True
DiT-XL/2,releaseDate,2023-03-02,,True
DistilProtBert,flops,190000000000000000000,"""Pretraining was done on five v100 32-GB Nvidia GPUs from a DGXcluster with a local batch size of 16 examples... Every epoch run took approximately 4 days, resulting in total pretraining time of 12 days""5 * 125 teraFLOP/s * 12 * 24 * 3600 * 0.3 (assumed utilization) = 1.9e20",True
DistilProtBert,numParams,230000000.0,,True
DistilProtBert,trainingTimeDays,288.0,12 days,True
DistilProtBert,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
DistilProtBert,releaseDate,2022-09-18,,True
Regularized SVD for Collaborative Filtering,numTokens,100480507.0,,True
Regularized SVD for Collaborative Filtering,releaseDate,2007-08-12,,True
Contriever,numParams,110000000.0,,True
Contriever,releaseDate,2021-12-16,,True
Qwen1.5 72B,numParams,72000000000.0,,True
Qwen1.5 72B,releaseDate,2024-02-04,,True
Perceptron (1960),flops,720000000,"4000 * 12000 * 15from the text ""This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visualstimuli in the perceptron,""from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second."" For the first system, the computing time averaged about 15 seconds per stimulus cycle, ""In Fig 10 we see up to 4000 stimuli",True
Perceptron (1960),numParams,1000.0,,True
Perceptron (1960),numTokens,5000.0,"from the text ""The two main simulation programs total about 5000 words each.""",True
Perceptron (1960),releaseDate,1960-03-30,,True
InternLM,numParams,100000000000.0,,True
InternLM,numTokens,750000000000.0,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens"" equals approximately 750B words for English, but the tokenizer's conversion ratio may be different for Chinese.",True
InternLM,gpuType,NVIDIA A100 SXM4 80 GB,,True
InternLM,releaseDate,2023-07-06,,True
ERNIE 4.0,releaseDate,2023-10-17,,True
Word Representations,numTokens,37000000.0,"Section 6: ""After cleaning, there are 37 million words (58%of the original) in 1.3 million sentences""",True
Word Representations,releaseDate,2010-06-01,,True
Transformer - LibriVox + Decoding/Rescoring,numParams,296000000.0,,True
Transformer - LibriVox + Decoding/Rescoring,releaseDate,2019-11-19,,True
DALL-E,flops,4.7e+22,source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb,True
DALL-E,numParams,12000000000.0,,True
DALL-E,numTokens,250000000.0,"""To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting250 million text-images pairs from the internet. """,True
DALL-E,costDollars,171537.131679011,,True
DALL-E,gpuCount,1024.0,,True
DALL-E,gpuType,NVIDIA Tesla V100 DGXS 16 GB,,True
DALL-E,releaseDate,2021-01-05,,True
AltCLIP,releaseDate,2022-11-12,,True
BASIC-L + Lion,numParams,3070000000.0,,True
BASIC-L + Lion,releaseDate,2023-02-13,,True
Engin-XL(NE),numParams,1500000000.0,,True
Engin-XL(NE),releaseDate,2021-12-11,,True
AlphaStar,flops,5.9250000000001e+22,384 TPUv3 chips for 44 days. Assume 33% utilization.https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days,True
AlphaStar,numParams,139000000.0,,True
AlphaStar,costDollars,512765.269878946,,True
AlphaStar,trainingTimeDays,1056.0,"""Each agent was trained using 32 third-generation tensorprocessing units (TPUs) over 44 days""",True
AlphaStar,gpuCount,384.0,,True
AlphaStar,gpuType,Google TPU v3,,True
AlphaStar,releaseDate,2019-10-30,,True
Transformer-XL + RMS dynamic eval,numParams,257000000.0,,True
Transformer-XL + RMS dynamic eval,releaseDate,2019-04-17,,True
Bidirectional RNN,numParams,13000.0,,True
Bidirectional RNN,numTokens,73920.0,"""the training data set consisting of 3696 sentencesfrom 462 speakers""Assuming avg sentence length of 20 words3696 * 20 total words",True
Bidirectional RNN,releaseDate,1997-11-01,,True
OpenAI Five,flops,6.7e+22,"""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.Source: Dota 2 with Large Scale Deep Reinforcement Learninghttps://arxiv.org/abs/1912.06680You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",True
OpenAI Five,numParams,159000000.0,,True
OpenAI Five,numTokens,454321373184.0,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4thframe which we call a timestep""--> 7.5 timesteps/s""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days296 * 24*3600 * 7.5 = 1.92e8This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?EDIT 14/06/2022Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",True
OpenAI Five,costDollars,166042.114482332,"Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",True
OpenAI Five,trainingTimeDays,7104.0,"""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days",True
OpenAI Five,gpuCount,1536.0,,True
OpenAI Five,releaseDate,2019-12-13,,True
DeciCoder,flops,2.9e+21,446b * 1.1b * 6 = 2.9e21,True
DeciCoder,numParams,1100000000.0,,True
DeciCoder,releaseDate,2023-08-15,,True
OPT-1.3B,numParams,1300000000.0,,True
OPT-1.3B,releaseDate,2022-06-21,,True
Transformer local-attention (NesT-B),flops,24057600000000000000,"17.9 GFLOPS per forward pass300 epochs1.28M training examples3.5 f_to_b pass ratio(From Imagenet paper-data, Besiroglu et al., forthcoming) ",True
Transformer local-attention (NesT-B),numParams,90100000.0,,True
Transformer local-attention (NesT-B),numTokens,1280000.0,,True
Transformer local-attention (NesT-B),costDollars,39.5132372266025,,True
Transformer local-attention (NesT-B),releaseDate,2021-05-26,,True
Continuous speech recognition by statistical methods,numTokens,12000.0,"800 sentences, counting 15 words per sentence gives 12000 words""All the results given are for a training set of 800 sentences and a test set of 100 sentences""",True
Continuous speech recognition by statistical methods,releaseDate,1976-04-30,,True
GPT-Neo-2.7B (finetuned),releaseDate,2021-03-21,,True
Quantized ADMM,releaseDate,2021-11-29,,True
Part-of-sentence tagging model,flops,145411200000000000,12 hours of training for POS taggingGeForce GTX TITAN X GPU0.33 utilization rate,True
Part-of-sentence tagging model,numTokens,912344.0,Table 2,True
Part-of-sentence tagging model,costDollars,0.96778739473914,,True
Part-of-sentence tagging model,trainingTimeDays,12.0,"""the model training requires about 12 hours for POS tagging and 8hours for NER""",True
Part-of-sentence tagging model,gpuCount,1.0,,True
Part-of-sentence tagging model,gpuType,NVIDIA GeForce GTX TITAN X,,True
Part-of-sentence tagging model,releaseDate,2016-05-29,,True
SEST,numTokens,4000000.0,size of SQuAD,True
SEST,gpuCount,1.0,,True
SEST,gpuType,NVIDIA GeForce GTX 1080,,True
SEST,releaseDate,2017-03-02,,True
EI-REHN-1000D,flops,10600000000000000,,True
EI-REHN-1000D,numParams,19000000.0,,True
EI-REHN-1000D,releaseDate,2017-08-14,,True
Wide Residual Network,releaseDate,2016-09-19,,True
Fraternal dropout + AWD-LSTM 3-layer (PTB),releaseDate,2017-10-31,,True
Compress-LSTM (66M),flops,33100000000000000,,True
Compress-LSTM (66M),numParams,66000000.0,,True
Compress-LSTM (66M),releaseDate,2019-02-06,,True
LSTM (PTB),releaseDate,2016-12-13,,True
Amended-DARTS,numParams,23000000.0,,True
Amended-DARTS,releaseDate,2019-10-25,,True
OpenCALM,numParams,7000000000.0,,True
OpenCALM,releaseDate,2023-05-15,,True
Multipop Adaptive Continuous Stack (WT2),numParams,26000000.0,,True
Multipop Adaptive Continuous Stack (WT2),releaseDate,2018-02-15,,True
Sparse coding model for V1 receptive fields,numTokens,10.0,"In Simulation Methods: ""The data for training were taken from ten 512 × 512pixel images of natural surroundings""",True
Sparse coding model for V1 receptive fields,releaseDate,1997-12-01,,True
Linear Transformer (large),flops,3890000000000000000,,True
Linear Transformer (large),numParams,90000000.0,,True
Linear Transformer (large),releaseDate,2021-02-22,,True
T2R + Random Init,flops,61000000000000000000,,True
T2R + Random Init,numParams,450000000.0,,True
T2R + Random Init,releaseDate,2021-03-24,,True
"Segatron XL large, M=384",flops,26500000000000000000,,True
"Segatron XL large, M=384",numParams,257000000.0,,True
"Segatron XL large, M=384",releaseDate,2020-04-30,,True
DLRM-2022,flops,1.1e+21,Figure 1https://arxiv.org/abs/2104.05158,True
DLRM-2022,numParams,3000000000000.0,,True
DLRM-2022,costDollars,2394.06678279481,,True
DLRM-2022,gpuType,"NVIDIA Tesla V100 DGXS 32 GB,NVIDIA A100",,True
DLRM-2022,releaseDate,2021-09-15,,True
RoboCat,numParams,1180000000.0,,True
RoboCat,releaseDate,2023-06-20,,True
PLATO-XL,numParams,11000000000.0,,True
PLATO-XL,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
PLATO-XL,releaseDate,2021-09-20,,True
NPLM,flops,1303898760000000,"""For example, consider the following architecture used in the experiments on the AP (AssociatedPress) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the orderof the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""The first 800,000 words were used for training... reducing the vocabulary size to |V| = 16,383convergence of the stochastic gradient ascent procedure was obtained after around 10to 20 epochs for the Brown corpusNOTE: there are two corpuses. The one represented in this calculation is the Brown one, which got a better improvement over sota",True
NPLM,numParams,11904264.0,,True
NPLM,numTokens,1000000.0,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",True
NPLM,releaseDate,2003-03-15,,True
Noisy Student (L2),flops,849346560000000000000,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""2048*4.00E+12*60**2*24*4*0.3 = 8.5e20",True
Noisy Student (L2),numParams,480000000.0,,True
Noisy Student (L2),numTokens,81000000.0,"""Due to duplications, there are only 81M unique images among these 130M images.""",True
Noisy Student (L2),trainingTimeDays,144.0,6 days,True
Noisy Student (L2),gpuCount,1024.0,,True
Noisy Student (L2),gpuType,Google TPU v3,,True
Noisy Student (L2),releaseDate,2019-11-11,,True
ESM1-670M (UR100),flops,140000000000000000000,"Information: 128 NVIDIA V100 GPUs [Pre-training details]275k steps [See Table S2: Hyperparameters]131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]Estimate:  275e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 1.4e20 FLOP",True
ESM1-670M (UR100),numParams,669200000.0,,True
ESM1-670M (UR100),gpuCount,128.0,,True
ESM1-670M (UR100),gpuType,NVIDIA V100,,True
ESM1-670M (UR100),releaseDate,2020-08-31,,True
PaLM-E,numParams,562000000000.0,,True
PaLM-E,releaseDate,2023-03-06,,True
Whisper v2,flops,1.1e+23,"""Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.""We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23",True
Whisper v2,numParams,1550000000.0,,True
Whisper v2,numTokens,9302400000.0,"""When scaled to 680,000 hours of multilingual and multitasksupervision, the resulting models generalize wellto standard benchmarks and are often competitivewith prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""13,680 words/h (estimate) * 680,000h = 9,302,400,000 words",True
Whisper v2,releaseDate,2022-12-05,,True
Gen-2,releaseDate,2023-03-20,,True
ONLSTM-SYD,flops,138999999999999980,,True
ONLSTM-SYD,numParams,25000000.0,,True
ONLSTM-SYD,releaseDate,2020-05-12,,True
CoRe,numParams,12400000000.0,,True
CoRe,gpuType,NVIDIA A100 SXM4 40 GB,,True
CoRe,releaseDate,2023-12-29,,True
Amazon Titan,releaseDate,2023-09-28,,True
EfficientNet-L2,numParams,480000000.0,,True
EfficientNet-L2,releaseDate,2019-05-28,,True
Vespa,numParams,231000.0,,True
Vespa,releaseDate,2021-12-30,,True
A Bayesian Approach to Unsupervised One-Shot Learning of Object Categories,numParams,100.0,,True
A Bayesian Approach to Unsupervised One-Shot Learning of Object Categories,releaseDate,2003-10-13,,True
ProtT5-XXL,flops,7.37e+22,source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb,True
ProtT5-XXL,numParams,11000000000.0,,True
ProtT5-XXL,numTokens,393000000000.0,"""Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids.""",True
ProtT5-XXL,costDollars,123918.362790999,,True
ProtT5-XXL,gpuCount,512.0,,True
ProtT5-XXL,gpuType,Google TPU v3,,True
ProtT5-XXL,releaseDate,2021-05-04,,True
DeepSeek LLM 67B,flops,8.04e+23,67B * 2T * 6 = 8.04e23,True
DeepSeek LLM 67B,numParams,67000000000.0,,True
DeepSeek LLM 67B,numTokens,1750000000000.0,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English""if it's half English and half Chinese, that's750B English words + 1T Chinese words = 1.75T wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.1m0mpkhs9ljx",True
DeepSeek LLM 67B,releaseDate,2024-01-05,,True
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),releaseDate,2017-08-07,,True
Denoising Autoencoders,releaseDate,2008-07-05,,True
Elastic weight consolidation,releaseDate,2016-12-02,,True
LMS,releaseDate,1960-06-30,,True
AlexaTM 20B,flops,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",True
AlexaTM 20B,numParams,19750000000.0,,True
AlexaTM 20B,batchSize,2000000.0,,True
AlexaTM 20B,trainingTimeDays,2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",True
AlexaTM 20B,gpuCount,128.0,,True
AlexaTM 20B,gpuType,NVIDIA A100,,True
AlexaTM 20B,gpuUtilization,0.4935,,True
AlexaTM 20B,releaseDate,2022-08-02,,True
AlexaTM 20B,batchSize,2000000.0,"""We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens""",True
CLIP ViT-H/14 - LAION-2B,numParams,986000000.0,,True
CLIP ViT-H/14 - LAION-2B,numTokens,2000000000.0,"2B size of  LAION-2Binput is image text pair""A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).""",True
CLIP ViT-H/14 - LAION-2B,releaseDate,2022-09-15,,True
TensorReasoner,releaseDate,2013-12-01,,True
Internal functionality of visual invariants,releaseDate,1979-05-02,,True
R-Transformer,flops,8400000000000000,,True
R-Transformer,numParams,15800000.0,,True
R-Transformer,releaseDate,2019-07-12,,True
LingoWhale-8B,numParams,8000000000.0,,True
LingoWhale-8B,releaseDate,2023-11-01,,True
Pointer Sentinel-LSTM (medium),flops,7490000000000000,,True
Pointer Sentinel-LSTM (medium),numParams,21000000.0,,True
Pointer Sentinel-LSTM (medium),releaseDate,2016-09-26,,True
CodeGeeX,flops,6.630000000001e+22,Assume 1 epoch on 850B tokens.C=6DN=6*850B*13Bhttps://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion,True
CodeGeeX,numParams,13000000000.0,,True
CodeGeeX,numTokens,637500000000.0,"As of June 22, 2022, CodeGeeX has been trained on more than 850 billion tokens",True
CodeGeeX,batchSize,6291456.0,,True
CodeGeeX,trainingTimeDays,156.1,"Assume 30% utilization on 1536 Ascend 910 calculating in FP16.https://www.wolframalpha.com/input?i=6+*+13+billion+*+850+billion+FLOP+%2F+%280.30*1536*256+TFLOPS%29If they used INT8 precision, the training time would be half of this.",True
CodeGeeX,gpuType,Huawei Ascend 910,,True
CodeGeeX,releaseDate,2022-06-22,,True
CodeGeeX,batchSize,6291456.0,Table 3. 2048 * 3072,True
CryptoGRU,releaseDate,2020-10-22,,True
ERNIE 3.0,flops,2.25e+22,"Section 3.3.3: """"The model is trained fora total of 375 billion tokens""Total compute approximated as 6*N*D",True
ERNIE 3.0,numParams,10000000000.0,,True
ERNIE 3.0,numTokens,668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.""1 GB ~ 167M chinese words",True
ERNIE 3.0,gpuCount,384.0,,True
ERNIE 3.0,gpuType,NVIDIA V100,,True
ERNIE 3.0,releaseDate,2021-07-05,,True
GNMT,flops,6.9e+21,"sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  = 6.9e6 PF = 79 pfs-dayssource: https://openai.com/blog/ai-and-compute/",True
GNMT,numParams,278000000.0,,True
GNMT,numTokens,360000000.0,"[WORDS]"" On WMT En→Fr, the training set contains 36M sentence pairs. On WMT En→De, the training set contains 5M sentence pairs.""36M sentences * 10 words/sentence",True
GNMT,costDollars,307573.498950631,,True
GNMT,trainingTimeDays,4320.0,,True
GNMT,gpuCount,96.0,,True
GNMT,gpuType,NVIDIA Tesla K80,,True
GNMT,releaseDate,2016-09-26,,True
AmoebaNet-A (F=448),flops,385296912000000000000,"450 K40 GPUs for 20k models (approx. 7 days).(From Imagenet paper-data, Besiroglu et al., forthcoming) ",True
AmoebaNet-A (F=448),numParams,469000000.0,,True
AmoebaNet-A (F=448),numTokens,1280000.0,,True
AmoebaNet-A (F=448),costDollars,5858.75438360632,,True
AmoebaNet-A (F=448),trainingTimeDays,168.0,"""Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).""",True
AmoebaNet-A (F=448),gpuCount,450.0,,True
AmoebaNet-A (F=448),gpuType,NVIDIA Tesla K40s,,True
AmoebaNet-A (F=448),releaseDate,2018-02-05,,True
SearchFusion,releaseDate,2013-04-02,,True
LLaMA-65B (LoRA finetuned),numParams,65200000000.0,,True
LLaMA-65B (LoRA finetuned),releaseDate,2023-05-23,,True
DITTO,flops,11000000000000000000,,True
DITTO,numParams,750000000.0,,True
DITTO,releaseDate,2022-06-06,,True
CaLM,flops,29000000000000000000,"""4 NVIDIA Quadro RTX4000 GPUs for 40 days""Calculation assuming FP32, utilization 30%:= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU",True
CaLM,numParams,86000000.0,,True
CaLM,numTokens,9000000.0,"""a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing""",True
CaLM,trainingTimeDays,960.0,"""The model reported in this work was trained on 4 NVIDIA QuadroRTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)""",True
CaLM,gpuCount,4.0,,True
CaLM,gpuType,NVIDIA Quadro RTX 4000,,True
CaLM,releaseDate,2022-12-19,,True
StarCoder 2 7B,flops,1.55e+23,estimation is given in Table 6 ,True
StarCoder 2 7B,numParams,15000000000.0,,True
StarCoder 2 7B,numTokens,3500000000000.0,"from Table 7, 3.5T tokens ",True
StarCoder 2 7B,gpuType,NVIDIA H100 SXM5,,True
StarCoder 2 7B,releaseDate,2024-02-29,,True
ProteinBERT,flops,65000000000000000000,"""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ∼670M records""28 * 24 * 3600 * 89 tFLOP/s * 0.3 (assumed utilization) = 6.5e19",True
ProteinBERT,numParams,16000000.0,,True
ProteinBERT,trainingTimeDays,672.0,28 days,True
ProteinBERT,gpuType,NVIDIA Quadro RTX 5000,,True
ProteinBERT,releaseDate,2022-02-10,,True
AlphaFold,flops,100000000000000000000,"Estimated in the blogpost belowhttps://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening""AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.""",True
AlphaFold,numParams,16340840.0,,True
AlphaFold,costDollars,241.593133715473,,True
AlphaFold,trainingTimeDays,120.0,"""Training time: about 5 days for 600,000 steps""",True
AlphaFold,releaseDate,2020-01-15,,True
Platypus-70B,numParams,70000000000.0,,True
Platypus-70B,gpuType,NVIDIA A100,,True
Platypus-70B,releaseDate,2023-08-14,,True
RT-2,numParams,55000000000.0,,True
RT-2,releaseDate,2023-07-28,,True
KwaiYiiMath,numParams,13000000000.0,,True
KwaiYiiMath,releaseDate,2023-10-19,,True
JFT,flops,843000000000000000000,Tesla K80 performance: 8.13 TFLOP/sAssume 40% utilization60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP,True
JFT,numTokens,300000000.0,,True
JFT,costDollars,21396.4173183048,,True
JFT,trainingTimeDays,1440.0,,True
JFT,gpuCount,50.0,,True
JFT,gpuType,NVIDIA Tesla K80,,True
JFT,releaseDate,2017-07-10,,True
RWKV-4 World (7B),numParams,7000000000.0,,True
RWKV-4 World (7B),releaseDate,2023-06-26,,True
RBM-tuning,releaseDate,2010-08-02,,True
AWD-LSTM + dynamic eval (PTB),releaseDate,2017-09-21,,True
GroupLens,numTokens,100000000.0,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",True
GroupLens,releaseDate,1994-10-22,,True
RNN (SGD+CLR) (PTB),numParams,2050000.0,,True
RNN (SGD+CLR) (PTB),releaseDate,2012-12-04,,True
CogVLM,flops,1.9880639999999998e+22,from table 8 on page 17230.1 FLOPS*days so 10**15*24*3600*230.1= 1.988e22,True
CogVLM,numParams,17000000000.0,,True
CogVLM,releaseDate,2023-11-06,,True
Generative BST,numParams,9400000000.0,,True
Generative BST,releaseDate,2021-03-05,,True
TAPE Transformer,flops,30000000000000000000,"""All self-supervised models are trained on four NVIDIA V100 GPUs for one week""(7 * 24 * 3600) s * 4 GPUs * 3.1e13 FLOP/s * 0.4 (utilization assumption) = 3e19",True
TAPE Transformer,numParams,38000000.0,,True
TAPE Transformer,trainingTimeDays,168.0,,True
TAPE Transformer,gpuCount,4.0,,True
TAPE Transformer,gpuType,NVIDIA V100,,True
TAPE Transformer,releaseDate,2019-06-19,,True
Stacked Denoising Autoencoders,releaseDate,2010-01-03,,True
OPT-1.3B (finetuned on PTB),numParams,1300000000.0,,True
OPT-1.3B (finetuned on PTB),releaseDate,2022-06-21,,True
Pythia-6.9b,numParams,6900000000.0,,True
Pythia-6.9b,releaseDate,2023-04-03,,True
Neuro-Symbolic Concept Learner,releaseDate,2019-04-26,,True
Super-vector coding,numTokens,24706.0,"9,963 + 14,743""PASCAL VOC 2007 consists of 9,963 images which are divided intothree subsets: training data (2501 images), validation data (2510 images), andtest data (4952 images). PASCAL VOC 2009 consists of 14,743 images and corre-spondingly are divided into three subsets: training data(3473 images), validationdata(3581 images), and testing data (7689 images).""",True
Super-vector coding,releaseDate,2010-01-01,,True
BPE,numTokens,37500000.0,"[WORDS]""We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens""100M tokens, around half will be in English, 0.75 words per token",True
BPE,releaseDate,2015-08-31,,True
Flan UL2,numParams,19500000000.0,,True
Flan UL2,releaseDate,2023-03-03,,True
GPT-3 175B (davinci),flops,3.14e+23,Table D.1https://arxiv.org/abs/2005.14165,True
GPT-3 175B (davinci),numParams,175000000000.0,,True
GPT-3 175B (davinci),numTokens,374000000000.0,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. We multiply this by 0.75 to give 374B words. 3.74e11========================[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""Converted to words using http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html2.85e11",True
GPT-3 175B (davinci),batchSize,3200000.0,,True
GPT-3 175B (davinci),costDollars,1131415.12384028,,True
GPT-3 175B (davinci),trainingTimeDays,355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,True
GPT-3 175B (davinci),gpuCount,10000.0,,True
GPT-3 175B (davinci),gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
GPT-3 175B (davinci),gpuUtilization,0.2196,,True
GPT-3 175B (davinci),releaseDate,2020-05-28,,True
GPT-3 175B (davinci),batchSize,3200000.0,"3.2M, per table 2.1",True
Baichuan2-53B,numParams,53000000000.0,,True
Baichuan2-53B,releaseDate,2023-08-09,,True
Zidong Taichu,numParams,100000000000.0,,True
Zidong Taichu,releaseDate,2021-08-11,,True
CT-MoS (WT2),flops,562000000000000000,,True
CT-MoS (WT2),numParams,45000000.0,,True
CT-MoS (WT2),releaseDate,2020-12-25,,True
Inflated 3D ConvNet,releaseDate,2017-06-01,,True
Zoneout + Variational LSTM (PTB),releaseDate,2016-09-26,,True
Mistral Large,flops,2.0000000001e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48https://x.com/EMostaque/status/1762152740938031484?s=20""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad MostaqueAssuming bf16 or fp16, H100 PCIe performance is 1513 TFLOPSAt 1.9 euro per H100-hour and 33% utilization, spending 20M euro produces 1.9*10^25 FLOP.https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+3958+TFLOPS+*+0.33https://www.scaleway.com/en/h100-pcie-try-it-now/",True
Mistral Large,costDollars,18500000.0,"In February 2024, 20M EUR = 22M USDConverting to 2020 USD, this is 18.5Mhttps://www.in2013dollars.com/us/inflation/2024?endYear=2020&amount=22000000",True
Mistral Large,trainingTimeDays,2500.0,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,True
Mistral Large,gpuType,NVIDIA H100 PCIe,,True
Mistral Large,releaseDate,2024-02-26,,True
"MSRA (C, PReLU)",flops,23974030080000000000,"""training C on eight K40 GPUs, takes about 3-4 weeks""0.33 util rate(From Imagenet paper-data, Besiroglu et al., forthcoming) ",True
"MSRA (C, PReLU)",numParams,87048800.0,,True
"MSRA (C, PReLU)",numTokens,1280000.0,"""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",True
"MSRA (C, PReLU)",costDollars,2166.21810494374,,True
"MSRA (C, PReLU)",trainingTimeDays,588.0,,True
"MSRA (C, PReLU)",releaseDate,2015-02-06,,True
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),flops,693000000000000000,,True
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),numParams,185000000.0,,True
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),releaseDate,2018-08-30,,True
RNN + char4-MS-vec,numParams,226000000.0,,True
RNN + char4-MS-vec,releaseDate,2019-07-17,,True
DeepSeekMoE-16B,flops,3.4e+22,"""With the DeepSeekMoE architecture, we scale up our MoE model to a larger scale with 16B totalparameters and train it on 2T tokens""""Evaluation results reveal that with only about 40% of computations, DeepSeekMoE 16B achieves comparable performancewith DeepSeek 7B (DeepSeek-AI, 2024), a dense model trained on the same 2T corpus""40% * 7B = 2.8B, so 2.8B effective parameters2.8B * 2T * 6 ~= 3.4e22",True
DeepSeekMoE-16B,numParams,16000000000.0,,True
DeepSeekMoE-16B,numTokens,1800000000000.0,"""Leveraging our architecture, we subsequently scale up the model parameters to 16B andtrain DeepSeekMoE 16B on a large-scale corpus with 2T tokens.""Probably a mix of English and Chinese. 1T English tokens is 0.75T words; 1T Chinese tokens is 1T words, so ~1.8T total",True
DeepSeekMoE-16B,gpuType,"NVIDIA A100,NVIDIA H800",,True
DeepSeekMoE-16B,releaseDate,2024-01-11,,True
Luminous-supreme,flops,2.8e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.312 trillion * 839000 * 3600 * 0.3 = 2.8e23",True
Luminous-supreme,numParams,70000000000.0,,True
Luminous-supreme,gpuType,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",,True
Luminous-supreme,releaseDate,2022-08-15,,True
T0-XXL,flops,1.792e+22,"From section B.1: ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device."" (512 cores for 270 hours)",True
T0-XXL,numParams,11000000000.0,,True
T0-XXL,trainingTimeDays,270.0,"""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.""",True
T0-XXL,gpuCount,256.0,,True
T0-XXL,gpuType,Google TPU v3,,True
T0-XXL,releaseDate,2021-10-15,,True
AdvSoft + 4 layer QRNN + dynamic evaluation,releaseDate,2019-06-10,,True
GPT-2 (1.5B),flops,4.3e+21,"We use COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT* N EPOCHS * N TOKENS IN TRAINING DATASETThe number of epochs is not reported, but this other paper [1] claims in table 1 that it is 20 or 100 epochs. 100 epochs is consistent with the original GPT paper. 40GB dataset is 8B words, or 1/0.75 * 8B = 10.66B tokens.6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e216 * (40 * 200 million * 1/0.75 * 100) * 1.5 billion parameters = 9.6e21Geometric mean is 4.29e21[1] https://arxiv.org/abs/1906.06669",True
GPT-2 (1.5B),numParams,1500000000.0,,True
GPT-2 (1.5B),numTokens,3000000000.0,“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”40GB is approximately 8e9 words.,True
GPT-2 (1.5B),costDollars,4692.88689188115,"https://en.wikipedia.org/wiki/GPT-2#:~:text=The%20cloud%20compute%20costs%20for,full%201.5%20billion%20parameter%20model).",True
GPT-2 (1.5B),releaseDate,2019-02-14,,True
ConvNet similarity metric,numTokens,140000.0,"The actual training set that was used contained140,000 image pairs that were evenly split between genuineand impostor.",True
ConvNet similarity metric,releaseDate,2005-06-20,,True
TaLK Convolution,flops,27800000000000000000,,True
TaLK Convolution,numParams,240000000.0,,True
TaLK Convolution,releaseDate,2020-02-08,,True
GPipe (Transformer),numParams,6000000000.0,,True
GPipe (Transformer),numTokens,20000000000.0,"[WORDS]Section 5: ""We use acorpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language""10^9 sentences * 20 words per sentence",True
GPipe (Transformer),releaseDate,2018-11-16,,True
Word2Vec (small),numParams,207600000.0,,True
Word2Vec (small),numTokens,692000.0,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",True
Word2Vec (small),releaseDate,2013-10-16,,True
Yuan 2.0,numParams,102600000000.0,,True
Yuan 2.0,releaseDate,2023-11-27,,True
TrOCR,numParams,558000000.0,,True
TrOCR,numTokens,703300000.0,"The input data to the model are images.684M + 3.3M + 16Mfrom Experiment section: ""In total, the first-stage pre-training dataset contains 684M textlines."" ""In total, the printed dataset consists of 3.3M textlines.""and from MJSynth, SynthText datasets there is ""about 16M text images.""",True
TrOCR,gpuCount,32.0,,True
TrOCR,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
TrOCR,releaseDate,2021-09-21,,True
Restricted Bolzmann machines,numTokens,100480507.0,"The training data set consists of 100,480,507ratings",True
Restricted Bolzmann machines,releaseDate,2007-06-20,,True
Megatron-Turing NLG 530B,flops,1.17e+24,https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx,True
Megatron-Turing NLG 530B,numParams,530000000000.0,,True
Megatron-Turing NLG 530B,numTokens,202500000000.0,"""Our training dataset consists of 339 billion tokens and wetrained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""1 token ~ 0.75 words",True
Megatron-Turing NLG 530B,batchSize,3932160.0,,True
Megatron-Turing NLG 530B,costDollars,3046994.0871934,,True
Megatron-Turing NLG 530B,trainingTimeDays,770.0,"Total compute was 1.17*10^24 FLOP.They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",True
Megatron-Turing NLG 530B,gpuCount,4480.0,,True
Megatron-Turing NLG 530B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Megatron-Turing NLG 530B,gpuUtilization,0.302,,True
Megatron-Turing NLG 530B,releaseDate,2021-10-11,,True
Megatron-Turing NLG 530B,batchSize,3932160.0,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" Final batch size is 1920 * 2048 = 3932160",True
Conditional probability machines,releaseDate,1956-07-01,,True
Textual Imager,releaseDate,2013-01-16,,True
Mamba-24M (SC09),numParams,23400000.0,,True
Mamba-24M (SC09),releaseDate,2023-12-01,,True
MLP as Bayesian Approximator,releaseDate,1990-12-01,,True
Pythia-160m,numParams,160000000.0,,True
Pythia-160m,releaseDate,2023-04-03,,True
Adaptive Broom Balancer,numParams,110.0,,True
Adaptive Broom Balancer,releaseDate,1988-07-24,,True
Transformer-XL DeFINE (107M),releaseDate,2019-11-27,,True
Yi-34B,flops,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?If so, 34b * 3T * 6 = 6.1e23",True
Yi-34B,numParams,34000000000.0,,True
Yi-34B,releaseDate,2023-11-02,,True
PermuteFormer,flops,3100000000000000000,,True
PermuteFormer,numParams,33000000.0,,True
PermuteFormer,releaseDate,2021-09-06,,True
BigGAN-deep 512x512,flops,3.00000000001e+21,"3e21, estimate taken from:https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",True
BigGAN-deep 512x512,numParams,112694781.0,,True
BigGAN-deep 512x512,numTokens,292000000.0,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. """,True
BigGAN-deep 512x512,costDollars,10448.4378707433,,True
BigGAN-deep 512x512,trainingTimeDays,48.0,"""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for most models""",True
BigGAN-deep 512x512,gpuCount,256.0,,True
BigGAN-deep 512x512,gpuType,Google TPU v3,,True
BigGAN-deep 512x512,releaseDate,2018-09-28,,True
Population-based DRL,flops,34900000000000000000,Source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,True
Population-based DRL,numParams,122000000.0,,True
Population-based DRL,costDollars,130.362699519232,,True
Population-based DRL,releaseDate,2018-07-03,,True
"ALiBi (L=3072, Lvalid = 3072)",flops,810000000000000000000,"From figure 5, 6000 GPU hours (Nvidia V100) 6000*  125 teraflop/s * 3600 * 0.3 = 8.1e20",True
"ALiBi (L=3072, Lvalid = 3072)",numParams,1300000000.0,,True
"ALiBi (L=3072, Lvalid = 3072)",releaseDate,2021-08-27,,True
Pangu 3.0,numParams,100000000000.0,,True
Pangu 3.0,releaseDate,2023-07-07,,True
Llama 2-7B,flops,8.4e+22,"Trained on 2 trillion tokens per Table 1. C = 6ND = 6*7B*2T = 8.4e+22 FLOP.Also, 7B model was trained on 184320 GPU-hours312 trillion * 184320 * 3600 * 0.3 = 6.21e22",True
Llama 2-7B,numParams,70000000000.0,,True
Llama 2-7B,numTokens,1500000000000.0,2 trillion tokens ~= 1.5 trillion words,True
Llama 2-7B,batchSize,4000000.0,,True
Llama 2-7B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Llama 2-7B,releaseDate,2023-07-18,,True
Llama 2-7B,batchSize,4000000.0,,True
GPT-2 (345M),numTokens,3000000000.0,“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”40GB is approximately 3e9 words.,True
GPT-2 (345M),releaseDate,2019-02-14,,True
Whisper v3,flops,2.7e+23,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23",True
Whisper v3,numParams,1550000000.0,,True
Whisper v3,numTokens,60000000000.0,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ceThe dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have200*60*5 million hours = 60,000,000,000 (60B) words",True
Whisper v3,releaseDate,2023-11-06,,True
Neocognitron,flops,228115200,"""It does not necessarily mean that all of these input synapses arealways fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""We multiply by 2 to account for multadds",True
Neocognitron,numParams,1140576.0,,True
Neocognitron,numTokens,5.0,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",True
Neocognitron,releaseDate,1980-04-01,,True
LLaVA 1.5,flops,65000000000000000000,"6.5e19 finetuning compute. fine-tuned from Vicuna-13B which is fine-tuned Llama-13B, which was 4.55e22 FLOP",True
LLaVA 1.5,numParams,13000000000.0,,True
LLaVA 1.5,trainingTimeDays,24.0,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,True
LLaVA 1.5,gpuCount,8.0,,True
LLaVA 1.5,gpuType,NVIDIA A100,,True
LLaVA 1.5,releaseDate,2023-11-05,,True
EVA,flops,3.7509433344e+21,"flops = (128) * (77.97 * 10**12) * (14.5 * 24 * 3600) * (0.3) = 3.75e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16",True
EVA,numParams,1011000000.0,,True
EVA,numTokens,29600000.0,from table 3: 29.6M images,True
EVA,trainingTimeDays,348.0,from Table 3 14.5 days = 348 hours,True
EVA,gpuCount,128.0,,True
EVA,gpuType,NVIDIA A100 SXM4 40 GB,,True
EVA,releaseDate,2022-11-14,,True
XLNet,numParams,360268800.0,,True
XLNet,releaseDate,2019-06-01,,True
LTM-1,releaseDate,2023-06-06,,True
BASIC-L,flops,4.12e+22,"6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8Two cores per chip, and 275 teraflop/s per chip (https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22",True
BASIC-L,numParams,3070000000.0,,True
BASIC-L,numTokens,6700000000.0,6.7B image-text pairs,True
BASIC-L,gpuType,Google TPU v4,,True
BASIC-L,releaseDate,2021-11-19,,True
Immediate trihead,releaseDate,2001-07-06,,True
TCN (148M),numParams,148000000.0,,True
TCN (148M),releaseDate,2018-02-15,,True
MemoReader,gpuType,NVIDIA M40,,True
MemoReader,releaseDate,2018-10-31,,True
M6-10T,flops,5.53e+21,"512 GPUs in 10 days - using NVIDIA V100 GPUsUsing the NVIDIA V100 Specifications this works out to be: 0.30 * 125E12 * 512 * 10 * 86400 = 1.66E22(Assuming 30% utilisation, and 125 TFLOPS)",True
M6-10T,numParams,10000000000000.0,,True
M6-10T,numTokens,8000000000.0,"""We conduct experiments for pretraining and finetuning to analyze model competence in upstream anddownstream tasks. Following the classical data setup for pretraining and finetuning, we pretrain the model on BookCorpus [52] and English Wikipedia [9], which are corpora with around 16GB of plaintexts.""I used http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html for the conversion to number of words",True
M6-10T,costDollars,20073.4941326502,,True
M6-10T,releaseDate,2021-10-08,,True
Falcon-7B,flops,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""",True
Falcon-7B,numParams,7000000000.0,,True
Falcon-7B,numTokens,1125000000000.0,"1125000000000.0 words  assuming 0.75 words per token (1.5T tokens)""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""",True
Falcon-7B,gpuCount,384.0,,True
Falcon-7B,gpuType,NVIDIA A100 SXM4 40 GB,,True
Falcon-7B,releaseDate,2023-04-24,,True
CTR-BERT,flops,64696320000000000000,flops = (8) * (312 * 10**12) * (24 * 3600) * (0.3)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)'CTR-BERTuses about 70 million parameters which can be trained on 8 A100 GPUs in less than a day (<1000USD).',True
CTR-BERT,numParams,70000000.0,,True
CTR-BERT,gpuCount,8.0,,True
CTR-BERT,gpuType,NVIDIA A100,,True
CTR-BERT,releaseDate,2021-12-06,,True
Gemini 1 Pro,gpuType,Google TPU v4,,True
Gemini 1 Pro,releaseDate,2023-12-06,,True
AWD-LSTM,numParams,24000000.0,,True
AWD-LSTM,releaseDate,2017-07-18,,True
Gemini Nano-1,numParams,1800000000.0,,True
Gemini Nano-1,gpuType,Google TPU v5e,,True
Gemini Nano-1,releaseDate,2023-12-19,,True
DMN,releaseDate,2016-06-20,,True
Wu Dao Aquila2 34B,numParams,34000000000.0,,True
Wu Dao Aquila2 34B,releaseDate,2023-10-13,,True
MuZero,flops,48000000000000000000,"third-generation Google Cloud TPU(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-playTraining for 12 hours (for Atari)Data from Parameter, Compute and Data Trends in Machine LearningGoogle v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)Utilization rate In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%Calculations for Atari:12 hours → 43200 seconds(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOPTraining time missing for boardgamesAssumption also 12 hours Also: 2.4017472 * 10^19 FLOPTotal cost ≈ 4.8 * 10^19 FLOP",True
MuZero,numParams,36864000.0,,True
MuZero,numTokens,20000000000.0,Table 1https://arxiv.org/pdf/1911.08265.pdf,True
MuZero,costDollars,121.179038227546,,True
MuZero,releaseDate,2019-11-19,,True
ESM1-43M,flops,28000000000000000000,"Information: 128 NVIDIA V100 GPUs [Pre-training details]840k steps [See Table S2: Hyperparameters]131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 42.6e6 parameters = 2.8e19 FLOP",True
ESM1-43M,numParams,42600000.0,,True
ESM1-43M,gpuCount,128.0,,True
ESM1-43M,gpuType,NVIDIA V100,,True
ESM1-43M,releaseDate,2020-08-31,,True
CapsNet (MNIST),numParams,8200000.0,,True
CapsNet (MNIST),numTokens,60000.0,Section 5: The dataset has 60K and 10K images for training and testing respectively.,True
CapsNet (MNIST),releaseDate,2017-10-26,,True
Heuristic problem solving for AI,releaseDate,1961-01-01,,True
W2v-BERT,numParams,1000000000.0,,True
W2v-BERT,releaseDate,2021-08-07,,True
Qwen-Audio-Chat,numParams,8460000000.0,,True
Qwen-Audio-Chat,releaseDate,2023-11-14,,True
KnGPT2,flops,124000000000000000000,,True
KnGPT2,numParams,83000000.0,,True
KnGPT2,releaseDate,2021-10-15,,True
AWD-FWM (PTB),releaseDate,2020-11-16,,True
RNMT+,flops,39506227200000000000,"32 * 9.526 TFLOPS * (120 * 3600) * 0.3 = 39506227200000000000(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)""All models were trained with synchronoustraining. RNMT+ and ConvS2S were trained with32 NVIDIA P100 GPUs ""performence of P100 is 9.526 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-16-gb.c2888training time is 120h from Table 1",True
RNMT+,numParams,378900000.0,,True
RNMT+,numTokens,544500000.0,"""We train our models on the standard WMT’14 En→Fr and En→De datasets that comprise 36.3M""We estimate 15 english words in one sentence.",True
RNMT+,trainingTimeDays,120.0,from Table 1,True
RNMT+,gpuCount,32.0,,True
RNMT+,gpuType,NVIDIA P100,,True
RNMT+,releaseDate,2018-04-26,,True
Llama 3,gpuCount,24576.0,,True
Llama 3,gpuType,NVIDIA H100 SXM5,,True
Llama 3,releaseDate,2024-06-01,,True
Social and content-based classification,numTokens,45000.0,"""Our data set consists of more than 45,000 movie rat-ings collected from approximately 260 users.""",True
Social and content-based classification,releaseDate,1998-07-01,,True
DNA Fine-Tuned Language Model (DFLM),flops,3500000000000000000,"""The pre-training of Human genome language model isresource-intensive (about 7-10 days on 2 NVIDIA TITAN X GPU).""Assuming FP32 and 30% utilizationEstimate: (10*24*3600) s * 6.7e12 FLOP/s * 2 * 0.3 = 3.5e18",True
DNA Fine-Tuned Language Model (DFLM),gpuType,NVIDIA TITAN Xp,,True
DNA Fine-Tuned Language Model (DFLM),releaseDate,2023-01-02,,True
EGRU (PTB),releaseDate,2022-06-13,,True
LRSO-GAN,releaseDate,2017-10-22,,True
3-Layer-Tensor-Transformer+AdaHessian,releaseDate,2020-06-01,,True
DMPFold,flops,12000000000000,"""Training of all models was performed using the Adam optimiser for 75epochs with default parameters""""The training set here was based on the same 6729 protein chains, ≤500 residues in length, with non-redundancy at the 25% sequence identity level and no unresolved main chain atoms""Estimate = 2 * 3.8e6 * 3 * 6729 * 75 ~ 1.2e13 FLOP",True
DMPFold,numParams,3800000.0,,True
DMPFold,numTokens,6729.0,"""The training set here was based on the same 6729 protein chains""",True
DMPFold,releaseDate,2018-11-29,,True
GPT-2+Active-SGD,flops,310000000000000000,,True
GPT-2+Active-SGD,numParams,124000000.0,,True
GPT-2+Active-SGD,releaseDate,2023-01-24,,True
Cohere Command Light,flops,1.001e+22,"https://docs.cohere.com/docs/environmental-impact2700kg CO2 equivalent. Cohere used this calculator: https://mlco2.github.io/impact/ This calculator claims that ~40000 TPUv3 hours causes ~3000 kg CO2 emissions in the ""us-west1"", ""us-west2"", and ""us-west3"" regions. Not clear what region the data center Cohere used was in. Google has data centers around the world; *most* regions are similarly carbon intensive as us-west but north-america-northeast is 10x less carbon intensive and south Asia is 5x more carbon intensive. So the calculation below could be quite off.Cohere most likely used TPUv4s, which the calculator does not support, which seem to be much more efficient (2.7x more, according to this https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)40000 hours * 123 teraflops * 3600 * 0.3 utilization * 2.7 = 1.4e22",True
Cohere Command Light,numParams,6000000000.0,,True
Cohere Command Light,gpuType,Google TPU v4,,True
Cohere Command Light,releaseDate,2023-11-30,,True
PSPNet,releaseDate,2017-07-21,,True
DeLight,flops,24000000000000000000,,True
DeLight,numParams,99000000.0,,True
DeLight,releaseDate,2020-08-03,,True
BERT-Large-CAS (WT103),releaseDate,2019-04-20,,True
ProxylessNAS,flops,37065600000000000000,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.At FP32, a V100 GPU has a peak performance of 1.56E+14 FLOPS.Utilization rate of 0.33.",True
ProxylessNAS,numTokens,1280000.0,,True
ProxylessNAS,costDollars,135.039869619647,,True
ProxylessNAS,gpuType,NVIDIA V100,,True
ProxylessNAS,releaseDate,2019-02-23,,True
ELECTRA,flops,3.1e+21,"Table 8: ""ELECTRA-1.75M"" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.",True
ELECTRA,numParams,335000000.0,,True
ELECTRA,releaseDate,2020-03-23,,True
LLaMA-7B (protein-oriented instructions finetuned),flops,2.78e+22,"Estimate 1: 1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOPfrom paper, Llama-7B took 82,432 GPU hours using A100sEstimate 2: 312 trillion FLOP/s * (82,432 * 3600) s * 0.3 = 2.78e22 FLOP",True
LLaMA-7B (protein-oriented instructions finetuned),numParams,7000000000.0,,True
LLaMA-7B (protein-oriented instructions finetuned),releaseDate,2023-10-02,,True
Sparse Energy-Based Model,numTokens,60000.0,,True
Sparse Energy-Based Model,releaseDate,2006-12-04,,True
GPT-NeoX-Japanese,numParams,2700000000.0,,True
GPT-NeoX-Japanese,releaseDate,2022-07-27,,True
ESM2-3B,flops,3.000000001e+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 1.8e22 FLOPfrom the paper's Supplementary Materials: ""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""30 days x 512 V100s x an imputed 30% utilization"": 5e22 FLOPGeometric mean: 3e22",True
ESM2-3B,numParams,3000000000.0,,True
ESM2-3B,trainingTimeDays,720.0,,True
ESM2-3B,releaseDate,2022-07-21,,True
Emu (Meta),numParams,2800000000.0,,True
Emu (Meta),numTokens,1100000000.0,"""We curate a large internal pre-training dataset consisting of 1.1 billion images to train our model. The model is trained with progressively increasing resolutions""",True
Emu (Meta),releaseDate,2023-09-27,,True
Perfusion,releaseDate,2023-05-02,,True
ESM1b,flops,460000000000000000000,"Information: 128 NVIDIA V100 GPUs [Pre-training details]8.5 hours on 64 GPUs per epoch, 56 epochs [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]128 NVIDIA V100 GPU, assuming  V100 PCIe single precision 14 TFLOPS and 0.3 utilization rateEstimate: (8.5*56*3600) s * 14e12 FLOP/s * 0.3 *64 = 4.6e20 FLOPs",True
ESM1b,numParams,652400000.0,,True
ESM1b,gpuCount,128.0,,True
ESM1b,gpuType,NVIDIA V100,,True
ESM1b,releaseDate,2020-12-15,,True
Markov-driven POS tagger,numParams,2447124.0,,True
Markov-driven POS tagger,numTokens,1000000.0,"""We use the ""treebank"" data described in Beale (1988). It contains 42,186 sentences (about one million words) from the Associated Press.""https://www.aclweb.org/anthology/J94-2001.pdf",True
Markov-driven POS tagger,releaseDate,1994-06-01,,True
DARTS,flops,11000000000000000,,True
DARTS,numParams,33000000.0,,True
DARTS,releaseDate,2018-06-24,,True
GRU + p-tHSM (pretrain via Brown) (WT2),releaseDate,2017-08-19,,True
DeepLab,releaseDate,2014-12-22,,True
MobileBERT,numParams,25300000.0,,True
MobileBERT,releaseDate,2020-04-06,,True
LaNet-L (CIFAR-10),numParams,44100000.0,,True
LaNet-L (CIFAR-10),releaseDate,2019-06-17,,True
RNN (SGD+CLR),numParams,195600.0,,True
RNN (SGD+CLR),releaseDate,2012-12-14,,True
OPT-1.3B (finetuned),numParams,1300000000.0,,True
OPT-1.3B (finetuned),releaseDate,2022-06-21,,True
FrameNet role labeling,numTokens,50000.0,"Abstract: ""The system is based on statistical classifiers trained on roughly 50,000 sentences""",True
FrameNet role labeling,releaseDate,2000-09-01,,True
StarGAN v2,releaseDate,2019-12-04,,True
OR-WideResNet,numParams,18200000.0,,True
OR-WideResNet,gpuType,NVIDIA Tesla K80,,True
OR-WideResNet,releaseDate,2017-01-07,,True
LLaMA-33B (LoRA finetuned),numParams,33000000000.0,,True
LLaMA-33B (LoRA finetuned),releaseDate,2023-05-23,,True
Aya,numParams,13000000000.0,,True
Aya,gpuCount,128.0,,True
Aya,gpuType,Google TPU v4,,True
Aya,releaseDate,2024-02-12,,True
Adaptive Agent,flops,2.8e+21,"""AdA was implemented using JAX (Bradbury et al., 2018) and the DeepMind JAX Ecosystem (Babuschkin et al., 2020) and trained on 64 Google TPUv3 devices. The wall-clock time for training this version of AdA from scratch was approximately 5 weeks: 1 week to train the teacher, and 4 weeks to train AdA""64 * 123 teraflop/s * 35 days * 24 * 3600 * 0.4 = 9.5e21This might be for all single-agent experiments in the paper, or just for the 76M model in Table D.1, I'm not sure.In Table E.2, the 533M-param model takes 2e20 FLOP to go through 5B learner steps, and was trained on 70B steps in total (Table 1). That would be 2.8e21 for 70B steps. That might be an underestimate because there are also teacher(?) steps.",True
Adaptive Agent,numParams,533000000.0,,True
Adaptive Agent,trainingTimeDays,840.0,5 weeks. Possible that this is for multiple models,True
Adaptive Agent,gpuType,Google TPU v3,,True
Adaptive Agent,releaseDate,2023-01-18,,True
MetaMimic,numParams,22000000.0,,True
MetaMimic,releaseDate,2018-10-11,,True
Gemma 7B,flops,2.52e+23,"6ND aproximation 6*7B*6T = 2.5e23""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""we can also try to estimate from: ""We estimate the carbon emissions from pretrain-ing the Gemma models to be ∼ 131 𝑡𝐶𝑂2𝑒𝑞. """,True
Gemma 7B,numParams,7751248896.0,,True
Gemma 7B,numTokens,4500000000000.0,"assuming 0.75 words per token - so 4500000000000.0 words""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",True
Gemma 7B,gpuCount,4096.0,,True
Gemma 7B,gpuType,Google TPU v5e,,True
Gemma 7B,releaseDate,2024-02-21,,True
LEP-AD,releaseDate,2023-03-15,,True
Stacked Semisuperviser Autoencoders,numParams,3000000.0,,True
Stacked Semisuperviser Autoencoders,numTokens,66087.0,"""The 20 Newsgroups dataset contains 18845postings taken from the Usenet newsgroup collection.Documents are partitioned into 20 topics. The datasetis split into 11314 training documents and 7531 testdocuments. Training and test articles are separated intime. Reuters has a predefined ModApte split of thedata into 11413 training documents and 4024 test doc-uments. Documents belong to one of 91 topics. TheOhsumed dataset has 34389 documents with 30689words and each document might be assigned to morethan one topic, for a total of 23 topics. The dataset issplit into training and test by randomly selecting the67% and the 33% of the data""total # documents = 11314 + 11413 + 34389*0.6I'm using #documents here since the task is document representation. Using #words would increase the size by ~3 OOMs",True
Stacked Semisuperviser Autoencoders,releaseDate,2008-07-15,,True
MusicLM,numParams,860000000.0,,True
MusicLM,releaseDate,2023-01-26,,True
2nd order FOFE-FNNLM,numParams,6000000.0,,True
2nd order FOFE-FNNLM,releaseDate,2015-05-06,,True
Monarch-GPT-2-Medium,flops,436000000000000000000,,True
Monarch-GPT-2-Medium,numParams,165000000.0,,True
Monarch-GPT-2-Medium,releaseDate,2022-04-01,,True
MobileNetV2,numParams,3400000.0,,True
MobileNetV2,releaseDate,2018-06-18,,True
Multi-task Cascaded CNN,releaseDate,2016-08-26,,True
RCAN,releaseDate,2018-07-08,,True
GPT-Neo,flops,7.9e+21,source: https://www.aitracker.org/,True
GPT-Neo,numParams,2700000000.0,,True
GPT-Neo,numTokens,885837004800.0,"""In aggregate, the Pile consists of over 825GiB of raw text data""(see GPT-NeoX)",True
GPT-Neo,costDollars,13685.989184888,,True
GPT-Neo,releaseDate,2021-03-21,,True
Big-Little Net (vision),flops,3936768000000000000,number of epochs (appendix A1) times flops per inference (from table 2) times dataset size times 3 (to account for backpropagation),True
Big-Little Net (vision),numParams,77360000.0,,True
Big-Little Net (vision),numTokens,1280000.0,size of ImageNet,True
Big-Little Net (vision),releaseDate,2018-07-10,,True
ProtGPT2,flops,4.1e+21,"""The model trained on 128 NVIDIA A100s in 4 days""128 * 4 * 24 * 3600 * 312 trillion FLOP/s * 0.3 = 4.1e21",True
ProtGPT2,numParams,738000000.0,,True
ProtGPT2,trainingTimeDays,96.0,,True
ProtGPT2,gpuCount,128.0,,True
ProtGPT2,gpuType,NVIDIA A100,,True
ProtGPT2,releaseDate,2022-07-27,,True
Subformer (122M),flops,5300000000000000000,,True
Subformer (122M),numParams,122000000.0,,True
Subformer (122M),releaseDate,2021-01-01,,True
ESRGAN,releaseDate,2018-09-01,,True
ResNeXt-101 32x48d,flops,8.74395e+21,"Table 6: 153e9 mult-adds.Section 2.4: ""minibatches of 8,064 images"".Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP",True
ResNeXt-101 32x48d,numParams,829000000.0,,True
ResNeXt-101 32x48d,numTokens,9525000000.0,Table 3: (300+1925+300+7000) million images,True
ResNeXt-101 32x48d,gpuCount,336.0,,True
ResNeXt-101 32x48d,releaseDate,2018-05-02,,True
CodeGen2,numParams,16000000000.0,,True
CodeGen2,releaseDate,2023-05-03,,True
AWD-LSTM + Phrase Induction + finetuning,releaseDate,2019-06-04,,True
Perceiver IO,numParams,425000000.0,,True
Perceiver IO,releaseDate,2020-02-08,,True
Perceptron Mark I,flops,694894.9377361819,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
Perceptron Mark I,numParams,1000.0,,True
Perceptron Mark I,numTokens,6.0,Appendix II describes an experiment with 6 stimulus patterns,True
Perceptron Mark I,releaseDate,1957-01-01,,True
LDM-1.45B,numParams,1450000000.0,,True
LDM-1.45B,numTokens,400000000.0,400M image-text pairs,True
LDM-1.45B,gpuType,NVIDIA A100,,True
LDM-1.45B,releaseDate,2021-12-20,,True
MPT-30B,flops,1.8e+23,30b * 1T tokens * 6 = 1.8e23,True
MPT-30B,numParams,30000000000.0,,True
MPT-30B,numTokens,3000000000000.0,"~4T tokens across sources, or 3T words at 0.75 words/token (ignoring the fact that some of the data is code)",True
MPT-30B,batchSize,4096000.0,,True
MPT-30B,gpuType,"NVIDIA A100 SXM4 40 GB,NVIDIA H100 SXM5",,True
MPT-30B,releaseDate,2023-06-22,,True
MPT-30B,batchSize,4096000.0,"last two batch sizes were 3,456,000 and 4,096,000, but 4,096,000 only used for last 5% of training""To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long...The model was trained in three stages using the MosaicML Platform: (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens""",True
AlphaMissense,releaseDate,2023-09-22,,True
Error Propagation,releaseDate,1986-01-03,,True
RoBERTa Large,flops,4.15383552e+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining stepsfrom 100K to 300K, and then further to 500K"".So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision.C=5*1024*3.13E+13*60**2*24*0.3 = 4.2e21",True
RoBERTa Large,numParams,355000000.0,,True
RoBERTa Large,numTokens,32000000000.0,160GB*200M words/GB = 3.2e10 words,True
RoBERTa Large,trainingTimeDays,120.0,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",True
RoBERTa Large,gpuCount,1024.0,,True
RoBERTa Large,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
RoBERTa Large,releaseDate,2019-07-01,,True
OverFeat,releaseDate,2013-12-21,,True
CoEdiT-xxl,numParams,11000000000.0,,True
CoEdiT-xxl,numTokens,3000000.0,"82k pairs of sentences. Roughly 20 words per sentence based on examples but mean length could be higher due to outliers.40*82k = ~3,000,000",True
CoEdiT-xxl,gpuType,NVIDIA A100,,True
CoEdiT-xxl,releaseDate,2023-05-17,,True
CogVideo,numParams,9000000000.0,,True
CogVideo,releaseDate,2022-05-29,,True
Transformer-XL+WN+AdamP,numParams,257000000.0,,True
Transformer-XL+WN+AdamP,releaseDate,2020-06-15,,True
CodeGen-Mono 16.1B,numParams,16100000000.0,,True
CodeGen-Mono 16.1B,gpuType,Google TPU v4,,True
CodeGen-Mono 16.1B,releaseDate,2023-02-27,,True
$\infty$-former (SM),flops,1.2e+22,,True
$\infty$-former (SM),numParams,117000000.0,,True
$\infty$-former (SM),releaseDate,2021-09-01,,True
Grok-1,releaseDate,2023-11-04,,True
Chinchilla,flops,5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""We see the number of flops in table 3",True
Chinchilla,numParams,70000000000.0,,True
Chinchilla,numTokens,1050000000000.0,Table 1 shows Chinchilla was training on 1.4 trillion tokens1 token ~ 0.75 words,True
Chinchilla,batchSize,3000000.0,,True
Chinchilla,costDollars,753491.57852839,,True
Chinchilla,gpuType,"Google TPU v4,Google TPU v3",,True
Chinchilla,releaseDate,2022-03-29,,True
Chinchilla,batchSize,3000000.0,"Table 1. ""1.5M → 3M""",True
ALIGN,flops,2.598670000001e+22,From author communication14.82K TPUv3 core-daysPrecision: bfloat16EstimationTPUv3 at float16: 123 TFLOPS/chip123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOPhttps://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33,True
ALIGN,numParams,820000000.0,,True
ALIGN,numTokens,1600000000.0,"Dataset contains 1.8B image-text pairs, then some duplicates are removed.",True
ALIGN,costDollars,357760.32532239,,True
ALIGN,trainingTimeDays,347.3,14820 TPU core-hours / 1024 TPU cores = 347.3 hours,True
ALIGN,gpuCount,512.0,,True
ALIGN,gpuType,Google TPU v3,,True
ALIGN,releaseDate,2021-06-11,,True
Stable Code 3B,flops,2.106e+22,"6ND = 2.7e9 * 1.3e12 * 6 = 2,106E+22""stable-code-3b is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. """,True
Stable Code 3B,numParams,2796431360.0,,True
Stable Code 3B,gpuCount,256.0,,True
Stable Code 3B,gpuType,NVIDIA A100 SXM4 40 GB,,True
Stable Code 3B,releaseDate,2024-01-09,,True
Perceptron for Large Margin Classification,numTokens,60000.0,"""The dataset consists of 60,000 training examples and 10,000 test examples.""",True
Perceptron for Large Margin Classification,releaseDate,1999-12-01,,True
ASE,flops,6104160000000000000,"Training was done using the Isaac Gym simulator on an NVIDIA V100 GPU. The model was trained on over 10 billion samples, which equates to 10 years of simulated experience time. Training took around 10 days on a single GPU.14.13 TFLOP/s * 10 days * 86400 s/day * 0.50 utilization = 6.1e+18 FLOP",True
ASE,trainingTimeDays,240.0,Training took around 10 days on a single GPU.,True
ASE,gpuType,NVIDIA Tesla V100 PCIe 16 GB,,True
ASE,releaseDate,2022-05-05,,True
D-LSRC(200)+KN5,numParams,7160000.0,,True
D-LSRC(200)+KN5,releaseDate,2017-08-22,,True
Histograms of Oriented Gradients,numTokens,1805.0," we produced a new and significantly morechallenging data set, ‘INRIA’, containing 1805 64×128 im-ages",True
Histograms of Oriented Gradients,releaseDate,2005-06-25,,True
ESM1-670M (UR50/D),flops,480000000000000000000,"Information: 128 NVIDIA V100 GPUs [Pre-training details]906k steps [See Table S2: Hyperparameters]131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]Estimate:  906e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 4.8e20 FLOP",True
ESM1-670M (UR50/D),numParams,669200000.0,,True
ESM1-670M (UR50/D),gpuCount,128.0,,True
ESM1-670M (UR50/D),gpuType,NVIDIA V100,,True
ESM1-670M (UR50/D),releaseDate,2020-08-31,,True
Mistral 7B + OVM,numParams,7000000000.0,,True
Mistral 7B + OVM,releaseDate,2023-11-16,,True
"Variational (untied weights, MC) LSTM (Large)",flops,5620000000000000,,True
"Variational (untied weights, MC) LSTM (Large)",numParams,66000000.0,,True
"Variational (untied weights, MC) LSTM (Large)",releaseDate,2015-12-16,,True
Learning past tenses,numParams,211600.0,,True
Learning past tenses,releaseDate,1986-01-03,,True
Search-Proven Best LSTM,flops,3340000000000000,,True
Search-Proven Best LSTM,numParams,20000000.0,,True
Search-Proven Best LSTM,releaseDate,2015-07-06,,True
AraGPT2-Mega,flops,2e+21,"source: https://github.com/lightonai/akronomicon/blob/10adaca9c74afa7d11f196947e410d248f25abe9/akrodb/American%20University%20of%20Beirut/AraGPT2-Mega.jsonAkronomicon uses units of petaflop/s-days. 20 petaflop/s-days ~= 2e21 FLOP.Our own validation of this estimate is below.For the Mega model: 9 days on a TPUv3-128, bfloat16 precision  (from author communication)A TPUv3-128 has 128 cores (you can infer this from footnote 9 on p.4 of the paper - 128 * 16GB = 2TB). TPUv3 has 2 cores per chip. So 64 chips.TPUv3 FLOP/s: 1.23E+14Utilization: use default value of 30% for Language domain (https://epochai.org/blog/estimating-training-compute)64 chips * 30% * 1.23E+14 FLOP/s * 9 days * 24h/day * 3600s/h~= 2e21 FLOP",True
AraGPT2-Mega,numParams,1500000000.0,,True
AraGPT2-Mega,numTokens,8800000000.0,"""The total dataset size is 77GB with 8.8B words [word count was done after preprocessing, where a whitespace is inserted before and after punctuations, brackets, numbers... which increased the total word count]""",True
AraGPT2-Mega,costDollars,3685.43255453402,,True
AraGPT2-Mega,releaseDate,2020-12-31,,True
SPHINX (Llama 2 13B),numParams,13000000000.0,,True
SPHINX (Llama 2 13B),trainingTimeDays,290.0,"""The pre-training time is around 125 hours on 32 A100 GPUs with a 7Blanguage model and about twice the time with a 13B language model."""" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13Blanguage model.""",True
SPHINX (Llama 2 13B),gpuCount,32.0,,True
SPHINX (Llama 2 13B),gpuType,NVIDIA A100 SXM4 40 GB,,True
SPHINX (Llama 2 13B),releaseDate,2023-11-13,,True
GLaM,flops,3.74e+23,"from paper: ""GLaM (64B/64E) training after 600B tokens consumes 456 MWh, about 1/3 of the energy cost of 1287 MWh used by GPT-3. Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we train using 1,024 TPU-v4 chips for 574 hours (with 280B tokens). This consumes 213 MWh or 1/6 of the GPT-3 energy cost""600/280 is almost exactly 456/213 (2.14) so the later tokens have the same per-token energy cost. 2.14*574*1024 = 1,257,840 TPU-v4 hoursTPU-v4s are 275 teraFLOP/s. Using our usual 0.3 utilization assumption, 275 trillion * 1,257,840 * 3600 * 0.3 = 3.74e23Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.",True
GLaM,numParams,1200000000000.0,,True
GLaM,numTokens,800000000000.0,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.",True
GLaM,batchSize,1000000.0,,True
GLaM,trainingTimeDays,1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.""326W measured system power per TPU-v4 chip""""The complete GLaM training using 600B tokens consumes only456 MWh""1024 TPU v4 chips(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",True
GLaM,gpuCount,1024.0,,True
GLaM,gpuType,Google TPU v4,,True
GLaM,releaseDate,2021-12-13,,True
GLaM,batchSize,1000000.0,"""We use a maximum sequencelength of 1024 tokens, and pack each input example to haveup to 1 million tokens per batch.""",True
LSTM-3-layer+Gadam,flops,26800000000000000,,True
LSTM-3-layer+Gadam,numParams,24000000.0,,True
LSTM-3-layer+Gadam,releaseDate,2020-03-02,,True
Big Transfer (BiT-L),numParams,928000000.0,,True
Big Transfer (BiT-L),gpuType,Google TPU v3,,True
Big Transfer (BiT-L),releaseDate,2019-12-24,,True
SciBERT,flops,89268480000000000000,"4*123e12*0.3*(7*24*3600) = 8.926848e+19(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)We have: 4 TPUv3 chips.123teraFLOPs per chip.7 days of training""We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). """,True
SciBERT,numParams,110000000.0,,True
SciBERT,numTokens,2475000000.0,"assuming 0.75 words per token 3.3B*0.75 = 2475000000""The average paper length is154 sentences (2,769 tokens) resulting in a corpussize of 3.17B tokens, similar to the 3.3B tokenson which BERT was trained.""",True
SciBERT,trainingTimeDays,168.0,1 week,True
SciBERT,gpuCount,4.0,,True
SciBERT,gpuType,Google TPU v3,,True
SciBERT,releaseDate,2019-03-26,,True
CoAtNet,flops,4.27e+22,"20.1K TPU-v3 core-daysTPUs have two cores per chip, and a chip is 123 teraflop/s https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22",True
CoAtNet,numParams,2440000000.0,,True
CoAtNet,gpuType,Google TPU v3,,True
CoAtNet,releaseDate,2021-06-09,,True
AlphaGo Fan,flops,380000000000000000000,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 weekTraining compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs",True
AlphaGo Fan,numParams,8209984.0,,True
AlphaGo Fan,costDollars,3076.07370277232,,True
AlphaGo Fan,releaseDate,2015-10-01,,True
Megatron-LM (1T),numParams,1000000000000.0,,True
Megatron-LM (1T),gpuType,NVIDIA A100,,True
Megatron-LM (1T),releaseDate,2021-04-09,,True
ADP-FAIRSEQ,releaseDate,2022-10-26,,True
CTM (CIFAR-10),gpuType,NVIDIA V100,,True
CTM (CIFAR-10),releaseDate,2023-10-01,,True
BPL,releaseDate,2015-12-11,,True
Sparrow,numParams,70000000000.0,,True
Sparrow,releaseDate,2022-09-28,,True
ALM 1.0,releaseDate,2022-11-28,,True
VQGAN + CLIP,releaseDate,2020-12-17,,True
Phrase-based translation,numParams,9178890.0,,True
Phrase-based translation,numTokens,20000000.0,"[WORDS]""We used the freely available Europarl corpus to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.""""These results are consistentover training corpus sizes from 10,000 sentence pairs to320,000 sentence pairs. ""So 20 million words or 320k sentence pairs.",True
Phrase-based translation,releaseDate,2003-05-01,,True
EquiDock,flops,10800000000000000000,"Training details here:https://docs.nvidia.com/bionemo-framework/latest/models/equidock.html32 A100s can do 30 epochs per hour on the DIPS dataset. Equidock was trained on 30 epochs on DIPS and 150 epochs on DB5.5. DIPS is about 100x bigger, so the large majority of compute was DIPS.32 A100-hours = 312 teraflops * 32 * 3600 * 0.3 ~= 1.08e19",True
EquiDock,releaseDate,2021-11-15,,True
Dou Bao,releaseDate,2023-08-18,,True
Fisher Kernel GMM,numTokens,30000.0,"""Approximately 30K images were available for training and 5K for testing. Both sets were manually multi-labeled""",True
Fisher Kernel GMM,trainingTimeDays,2.5,"""With Fisher kernels, the trainingcost is reduced down to approximately 2h30.""",True
Fisher Kernel GMM,releaseDate,2017-01-01,,True
RNN+weight noise+dynamic eval,flops,4210000000000000,,True
RNN+weight noise+dynamic eval,numParams,54000000.0,,True
RNN+weight noise+dynamic eval,releaseDate,2013-08-04,,True
Phenaki,numParams,1800000000.0,,True
Phenaki,releaseDate,2022-10-05,,True
Parti,flops,3.962895376192635e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.Table 1 shows for the 20B model16 encoder layers64 decoder layersDmodel = 4096Dhidden = 16384Num heads = 64Just below table 1:""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.Section 3, Training: ""a totalof 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""",True
Parti,numParams,20000000000.0,,True
Parti,numTokens,4800000000.0,,True
Parti,costDollars,486659.76703549,,True
Parti,gpuType,Google TPU v4,,True
Parti,releaseDate,2022-06-22,,True
AWD-LSTM + MoS + Partial Shuffled,flops,328000000000000000,,True
AWD-LSTM + MoS + Partial Shuffled,numParams,35000000.0,,True
AWD-LSTM + MoS + Partial Shuffled,releaseDate,2019-06-10,,True
AbLang,numParams,355000000.0,,True
AbLang,releaseDate,2022-01-22,,True
D-LSRC(100)+KN5,releaseDate,2017-08-22,,True
DALL·E 3,releaseDate,2023-10-19,,True
Mogrifier RLSTM (WT2),flops,109000000000000000,,True
Mogrifier RLSTM (WT2),numParams,35000000.0,,True
Mogrifier RLSTM (WT2),releaseDate,2022-11-03,,True
Tensorized Transformer (small),releaseDate,2019-06-24,,True
BLOOM-176B,flops,3.6e+23,https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32384 A100 GPUs * 116 days = 3.6e23 at 30% utilization,True
BLOOM-176B,numParams,176247271424.0,,True
BLOOM-176B,numTokens,262500000000.0,350B words ~= 262B tokens,True
BLOOM-176B,batchSize,4194304.0,,True
BLOOM-176B,trainingTimeDays,2808.0,117 days * 24 hours/day,True
BLOOM-176B,gpuCount,384.0,,True
BLOOM-176B,gpuType,NVIDIA A100 SXM4 80 GB,,True
BLOOM-176B,releaseDate,2022-11-08,,True
BLOOM-176B,batchSize,4194304.0,Table 3. 2048*2048,True
Enhanced Neighborhood-Based Filtering,releaseDate,2007-10-28,,True
GPT-4,flops,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",True
GPT-4,numTokens,4900000000000.0,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",True
GPT-4,trainingTimeDays,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,True
GPT-4,gpuCount,25000.0,,True
GPT-4,gpuType,NVIDIA A100 SXM4 40 GB,,True
GPT-4,gpuUtilization,0.34,,True
GPT-4,releaseDate,2023-03-15,,True
base LM+GNN,releaseDate,2021-10-17,,True
DLRM-2020,flops,4000000000000000000,Figure 1https://arxiv.org/abs/2104.05158,True
DLRM-2020,numParams,100000000000.0,,True
DLRM-2020,costDollars,14.5989048237456,,True
DLRM-2020,releaseDate,2019-05-31,,True
TD(0),releaseDate,1977-08-01,,True
R-FCN,flops,61492939794000000,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)They used a Nvidia K40 GPU and report training time/image in seconds (table 3)Assumed a 0.33 util rate",True
R-FCN,numTokens,94427.0,,True
R-FCN,costDollars,5.50580435006535,,True
R-FCN,releaseDate,2016-06-21,,True
CogView,flops,2.68e+22,source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb,True
CogView,numParams,4000000000.0,,True
CogView,numTokens,50000000000.0,"""We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).""250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokensSo 30M text-image pairs and 50 billion words",True
CogView,costDollars,44452.3918799278,,True
CogView,gpuCount,512.0,,True
CogView,gpuType,NVIDIA Tesla V100 DGXS 16 GB,,True
CogView,releaseDate,2021-05-26,,True
CodeT5+,numParams,16000000000.0,,True
CodeT5+,gpuType,NVIDIA A100,,True
CodeT5+,releaseDate,2023-05-20,,True
TPM-LVD,numParams,1120000000.0,,True
TPM-LVD,releaseDate,2022-10-10,,True
Engin-Medium(NE),releaseDate,2021-12-11,,True
YOLOv3,flops,50939199920000000000,We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examplesAssuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf,True
YOLOv3,numParams,56933216.0,,True
YOLOv3,numTokens,1281167.0,Source: https://image-net.org/download.php,True
YOLOv3,costDollars,295.757786928036,,True
YOLOv3,gpuType,"NVIDIA M40,NVIDIA GTX Titan X",,True
YOLOv3,releaseDate,2018-04-08,,True
NLP from scratch,numParams,5000000.0,,True
NLP from scratch,numTokens,852000000.0,"""Section 4 leverages large unlabeled data sets (∼ 852 million words)""",True
NLP from scratch,releaseDate,2011-11-08,,True
Semi-Supervised Embedding for DL,releaseDate,2008-07-05,,True
Universal approximation via Feedforward Networks,releaseDate,1989-03-09,,True
Turing-NLG,flops,1.57e+22,source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb,True
Turing-NLG,numParams,17000000000.0,,True
Turing-NLG,numTokens,34800000000.0,"Authors say they pretrain on the same data as for Megatron-LM. From the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf""The resulting aggregatecorpus contains 174 GB of deduplicated text.""174GB * 2e8words/GB = 3.48e10 words",True
Turing-NLG,costDollars,58395.6192949826,,True
Turing-NLG,gpuCount,256.0,,True
Turing-NLG,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
Turing-NLG,releaseDate,2020-02-13,,True
Transformer ELMo,numParams,56000000.0,,True
Transformer ELMo,releaseDate,2019-01-01,,True
XVERSE-13B-2,numParams,13000000000.0,,True
XVERSE-13B-2,numTokens,2800000000000.0,"Multilingual, 3.2 trillion tokens. Likely majority Chinese and English, so I'll assume .87 words per token, or ~2.8 trillion words",True
XVERSE-13B-2,releaseDate,2023-11-06,,True
SemExp,releaseDate,2020-07-02,,True
RaSoR,numTokens,4000000.0,number of words in SQuAD,True
RaSoR,releaseDate,2020-12-23,,True
BART-large,numParams,406291456.0,,True
BART-large,releaseDate,2019-10-29,,True
EI-REHN-1200D,releaseDate,2017-08-14,,True
ViT-22B,flops,4.0001e+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a14 × 14 patch extracted from 224 × 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k:approximately 3 epochs""""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""256 * 177k * 65k = 3T tokens6 * 22B * 3T = 3.96e23 ~= 4e23also, MFU was high:""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward andbackward pass) on TPUv4 (Jouppi et al., 2020). ViT-22B’s model flops utilization (MFU) (Chowdhery et al.,2022; Dehghani et al., 2021a) is 54.9%, indicating a very efficient use of the hardware.""",True
ViT-22B,numParams,21743000000.0,,True
ViT-22B,numTokens,4000000000.0,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",True
ViT-22B,gpuType,Google TPU v4,,True
ViT-22B,releaseDate,2023-02-10,,True
Table-GPT,numParams,175000000000.0,,True
Table-GPT,releaseDate,2023-10-13,,True
Dropout-LSTM+Noise(Bernoulli) (PTB),releaseDate,2018-05-03,,True
DALL·E 2,numParams,3500000000.0,,True
DALL·E 2,numTokens,650000000.0,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability""",True
DALL·E 2,releaseDate,2022-04-06,,True
Densely Connected LSTM + Var. Dropout,flops,12800000000000000,,True
Densely Connected LSTM + Var. Dropout,numParams,23000000.0,,True
Densely Connected LSTM + Var. Dropout,releaseDate,2017-07-19,,True
A3C FF hs,releaseDate,2016-02-04,,True
eDiff-I,numParams,9100000000.0,,True
eDiff-I,numTokens,1000000000.0,"""The final dataset to train our model contains about one billion text-image pairs""",True
eDiff-I,gpuType,NVIDIA A100,,True
eDiff-I,releaseDate,2022-11-02,,True
Motion-Driven 3D Feature Tracking,numTokens,1500.0,"""The total number of possible input patterns was 65,536. Training sets of 650 and 1500 patterns picked at random from this total were used.""",True
Motion-Driven 3D Feature Tracking,releaseDate,1988-07-01,,True
FNetAR Medium,numParams,34300000.0,,True
FNetAR Medium,releaseDate,2021-07-22,,True
Fractional Max-Pooling,flops,100000000000000000,"For the 12M param model, training required ""18 hours on a GeForce GTX 780"". So would be somewhat larger for 27M.4 TFLOPS * 18 * 3600 * 0.4 = 1e17",True
Fractional Max-Pooling,numParams,27000000.0,,True
Fractional Max-Pooling,trainingTimeDays,18.0,,True
Fractional Max-Pooling,gpuType,NVIDIA GeForce GTX 780,,True
Fractional Max-Pooling,releaseDate,2014-12-18,,True
GPT-Neo-1.3B,releaseDate,2021-03-21,,True
RT-Trajectory,releaseDate,2023-11-03,,True
DBLSTM,numParams,29900000.0,,True
DBLSTM,releaseDate,2013-12-08,,True
DALL-E mini,flops,382579200000000000,flops = (4) * (1230 * 10**9) * (72 * 3600) * (0.3) = 3.8e17(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions,True
DALL-E mini,numTokens,30000000.0,3M+12M+15M = 30M from https://huggingface.co/dalle-mini/dalle-mini#training-data,True
DALL-E mini,trainingTimeDays,72.0,from https://huggingface.co/dalle-mini/dalle-mini#dall%C2%B7e-mini-estimated-emissions,True
DALL-E mini,gpuCount,4.0,,True
DALL-E mini,gpuType,Google TPU v3,,True
DALL-E mini,releaseDate,2021-10-26,,True
StarCoder,flops,1.12e+23,"""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23",True
StarCoder,numParams,15500000000.0,,True
StarCoder,batchSize,4194304.0,,True
StarCoder,trainingTimeDays,625.5,"625.5 hours = 320256 /512512 GPUs from ""We trained our model on a GPU cluster with 512 A100 80 GB GPUs ""320256 GPU hours from ""Based on the total number of GPU hours that training took (320,256)""citations from sections 5.6 and 5.7",True
StarCoder,gpuCount,512.0,,True
StarCoder,gpuType,NVIDIA A100 SXM4 80 GB,,True
StarCoder,releaseDate,2023-05-09,,True
StarCoder,batchSize,4194304.0,"""We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are observed""",True
SPIDER2,flops,8540000000000,"120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. Each input takes 17 residues.",True
SPIDER2,numParams,116112.0,,True
SPIDER2,releaseDate,2016-10-28,,True
ViT + DINO,flops,210000000000000000000,"""Overall, training DINO with Vision Transformersachieves 76.1 top-1 accuracy using two 8-GPU servers for 3days""GPU is V10016 * 125 teraflops * 3 days * 0.4 utilization= 2.1e20However, this isn't the best result in the paper (which is 80.1% with ViT-B/8). 76.1% is the result from ViT-B/16 per Table 2, which may be 5x cheaper than ViT-B/8 based on Table 1?",True
ViT + DINO,numParams,85000000.0,,True
ViT + DINO,gpuType,NVIDIA V100,,True
ViT + DINO,releaseDate,2021-04-29,,True
CamemBERT,flops,830000000000000000000,"""Unless otherwise specified,our models use the BASE architecture, and arepretrained for 100k backpropagation steps on 256Nvidia V100 GPUs (32GB each) for a day""256 V100-days256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)= 8.3e20""Following (Liu et al., 2019), weoptimize the model using Adam (Kingma and Ba,2014) (β1 = 0.9, β2 = 0.98) for 100k steps withlarge batch sizes of 8192 sequences, each sequencecontaining at most 512 tokens""Using compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20",True
CamemBERT,numParams,335000000.0,,True
CamemBERT,numTokens,24000000000.0," 31.9B tokens, Table 6. 24B words using 0.75 words/token",True
CamemBERT,trainingTimeDays,24.0,1 day for each model (may not have been a full 24 hours),True
CamemBERT,gpuType,NVIDIA V100,,True
CamemBERT,releaseDate,2019-11-10,,True
DCN+,numTokens,4000000.0,"from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairsdownload-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=downloadwc -w on train-v.1.1 returns 4017471 words so around 4M words",True
DCN+,releaseDate,2017-10-31,,True
Phi-1.5,flops,1.17e+21,"150B training tokens150B*1.3B*6 = 1.17e21also, took 1.5k GPU-hours with A100s, per Table 11500 * 312 trillion * 3600 * 0.3 = 5.05e20",True
Phi-1.5,numParams,1300000000.0,,True
Phi-1.5,numTokens,22500000000.0,"30B tokens, or ~22.5B words",True
Phi-1.5,trainingTimeDays,192.0,,True
Phi-1.5,gpuType,NVIDIA A100 SXM4 40 GB,,True
Phi-1.5,releaseDate,2023-09-11,,True
VGG19,numParams,144000000.0,,True
VGG19,numTokens,1300000.0,"""In this section, we present the image classification results achieved by the describedConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",True
VGG19,releaseDate,2014-09-04,,True
Diffractive Deep Neural Network,numParams,8000000000.0,,True
Diffractive Deep Neural Network,numTokens,55000.0,"size of MNIST""For this task, phase-only transmission masks were designed by training a 5-layer D2NN with ~55,000 images from MNIST handwritten digit database (14). """,True
Diffractive Deep Neural Network,releaseDate,2018-04-14,,True
YuYan 11B,numParams,11000000000.0,,True
YuYan 11B,gpuType,"NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti",,True
YuYan 11B,releaseDate,2022-07-15,,True
FAIRSEQ Adaptive Inputs,flops,7300000000000000000,,True
FAIRSEQ Adaptive Inputs,numParams,247000000.00000003,,True
FAIRSEQ Adaptive Inputs,releaseDate,2019-04-01,,True
Pragmatic Theory solution (Netflix 2009),numTokens,100480507.0,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",True
Pragmatic Theory solution (Netflix 2009),releaseDate,2009-08-01,,True
Diffusion-GAN,gpuType,NVIDIA V100,,True
Diffusion-GAN,releaseDate,2022-06-05,,True
Faster R-CNN,releaseDate,2015-06-04,,True
Dropout (MNIST),flops,6039370800000000,Num mul-add / forward pass2 FLOPs / mult-add3 total mult-add / fp mult-add3000 epochs60000 training samples,True
Dropout (MNIST),numParams,5592010.0,,True
Dropout (MNIST),numTokens,60000.0,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",True
Dropout (MNIST),costDollars,0.102126558145071,,True
Dropout (MNIST),gpuType,NVIDIA GeForce GTX 580,,True
Dropout (MNIST),releaseDate,2012-06-03,,True
Mixtral 8x7B,numParams,46700000000.0,,True
Mixtral 8x7B,releaseDate,2023-12-11,,True
Code Llama-34B,flops,5.3e+23,"1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute",True
Code Llama-34B,numParams,34000000000.0,,True
Code Llama-34B,batchSize,4000000.0,,True
Code Llama-34B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Code Llama-34B,releaseDate,2023-08-14,,True
Code Llama-34B,batchSize,4000000.0,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. ""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""Subsequent fine-tuning batch sizes are 500k-1M. ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",True
Mnemonic Reader,numTokens,4000000.0,size of SQuAD,True
Mnemonic Reader,releaseDate,2017-05-08,,True
ESM1-670M (UR50/S),flops,440000000000000000000,"Information: 128 NVIDIA V100 GPUs [Pre-training details]840k steps [See Table S2: Hyperparameters]131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 669.2e6 parameters = 4.4e20 FLOP",True
ESM1-670M (UR50/S),numParams,669200000.0,,True
ESM1-670M (UR50/S),gpuCount,128.0,,True
ESM1-670M (UR50/S),gpuType,NVIDIA V100,,True
ESM1-670M (UR50/S),releaseDate,2020-08-31,,True
Flan-T5 11B,numParams,11000000000.0,,True
Flan-T5 11B,gpuType,Google TPU v4,,True
Flan-T5 11B,releaseDate,2022-10-20,,True
Inception v3,numParams,23626728.0,,True
Inception v3,numTokens,1200000.0,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",True
Inception v3,releaseDate,2015-12-02,,True
Automated WSD via WordNet,numTokens,5000.0,"They do two experiments, one on a dataset of 5.000 tagged words andanother one on two datasets containing a total of around 40 million words, of which they only select 38 unique words and manually annotate the senses?I think the first one is more representative",True
Automated WSD via WordNet,releaseDate,2004-07-01,,True
Decision tree (classification),flops,63000000000000,"The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs",True
Decision tree (classification),numParams,120000000.0,,True
Decision tree (classification),numTokens,14460.0,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,True
Decision tree (classification),releaseDate,2001-12-08,,True
Solar-10.7B,numParams,10700000000.0,,True
Solar-10.7B,releaseDate,2023-12-23,,True
REINFORCE in Stochastic Connectionism,releaseDate,1992-05-01,,True
JIANG,flops,4.03e+22,"""The training was conducted using 96 A100 80G GPUs, and the entire process took approximately 52 days.""312 teraflop/s * 96 * 52 * 24 * 3600 * 0.3 = 4e22",True
JIANG,numTokens,467000000000.0,"467B tokens (inferred from Table 1).It's a mix of Chinese and English text, I'll use our standard 1:1 token:words ratio for Chinese.",True
JIANG,batchSize,6000000.0,,True
JIANG,trainingTimeDays,1200.0,52 days,True
JIANG,gpuType,NVIDIA A100 SXM4 80 GB,,True
JIANG,releaseDate,2023-08-01,,True
JIANG,batchSize,6000000.0,"""During the training process, we employed a large batch size of 6 million tokens to enhance the model’s stability""",True
EN^2AS with performance reward,numParams,23000000.0,,True
EN^2AS with performance reward,releaseDate,2019-07-22,,True
Jiutian,releaseDate,2023-10-12,,True
Empirical evaluation of deep architectures,releaseDate,2007-06-01,,True
Diabetic Retinopathy Detection Net,releaseDate,2016-12-13,,True
Deep Deterministic Policy Gradients,releaseDate,2015-09-09,,True
BellKor 2008,numTokens,100480507.0,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",True
BellKor 2008,releaseDate,2009-08-01,,True
ERNIE 3.0 Titan,flops,1.0421e+24,The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP,True
ERNIE 3.0 Titan,numParams,260000000000.0,,True
ERNIE 3.0 Titan,numTokens,668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""Assuming 167M words per GB",True
ERNIE 3.0 Titan,batchSize,1048576.0,,True
ERNIE 3.0 Titan,gpuCount,1920.0,,True
ERNIE 3.0 Titan,gpuType,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB",,True
ERNIE 3.0 Titan,releaseDate,2021-12-23,,True
ERNIE 3.0 Titan,batchSize,1048576.0,"""The maximum sequence length of context andthe memory length of language generation is 512 and 128, respectively""In table 1, they use a global batch size of 512 when data parallelism is ""1"" and 2048 when DP is ""4"". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.2048 * 512 = 1048576.",True
KN5 LM + RNN 400/10 (WSJ),flops,61440000000000000,"""Convergence is usuallyachieved after 10-20 epochs.""Assuming a backward-forward ratio of 2:1, since this is a shallow network",True
KN5 LM + RNN 400/10 (WSJ),numParams,80000000.0,,True
KN5 LM + RNN 400/10 (WSJ),numTokens,6400000.0,"The training corpus consists of 37M words from NYT section of English Gigaword. As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models (300K sentences) - it takes several weeks to train the most complex models",True
KN5 LM + RNN 400/10 (WSJ),costDollars,2.0283479276474,,True
KN5 LM + RNN 400/10 (WSJ),releaseDate,2010-09-26,,True
IBM Model 4,numTokens,800000.0,[WORDS]See FIgure 6,True
IBM Model 4,releaseDate,1999-07-02,,True
ESM1v,flops,135000000000000070000,"""ESM-1v models are pre-trained for 6 days on 64 V100 GPUs"" [F - Compute costs]Assuming  V100 PCIe single precision 14 TFLOPS and 0.3 utilization rate Estimate: (6*24*3600) s * 14e12 FLOP/s * 0.3 *64 = 1.4e20 FLOPsAlternative estimate based on Figure 7: 10^(7.5) GPU-seconds * 14e12 FLOP/s * 0.3 = 1.3e20 FLOPsMean: 1.35e20 FLOP",True
ESM1v,numParams,650000000.0,,True
ESM1v,numTokens,98000000.0,"""We train ESM-1v, a 650M parameter transformer language model for prediction of variant effects, on 98 million diverse protein sequences across evolution""",True
ESM1v,gpuCount,64.0,,True
ESM1v,gpuType,NVIDIA V100,,True
ESM1v,releaseDate,2021-11-17,,True
MetNet,releaseDate,2020-03-24,,True
Imagen Video,numParams,11600000000.0,,True
Imagen Video,releaseDate,2022-10-05,,True
top-down frozen classifier,releaseDate,2021-02-09,,True
FLM-101B,flops,5.72e+22,"192 GPUs * 160 TFLOP/s per GPU (reported, adjusted for utilization) * 21.54 days * 24 * 3600 = 5.72e22",True
FLM-101B,numParams,101000000000.0,,True
FLM-101B,numTokens,350000000000.0,"Trained with 311.54B tokens. The dataset is approximately 50/50 English/Chinese: ""It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5% for language modeling"". We assume 1 Chinese word per token and 0.75 English words per token (0.875 on average). 311B/0.875 ~= 350B.",True
FLM-101B,batchSize,4310000.0,,True
FLM-101B,costDollars,84000.0,Authors report $100k. Adjusted for inflation.,True
FLM-101B,trainingTimeDays,517.0,"""Under this growth schedule, the total time cost for our 101B model is 21.54 days""",True
FLM-101B,gpuType,NVIDIA A800,,True
FLM-101B,releaseDate,2023-09-07,,True
FLM-101B,batchSize,4310000.0,Table 1,True
DistilBERT,flops,12441600000000000000,Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.1.6e13*8*60**2*90*0.3 = 1.2e19,True
DistilBERT,numParams,66000000.0,,True
DistilBERT,releaseDate,2019-10-02,,True
6-layer MLP (MNIST),flops,130788000000000,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""I assume that the number of passes per epoch is 60k, the training set size.",True
6-layer MLP (MNIST),numParams,12110000.0,,True
6-layer MLP (MNIST),numTokens,60000.0,"""MNIST consists of two datasets, one for training (60,000 images) and one for testing (10,000 images). Many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole un-deformed training set for validation, without wasting training images""",True
6-layer MLP (MNIST),costDollars,0.0073813872645649,,True
6-layer MLP (MNIST),releaseDate,2010-03-01,,True
NeMO Megatron GPT 20B,numParams,20000000000.0,,True
NeMO Megatron GPT 20B,releaseDate,2022-09-15,,True
ERNIE-Doc (151M),releaseDate,2020-12-31,,True
Transformer-XL Large + Phrase Induction,flops,7300000000000000000,,True
Transformer-XL Large + Phrase Induction,numParams,257000000.0,,True
Transformer-XL Large + Phrase Induction,releaseDate,2019-06-04,,True
LSTM,flops,21008000000000,"""Due to limited computation time, training is stopped after 5 million sequence presentations""Each sequence has p=100 elements in the long-delay setting.COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN",True
LSTM,numParams,10504.0,,True
LSTM,numTokens,1273000.0,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stoppingcriterion.This applies to experiment 5 (multiplication)Sequences have random lengths, on the order of 100-1000 (table 7 )",True
LSTM,releaseDate,1997-11-15,,True
RNNLM + Dynamic KL Regularization (WT2),flops,21900000000000000,,True
RNNLM + Dynamic KL Regularization (WT2),numParams,87600000.0,,True
RNNLM + Dynamic KL Regularization (WT2),releaseDate,2018-01-01,,True
SCRN(Structurally Constrained Recurrent Network),numParams,26500000.0,,True
SCRN(Structurally Constrained Recurrent Network),releaseDate,2014-12-24,,True
mT5-XXL,flops,7.8e+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""1 trillion tokens * 13 billion params * 6 = 7.8e22",True
mT5-XXL,numParams,13000000000.0,,True
mT5-XXL,numTokens,750000000000.0,"""totaling 6.6B pages and 6.3T tokens""It's multilingual so we don't have a standard word:token ratio, but using the 0.75 for English that's ~5 trillion.Distribution by language is in Appendix A.The model was trained for 0.159 equivalent epochs of the full dataset, or 1 epoch on a subset of 1 trillion tokens.",True
mT5-XXL,batchSize,1048576.0,,True
mT5-XXL,releaseDate,2020-10-20,,True
mT5-XXL,batchSize,1048576.0,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""",True
Nucleotide Transformer,flops,1.2e+22,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""Assuming 78 TFLOP / s for 32-bit calculations and 0.5 utilization rateEstimate: 78e12 FLOP/s * 128 GPUs * 28 days * 86400 seconds * 0.5 utilization rate = 1.2e22",True
Nucleotide Transformer,numParams,2500000000.0,,True
Nucleotide Transformer,trainingTimeDays,672.0,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""",True
Nucleotide Transformer,gpuCount,128.0,,True
Nucleotide Transformer,gpuType,NVIDIA A100,,True
Nucleotide Transformer,releaseDate,2023-01-15,,True
DImensionality Reduction,numParams,3800000.0,,True
DImensionality Reduction,numTokens,70000.0,"After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA(Fig. 2B)",True
DImensionality Reduction,releaseDate,2006-07-18,,True
SqueezeBERT,numParams,51100000.0,,True
SqueezeBERT,releaseDate,2020-06-10,,True
Big Transformer for Back-Translation,flops,108084326400000000000,"(128) * (28.26 * 10**12) * (27*3600 + 40*60) * (0.3)  = 108084326400000000000(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  ""We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are intercon-nected by Infiniband. Experiments are run on 16machines and we perform 30K synchronous up-dates. """"We train models with 16-bit floating pointoperations""from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957 V100 have 28.26 TFLOPSin section 5.6 we have""train this system we perform 300K training up-dates in 27h 40min on 128 GPUs;""",True
Big Transformer for Back-Translation,numTokens,3390000000.0,"""Finally, for WMT English-German we train on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs.""We assume that 1 sentence have 15 words",True
Big Transformer for Back-Translation,trainingTimeDays,27.666,"""training updates in 27h 40min on 128 GPUs""",True
Big Transformer for Back-Translation,gpuCount,128.0,,True
Big Transformer for Back-Translation,gpuType,NVIDIA V100,,True
Big Transformer for Back-Translation,releaseDate,2018-08-28,,True
Hopfield Networks (2020),releaseDate,2020-07-16,,True
Mamba-2.8B,flops,5.400000000000001e+21,"""Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models.""3b * 300b * 6 = 5.4e21Note: this is a new architecture so not sure how well 6*params*data works as a heuristicFigure 4 shows perplexity curves where Mamba is trained up to 2e20 FLOP, but those are for the 125M and 1.3B variants.",True
Mamba-2.8B,numParams,2800000000.0,,True
Mamba-2.8B,releaseDate,2023-12-01,,True
data2vec (speech),numParams,705134592.0,,True
data2vec (speech),numTokens,13132800.0,"Section 5.2:""we pre-train data2vec on the 960hours of speech audio data from Librispeech (LS-960)""13,680 words per hour",True
data2vec (speech),releaseDate,2022-01-20,,True
SeqVec,flops,41000000000000000000,"3 weeks, 5 NVIDIA Titan GPUs (Assuming NVIDIA Titan V and 30% utilization rate for calculation) with 12 GB memory, ",True
SeqVec,numParams,93000000.0,,True
SeqVec,trainingTimeDays,508.0,,True
SeqVec,gpuCount,5.0,,True
SeqVec,gpuType,NVIDIA Titan V,,True
SeqVec,releaseDate,2019-12-17,,True
Visualizing CNNs,flops,532000000000000000,1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization = 532 PF = 0.0062 pfs-daysSource: https://openai.com/blog/ai-and-compute,True
Visualizing CNNs,costDollars,9.02117930281462,,True
Visualizing CNNs,gpuType,NVIDIA GeForce GTX 580,,True
Visualizing CNNs,releaseDate,2013-11-12,,True
UnifiedQA,numParams,11000000000.0,,True
UnifiedQA,trainingTimeDays,36.0,"""pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""",True
UnifiedQA,gpuType,Google TPU v3,,True
UnifiedQA,releaseDate,2020-05-02,,True
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",numParams,35000000.0,,True
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",releaseDate,2018-09-18,,True
LSTM-300units,numParams,12000000.0,,True
LSTM-300units,releaseDate,2012-09-01,,True
NMT Transformer 437M,numParams,437700000.0,,True
NMT Transformer 437M,releaseDate,2019-02-28,,True
Tensorized Transformer (large PTB),releaseDate,2019-06-24,,True
RWKV-4 14B,flops,2.78e+22,"from HuggingFace page: https://huggingface.co/BlinkDL/rwkv-4-pile-14btrained for 331B tokens14 billion * 331 billion * 6 = 2.78e22paper notes that a forward pass is almost exactly 2x parameters (within 2%): ""Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant."" and that 6*params*tokens is a good approximation because it's not a transformer: ""FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D2L), which is thetwice (add and multiply) the number of parametersin linear layers. The backwards pass FLOPs can beapproximated as twice that of the forward pass. Sothe total is 6(V D + 13D2L) per token for training(3x fw FLOPs). It is noteworthy that FLOPs areindependent of the context length, unlike regulartransformers""",True
RWKV-4 14B,numParams,14000000000.0,,True
RWKV-4 14B,batchSize,262144.0,,True
RWKV-4 14B,gpuType,NVIDIA A100 SXM4 80 GB,,True
RWKV-4 14B,releaseDate,2023-05-22,,True
RWKV-4 14B,batchSize,262144.0,"262144 (or 131072?)""To train the models mentioned, we... switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens""",True
EfficientZero,releaseDate,2021-10-30,,True
Jais,flops,3.08e+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,True
Jais,numParams,13000000000.0,,True
Jais,numTokens,300000000000.0,395B tokens ~= 300B words,True
Jais,batchSize,3932160.0,,True
Jais,trainingTimeDays,600.0,2023 June 25 to July 18 = 25 days = 600 hours,True
Jais,releaseDate,2023-08-29,,True
Jais,batchSize,3932160.0,"""After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. """,True
AudioGen,flops,7.2e+21,"""the large model was trained on 128 A100 GPUs for 200k steps (∼1 week)""A100s are 312 teraflop/s128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21",True
AudioGen,numParams,1000000000.0,,True
AudioGen,trainingTimeDays,168.0,1 week,True
AudioGen,gpuType,NVIDIA A100,,True
AudioGen,releaseDate,2023-03-05,,True
Tagging via Viterbi Decoding,releaseDate,2002-06-01,,True
Deep Belief Nets,numParams,1600000.0,,True
Deep Belief Nets,numTokens,60000.0,"""The network that performed best on the validation set wasthen tested and had an error rate of 1.39%. This network wasthen trained on all 60,000 training images8 until its error-rateon the full training set was as low as its final error-rate hadbeen on the initial training set of 44,000 images.""",True
Deep Belief Nets,releaseDate,2006-07-18,,True
MnasNet-A1 + SSDLite,flops,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPUAssuming a 33% utilization rate:4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOPHowever, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",True
MnasNet-A1 + SSDLite,numParams,4900000.0,,True
MnasNet-A1 + SSDLite,numTokens,118000.0,,True
MnasNet-A1 + SSDLite,costDollars,4330.99721777198,,True
MnasNet-A1 + SSDLite,trainingTimeDays,108.0,,True
MnasNet-A1 + SSDLite,gpuCount,256.0,,True
MnasNet-A1 + SSDLite,gpuType,Google TPU v3,,True
MnasNet-A1 + SSDLite,releaseDate,2019-05-29,,True
LSTM with forget gates,numParams,276.0,,True
LSTM with forget gates,numTokens,30000.0,"Training was stopped after at most 30000training streams, each of which was endedwhen the first prediction error or the100000th successive input symbol occurredNOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)",True
LSTM with forget gates,releaseDate,1999-01-02,,True
Q-learning,releaseDate,1989-01-01,,True
ProtBERT-BFD,flops,3.9e+22,"""FLOP = 420M*2*(800k*512*32k+200k*2048*6k) +  420M*4*(800k*512*32k+200k*2048*6k), 1M steps total split into two phases, (1) 800k steps, seq length 512 (bath size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd""",True
ProtBERT-BFD,numParams,420000000.0,,True
ProtBERT-BFD,gpuCount,1024.0,,True
ProtBERT-BFD,gpuType,Google TPU v3,,True
ProtBERT-BFD,releaseDate,2021-05-04,,True
VRNS-RNN-3-3-5,numParams,1500000.0,,True
VRNS-RNN-3-3-5,releaseDate,2022-10-04,,True
ERNIE 3.5,releaseDate,2023-06-27,,True
Mesh-TensorFlow Transformer 2.9B (translation),flops,68428800000000000000,"flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 45TFLOPs per chips",True
Mesh-TensorFlow Transformer 2.9B (translation),numParams,2900000000.0,,True
Mesh-TensorFlow Transformer 2.9B (translation),trainingTimeDays,22.0,"from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",True
Mesh-TensorFlow Transformer 2.9B (translation),gpuCount,64.0,,True
Mesh-TensorFlow Transformer 2.9B (translation),gpuType,Google TPU v2,,True
Mesh-TensorFlow Transformer 2.9B (translation),releaseDate,2018-11-05,,True
Decay RNN,numParams,1400000.0,,True
Decay RNN,releaseDate,2020-05-17,,True
Self-Attention and Convolutional Layers,flops,675000000000000000,(15e9) * (300) * (50000) * 3 = 675000000000000000(inference compute) * (epochs) * (dataset size) * (constant to account for backpropagation) epochs from appendix B table 2inference compute from table 1,True
Self-Attention and Convolutional Layers,numParams,29500000.0,,True
Self-Attention and Convolutional Layers,numTokens,50000.0,size of CIFAR-10,True
Self-Attention and Convolutional Layers,releaseDate,2019-11-08,,True
4-gram + 8 DENN,numParams,16100000.000000002,,True
4-gram + 8 DENN,releaseDate,2014-12-22,,True
RNN 500/10 + RT09 LM (NIST RT05),flops,3414636000000000,"""Convergence is usually achieved after 10-20 epochs.""Assuming a backward-forward ratio of 2:1, since this is a shallow network",True
RNN 500/10 + RT09 LM (NIST RT05),numParams,5269500.0,,True
RNN 500/10 + RT09 LM (NIST RT05),numTokens,5400000.0,"""Table 4: Comparison of very large back-off LMs and RNN LMstrained only on limited in-domain data (5.4M words).""",True
RNN 500/10 + RT09 LM (NIST RT05),costDollars,0.113055458262314,,True
RNN 500/10 + RT09 LM (NIST RT05),releaseDate,2010-09-26,,True
Wu Dao - Wen Su,releaseDate,2021-03-01,,True
Gen-1,releaseDate,2023-02-06,,True
T2R 75% + Pretrain,releaseDate,2021-03-24,,True
TrellisNet,flops,2780000000000000000,,True
TrellisNet,numParams,180000000.0,,True
TrellisNet,releaseDate,2018-10-15,,True
Pythia-410m,numParams,410000000.0,,True
Pythia-410m,releaseDate,2023-04-03,,True
Residual Dense Network,releaseDate,2018-02-24,,True
LSTM (2018),numParams,13000000.0,,True
LSTM (2018),releaseDate,2018-03-04,,True
Ngram corpus,releaseDate,2012-07-08,,True
Conformer + Wav2vec 2.0 + Noisy Student,flops,7.6e+21,"""We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...We fine-tune the pre-trained checkpoints (400k steps) with global batchsize 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models""TPU v3 chips are 123 teraflop/s. 2 chips per core512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21",True
Conformer + Wav2vec 2.0 + Noisy Student,numParams,1000000000.0,,True
Conformer + Wav2vec 2.0 + Noisy Student,trainingTimeDays,168.0,7 days,True
Conformer + Wav2vec 2.0 + Noisy Student,gpuCount,256.0,,True
Conformer + Wav2vec 2.0 + Noisy Student,gpuType,Google TPU v3,,True
Conformer + Wav2vec 2.0 + Noisy Student,releaseDate,2020-10-20,,True
GoogLeNet / InceptionV1,flops,1557140125176000000,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
GoogLeNet / InceptionV1,numParams,6797700.0,,True
GoogLeNet / InceptionV1,numTokens,1200000.0,"""The ILSVRC 2014 classification challenge involves thetask of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing[...]We participated in the challenge with no external dataused for training.""",True
GoogLeNet / InceptionV1,costDollars,14.1646781477527,,True
GoogLeNet / InceptionV1,releaseDate,2015-06-07,,True
PreTrans-3L-250H,numParams,43000000.0,,True
PreTrans-3L-250H,releaseDate,2013-03-22,,True
Claude 1.3,releaseDate,2023-04-18,,True
Adaptive LSTM + DeFINE,releaseDate,2019-11-27,,True
Learnability theory of language development,releaseDate,1984-07-01,,True
ONE-PEACE,flops,180000000000000000000,4 billion params * 7.5 billion data * 6 = 1.8e20.see training dataset size notes. this estimate required some more assumptions than usual.,True
ONE-PEACE,numParams,4000000000.0,,True
ONE-PEACE,numTokens,1600000000.0,"""After these steps, we retain about 1.5 billion image-text pairs""...""We also perform simple cleaning on the data, which involves removing samples with text lengths less than 3 or greater than512, as well as texts containing non-English or emoji characters. Ultimately, we obtain about 2.4 million audio-text pairs, with a total duration of around 8,000 hours""8000 hours = 480,000 minutes = ~109,440,000 words at 228 wpmhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pqTrained on 10 epochs for audio. For text, they train on ""200K steps with a batch size of 32768"" = 6,533,600,000Adding together, they train on ~ 7.5b data points on a dataset of 1.6b, for ~4.7 epochs on average.",True
ONE-PEACE,releaseDate,2023-05-18,,True
MemSizer,flops,7300000000000000000,,True
MemSizer,numParams,357000000.0,,True
MemSizer,releaseDate,2022-03-23,,True
ADALINE,flops,9900,"""The method of searching that has proven most useful is the method of steepest descent""Apparently each pattern was only shown once to the system.So the training compute is (forward pass compute) * (3 for backprop) * dataset size",True
ADALINE,numParams,17.0,,True
ADALINE,numTokens,100.0,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",True
ADALINE,releaseDate,1960-06-30,,True
DEQ-Transformer (Post-LN) + Jacobian Regularisation,flops,29000000000000000000,,True
DEQ-Transformer (Post-LN) + Jacobian Regularisation,numParams,98000000.0,,True
DEQ-Transformer (Post-LN) + Jacobian Regularisation,releaseDate,2021-06-28,,True
DQN-2015,numParams,1693362.0,,True
DQN-2015,numTokens,50000000.0,"Methods: ""we trained for a total of 50 million frames""",True
DQN-2015,releaseDate,2015-02-25,,True
EfficientNetV2,flops,95600000000000000000,"Table 7, page 7: 45 hours on 32 TPUv3 cores.""Each v3 TPU chip contains two TensorCores.""TPU performance per chip = 123e12 FLOP/s32 cores = 16 chips123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOPhttps://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3",True
EfficientNetV2,numParams,208000000.0,,True
EfficientNetV2,numTokens,14197122.0,,True
EfficientNetV2,trainingTimeDays,45.0,Table 7,True
EfficientNetV2,gpuType,Google TPU v3,,True
EfficientNetV2,releaseDate,2021-06-23,,True
Inceptionv4,numParams,43000000.0,,True
Inceptionv4,releaseDate,2016-02-23,,True
PPLX-70B-Online,numParams,70000000000.0,,True
PPLX-70B-Online,releaseDate,2023-11-29,,True
MegaMolBART,flops,720000000000000000000,"""MegaMolBART was trained with data parallelism on 64 V100 32 GB GPUs (4 nodes x 16 GPUs) for 8 epochs (approximately 160k iterations or ~80 wall-clock hours) using a batch size of 32 molecules per GPU (micro batch)""https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html64 * 130 teraflops * 80 * 3600 * 0.3 = 7.2e20",True
MegaMolBART,numParams,45000000.0,,True
MegaMolBART,trainingTimeDays,80.0,,True
MegaMolBART,gpuType,NVIDIA V100,,True
MegaMolBART,releaseDate,2021-09-14,,True
Tensorized Transformer (257M),flops,4760000000000000000,,True
Tensorized Transformer (257M),numParams,257000000.0,,True
Tensorized Transformer (257M),releaseDate,2019-06-24,,True
LLaVA-NeXT-34B (LLaVA-1.6),flops,258785280000000000000,"2.6e20 = 32 * 312e12 * 0.3 * 24* 3600 = num gpus * peak flops * assumed utilization rate * time in seconds""The largest 34B variant finishes training in ~1 day with 32 A100s.""",True
LLaVA-NeXT-34B (LLaVA-1.6),numParams,34750000000.0,,True
LLaVA-NeXT-34B (LLaVA-1.6),trainingTimeDays,24.0,"""The largest 34B variant finishes training in ~1 day with 32 A100s.""",True
LLaVA-NeXT-34B (LLaVA-1.6),gpuCount,32.0,,True
LLaVA-NeXT-34B (LLaVA-1.6),gpuType,NVIDIA A100,,True
LLaVA-NeXT-34B (LLaVA-1.6),releaseDate,2024-01-30,,True
NoisyNet-Dueling,releaseDate,2017-06-30,,True
Dropout (TIMIT),numParams,48840185.0,,True
Dropout (TIMIT),numTokens,41620.0,"4162 utterances, guesstimated avg 10 words per utterance",True
Dropout (TIMIT),gpuType,NVIDIA GeForce GTX 580,,True
Dropout (TIMIT),releaseDate,2012-06-03,,True
BIDAF,flops,3468614400000000000,"flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =citation from the section about cloze test experiments ""The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4"" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).flops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632",True
BIDAF,numParams,2600000.0,,True
BIDAF,numTokens,47160000.0,"""In a cloze test, the reader is asked to fill in words that have been removed from a passage,for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-sive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)examples from CNN and DailyMail news articles, respectively. ""assuming 40 words per example we get around 47160000 words (SQuAD have around 40 words per example - so I think it should be similar case for this dataset)",True
BIDAF,trainingTimeDays,60.0,see compute notes,True
BIDAF,gpuCount,8.0,,True
BIDAF,gpuType,NVIDIA GTX Titan X,,True
BIDAF,releaseDate,2016-11-05,,True
Persia,numParams,100000000000000.0,,True
Persia,gpuType,NVIDIA V100,,True
Persia,releaseDate,2021-11-23,,True
Transformer + GFM,flops,8039999999999999000,,True
Transformer + GFM,numParams,185000000.0,,True
Transformer + GFM,releaseDate,2022-12-01,,True
VALL-E X,numParams,700000000.0,,True
VALL-E X,numTokens,910000000.0,"70k hours, mix of Chinese and English but mostly English70k * 13k words/hour = 910,000,000 words",True
VALL-E X,gpuType,NVIDIA V100,,True
VALL-E X,releaseDate,2023-03-07,,True
TRIMELMlong (150M),numParams,150000000.0,,True
TRIMELMlong (150M),releaseDate,2022-05-25,,True
GloVe (32B),numParams,120000000.0,,True
GloVe (32B),numTokens,42000000000.0,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",True
GloVe (32B),releaseDate,2014-01-01,,True
StarCoder 2 15B,flops,3.87e+23,estimation is given in Table 6 ,True
StarCoder 2 15B,numParams,7000000000.0,,True
StarCoder 2 15B,numTokens,4100000000000.0,"from Table 7, 4.1T tokens ",True
StarCoder 2 15B,releaseDate,2024-02-29,,True
EGRU (WT2),flops,2310000000000000000,,True
EGRU (WT2),numParams,74000000.0,,True
EGRU (WT2),releaseDate,2022-06-13,,True
Prototypical networks,releaseDate,2017-03-15,,True
YOLOv2,numParams,51000000.0,,True
YOLOv2,releaseDate,2016-12-25,,True
TD-Gammon,flops,18232157622832.703,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
TD-Gammon,numParams,25000.0,,True
TD-Gammon,numTokens,6300000.0,"""This network was trainedfor over 300,000 training games""Each backgammon game has an avg of around 21 movementshttps://www.bkgm.com/rgb/rgb.cgi?view+712",True
TD-Gammon,releaseDate,1992-05-01,,True
AdClickNet,releaseDate,2014-08-24,,True
Inflection-2,flops,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""(the second 1 is there because of airtable being wonky, it's not a real sig fig)",True
Inflection-2,gpuCount,5000.0,,True
Inflection-2,gpuType,NVIDIA H100 SXM5,,True
Inflection-2,releaseDate,2023-11-22,,True
Alleviated TOI 10 (WT103),releaseDate,2019-09-18,,True
BLOOM-7.1B,flops,1.48e+22,,True
BLOOM-7.1B,numParams,7070000000.0,,True
BLOOM-7.1B,releaseDate,2022-07-05,,True
NetTalk (transcription),flops,28328002560,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,True
NetTalk (transcription),numParams,18629.0,,True
NetTalk (transcription),numTokens,1024.0,"We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",True
NetTalk (transcription),releaseDate,1987-06-06,,True
"Listen, Attend and Spell",releaseDate,2015-08-20,,True
Calm2-7B,flops,5.46e+22,6*7B*1.3T = 546000000000000000000006ND aproximation,True
Calm2-7B,numParams,7000000000.0,,True
Calm2-7B,numTokens,1300000000000.0,"""1.3T tokens of publicly available Japanese and English datasets. "" so around 1.3T words assuming 1 word per token.",True
Calm2-7B,releaseDate,2023-11-01,,True
MMS-1B,numParams,1000000000.0,,True
MMS-1B,gpuCount,64.0,,True
MMS-1B,gpuType,NVIDIA A100 SXM4 80 GB,,True
MMS-1B,releaseDate,2023-05-22,,True
Codex,numParams,12000000000.0,,True
Codex,numTokens,31800000000.0,"""Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average linelength greater than 100, had maximum line length greaterthan 1000, or contained a small percentage of alphanumericcharacters. After filtering, our final dataset totaled 159 GB.""1 GB ~ 200M words",True
Codex,releaseDate,2021-07-07,,True
Memformer (4 encoder + 16 decoder),flops,12000000000000000000,,True
Memformer (4 encoder + 16 decoder),numParams,76200000.0,,True
Memformer (4 encoder + 16 decoder),releaseDate,2020-10-14,,True
gLM,flops,151000000000000000000,"""The training stage takes several weeks on four NVIDIA A100 GPUs.""Assumption: 2 weeks, 40% utilization rate, 78 TFLOP peak rateEstimate: =(2*7*24*3600) s * 78e12 FLOP/s *4 GPU * 0.4",True
gLM,numParams,1000000000.0,,True
gLM,gpuType,NVIDIA A100,,True
gLM,releaseDate,2023-04-08,,True
GPT-J-6B,flops,1.5e+22,source: zero shot evaluation table in GitHub,True
GPT-J-6B,numParams,6053381344.0,,True
GPT-J-6B,numTokens,160000000000.0,"""The model was trained on 400B tokens from The Pile dataset with 800GB text.""1 GB ~ 200M words",True
GPT-J-6B,costDollars,25176.8016041484,,True
GPT-J-6B,releaseDate,2021-05-01,,True
Honghu Graphic,numParams,2000000000.0,,True
Honghu Graphic,releaseDate,2023-06-28,,True
Fully Convolutional Networks,releaseDate,2014-11-14,,True
NMM(LSTM+RNN),numParams,5180000.0,,True
NMM(LSTM+RNN),releaseDate,2017-08-23,,True
Multiscale deformable part model,releaseDate,2008-06-23,,True
RNN-SpeedUp,numTokens,697500.0,"Section 3: ""The data used in the following experiments were obtained fromPenn Tree Bank: sections 0-20 were used as training data (about930K tokens)""0.75 words per token for English",True
RNN-SpeedUp,releaseDate,2011-05-22,,True
Fairseq-dense 13B,flops,3.267e+22,Table 1,True
Fairseq-dense 13B,numParams,13000000000.0,,True
Fairseq-dense 13B,numTokens,84000000000.0,"112B tokens, or 84B words at 0.75 English words/token. ""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and the English subset of CC100, totalling 112B tokens""...""All models are trained for 300B tokens with a sequence length of 2048 tokens.""",True
Fairseq-dense 13B,gpuType,NVIDIA A100,,True
Fairseq-dense 13B,releaseDate,2021-12-20,,True
MetaLM,releaseDate,2022-06-13,,True
RQ-Transformer (3.8B params ImageNet dataset) copy,flops,1455667200000000000,"flops = (4) * (3120 * 10**9) * (4.5*24 * 3600) * (0.3) = 1455667200000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. ""we provide details for LSUN-cat with largest compute",True
RQ-Transformer (3.8B params ImageNet dataset) copy,numParams,1388000000.0,,True
RQ-Transformer (3.8B params ImageNet dataset) copy,numTokens,1200000.0,size of ImageNet,True
RQ-Transformer (3.8B params ImageNet dataset) copy,trainingTimeDays,108.0,"4.5  days for ImageNet for 3.8B model""We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to trainRQ-Transformers of other sizes. The training time is <9 days for LSUN-cat, LSUN-bedroom, <4.5 days for ImageNet, and CC-3M, and <1 day for LSUN-church and FFHQ. """,True
RQ-Transformer (3.8B params ImageNet dataset) copy,gpuCount,4.0,,True
RQ-Transformer (3.8B params ImageNet dataset) copy,gpuType,NVIDIA A100,,True
RQ-Transformer (3.8B params ImageNet dataset) copy,releaseDate,2022-03-03,,True
ViT-G/14,flops,3.4e+21,"source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodbAlternatively: per paper, ViT-G required between 20-30k TPUv3 core-days to train (from eyeballing the tick marks in Figure 9).TPUv3 is 123 teraflop/s per chip, 2 cores per chip123 trillion * (1/2) * 25,000 * 3600 * 0.4 = 2.2e21",True
ViT-G/14,numParams,1843000000.0,,True
ViT-G/14,numTokens,3000000000.0,"""For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset usedin many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists ofnearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automaticpipeline""",True
ViT-G/14,costDollars,5541.83721088954,,True
ViT-G/14,gpuType,Google TPU v3,,True
ViT-G/14,releaseDate,2021-06-08,,True
LightOn Mini,flops,2.4e+23,6ND aproximation: 6*40B*1T = 2.4e23,True
LightOn Mini,numParams,40000000000.0,,True
LightOn Mini,numTokens,750000000000.0,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""  assuming 0.75 words per token - 750000000000.0 words",True
LightOn Mini,releaseDate,2023-03-21,,True
Self Organizing System,numParams,225.0,,True
Self Organizing System,numTokens,256.0,""" The modifier was thendisabled so that no further changes in the net couldoccur and all 256 possible input patterns were then presented in turn.""""For these purposes, 16-element nets (8 input and 8output) were used because it was desired to exhaust allpossible input patterns, and we were limited to about2^8 inputs by available time. """,True
Self Organizing System,releaseDate,1955-03-01,,True
Grok-0,numParams,33000000000.0,,True
Grok-0,releaseDate,2023-11-04,,True
N-gram,releaseDate,2014-12-24,,True
Samsung Gauss Language,releaseDate,2023-11-08,,True
AlphaZero,flops,3.6679273004682866e+22,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
AlphaZero,numTokens,700000.0,"""We trained a separate instance of AlphaZero for each game. Training proceededfor 700,000 steps""",True
AlphaZero,costDollars,162054.697156363,,True
AlphaZero,gpuCount,64.0,,True
AlphaZero,gpuType,Google TPU v2,,True
AlphaZero,releaseDate,2017-12-05,,True
Multitask Unified Model (MUM),releaseDate,2021-05-18,,True
Rubik's cube ADR robot,flops,854000000000000000000,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,True
Rubik's cube ADR robot,numParams,27769565.0,,True
Rubik's cube ADR robot,numTokens,62400000.0,""" The cumulative amount of experience over that period used for training on theRubik’s cube is roughly 13 thousand years, which is on the same order of magnitude as the 40 thousand years used byOpenAI Five""13/40 * 1.92e8 = 6.24e7",True
Rubik's cube ADR robot,costDollars,3102.26727504595,,True
Rubik's cube ADR robot,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
Rubik's cube ADR robot,releaseDate,2019-10-15,,True
BigSSL,numParams,8000000000.0,,True
BigSSL,numTokens,42626880000.0,"Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hoursNote this involves significant self-training: ""Noisy student training (NST) [23], [41] is a self-trainingmethod where a teacher model generates pseudo-labels for alarge unlabeled dataset, which is in turn used to train a studentmodel with augmentation.""1 hour ~ 13,680 words13680 * 3116000 = 42626880000",True
BigSSL,releaseDate,2021-01-10,,True
Web mining + Decision tree recommender,releaseDate,2002-10-01,,True
"Segatron -XL base, M=150 + HCP",releaseDate,2022-03-21,,True
GPU implementation of neural networks,releaseDate,2004-06-01,,True
AWD-LSTM+WT+Cache+IOG (PTB),releaseDate,2017-09-26,,True
ReLU (NORB),numParams,16210006.0,,True
ReLU (NORB),numTokens,291600.0,"""There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).""",True
ReLU (NORB),releaseDate,2010-06-15,,True
PaLM (540B),flops,2.5272e+24,See Table 20: https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.Equivalent to 6144 TPUv4 for 1368 hours.46.2% model FLOPs utilization,True
PaLM (540B),numParams,540350000000.0,,True
PaLM (540B),numTokens,585000000000.0,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""1 token ~ 0.75 words",True
PaLM (540B),batchSize,4000000.0,,True
PaLM (540B),costDollars,3232806.53266529,"Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",True
PaLM (540B),trainingTimeDays,1368.0,6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.Equivalent to 6144 TPUv4 for 1368 hours.,True
PaLM (540B),gpuCount,6144.0,,True
PaLM (540B),gpuType,Google TPU v4,,True
PaLM (540B),gpuUtilization,0.462,,True
PaLM (540B),releaseDate,2022-04-04,,True
PaLM (540B),batchSize,4000000.0,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k""",True
Transformer-XL DeFINE (141M),flops,6200000000000000000,,True
Transformer-XL DeFINE (141M),numParams,141000000.0,,True
Transformer-XL DeFINE (141M),releaseDate,2019-11-27,,True
Pangu-Weather,flops,3.98e+22,"""Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16 days on a cluster of 192 NVIDIATesla-V100 GPUs.""192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22",True
Pangu-Weather,numParams,256000000.0,,True
Pangu-Weather,trainingTimeDays,1536.0,"4*16 = 64 days""Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16 days on a cluster of 192 NVIDIATesla-V100 GPUs.""",True
Pangu-Weather,gpuCount,192.0,,True
Pangu-Weather,gpuType,NVIDIA V100,,True
Pangu-Weather,releaseDate,2023-07-05,,True
CPC v2,numParams,303000000.0,,True
CPC v2,releaseDate,2019-05-22,,True
Pointer Sentinel-LSTM,releaseDate,2016-09-26,,True
Japanese dialog transformers,numParams,1600000000.0,,True
Japanese dialog transformers,numTokens,2100000000.0,"[Pairs of text]""We obtained 2.1 billion (521 GB) pairs by this method. The average number of utterances in the input context was 2.913, and the average number of characters was 62.3 for the input context and 20.3 for the target utterance""",True
Japanese dialog transformers,releaseDate,2021-11-09,,True
GPT-4V,releaseDate,2023-09-25,,True
Llama 2-34B,flops,4.08e+23,"All models sizes trained on 2.0T tokens, per table 12T * 34b * 6 = 4.08e23Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",True
Llama 2-34B,numParams,34000000000.0,,True
Llama 2-34B,batchSize,4000000.0,,True
Llama 2-34B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Llama 2-34B,releaseDate,2023-07-18,,True
Llama 2-34B,batchSize,4000000.0,,True
Fold2Seq,numTokens,45995.0,"""Training set includes 45995 proteins belonging to a total of 971 folds""",True
Fold2Seq,gpuType,NVIDIA Tesla K80,,True
Fold2Seq,releaseDate,2021-06-24,,True
Emu (BAAI),flops,2.70000000001e+21,"""We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.""https://www.wolframalpha.com/input?i=128*312+TFLOPS+*+2+days+*+0.4",True
Emu (BAAI),numParams,14000000000.0,,True
Emu (BAAI),trainingTimeDays,48.0,"""We train the model on 128 NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the pretraining takes approximately 2 days.""",True
Emu (BAAI),gpuCount,128.0,,True
Emu (BAAI),gpuType,NVIDIA A100 SXM4 80 GB,,True
Emu (BAAI),releaseDate,2023-07-11,,True
BigChaos OptiBlend,numTokens,100480507.0,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",True
BigChaos OptiBlend,releaseDate,2009-08-01,,True
bilingual-gpt-neox-4b,flops,1.2e+22,3.8 billion params * 524b tokens * 6 = 1.2e22,True
bilingual-gpt-neox-4b,numParams,3800000000.0,,True
bilingual-gpt-neox-4b,releaseDate,2023-07-31,,True
CODA,numParams,247000000.00000003,,True
CODA,releaseDate,2021-05-31,,True
HyperNEAT,numParams,239712.0,,True
HyperNEAT,releaseDate,2014-03-05,,True
GPT-NeoX-20B,flops,9.31627008e+22,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,True
GPT-NeoX-20B,numParams,20000000000.0,,True
GPT-NeoX-20B,numTokens,177167400000.0,"""In aggregate, the Pile consists of over 825GiB of raw text data""1 GB ~ 200M words",True
GPT-NeoX-20B,batchSize,3150000.0,,True
GPT-NeoX-20B,costDollars,202407.464363562,,True
GPT-NeoX-20B,trainingTimeDays,2160.0,see other notes,True
GPT-NeoX-20B,gpuCount,96.0,,True
GPT-NeoX-20B,gpuType,NVIDIA A100 SXM4 40 GB,,True
GPT-NeoX-20B,gpuUtilization,0.375,,True
GPT-NeoX-20B,releaseDate,2022-02-09,,True
GPT-NeoX-20B,batchSize,3150000.0,"""we opt to use the same batch size as OpenAI’s 175B model–approximately 3.15M tokens, or 1538 contexts of 2048 tokens each, and train for a total of 150,000 steps""",True
U-PaLM (540B),flops,2.53e+24,"""The total number of extra tokens we train on for the 540Bmodel is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""original PaLM was 2.527e+24. adding 0.16% is ~2.53e24",True
U-PaLM (540B),numParams,540000000000.0,,True
U-PaLM (540B),trainingTimeDays,120.0,5 days,True
U-PaLM (540B),gpuCount,512.0,,True
U-PaLM (540B),gpuType,Google TPU v4,,True
U-PaLM (540B),releaseDate,2022-10-20,,True
AMDIM,numParams,626000000.0,,True
AMDIM,releaseDate,2019-06-03,,True
Samsung Gauss Code,releaseDate,2023-11-08,,True
DreamLLM,flops,754790400000000000000,flops = (128) * ( 312 * 10**12) * (17.5 * 3600) * (0.3) = 7.5e20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from Table 11:128xA800 GPU(6+10+1.5) hours,True
DreamLLM,numParams,7000000000.0,,True
DreamLLM,trainingTimeDays,17.5,from Table 11: (6+10+1.5) hours,True
DreamLLM,gpuCount,128.0,,True
DreamLLM,gpuType,NVIDIA A800,,True
DreamLLM,releaseDate,2023-09-20,,True
RNN,releaseDate,2012-12-01,,True
Iterative Bootstrapping WSD,numTokens,460000000.0,the data were extracted from a 460 million word corpus,True
Iterative Bootstrapping WSD,releaseDate,1995-06-26,,True
BERT-Large-CAS (WT2),releaseDate,2019-04-20,,True
LUKE,flops,175799808000000000000,"(16) * (1413 * 10**10) * (30 * 24 * 3600) * (0.3) = 175799808000000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Dockercontainer 19.02 hosted on a server with two IntelXeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30days.""peak flops for fp32 from https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957",True
LUKE,numParams,484000000.0,,True
LUKE,numTokens,3500000000.0,"""As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. """,True
LUKE,batchSize,2048.0,,True
LUKE,trainingTimeDays,720.0,see compute notes,True
LUKE,gpuCount,16.0,,True
LUKE,gpuType,NVIDIA V100,,True
LUKE,releaseDate,2020-10-02,,True
LUKE,batchSize,2048.0,table in appendix A,True
Llama Guard,numParams,7000000000.0,,True
Llama Guard,numTokens,3000000.0,"14k prompt-response pairs. Based on training details it's 4M tokens, or 3M words",True
Llama Guard,gpuType,NVIDIA A100 SXM4 80 GB,,True
Llama Guard,releaseDate,2023-12-07,,True
Palmyra Large 20B,flops,9.6e+22,"""Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.""I'm not sure if the 800B is how many tokens the model was trained on, or the size of the dataset. But the dataset linked on HuggingFace has 1T tokens, so 800B as tokens trained is more likely.20B*800B*6 = 9.6e22",True
Palmyra Large 20B,numParams,20000000000.0,,True
Palmyra Large 20B,numTokens,750000000000.0,"1 trillion tokens, or 750B words: https://huggingface.co/datasets/Writer/palmyra-data-index",True
Palmyra Large 20B,releaseDate,2023-03-01,,True
ALBERT,numParams,18000000.0,,True
ALBERT,numTokens,3300000000.0,"Pretraining same as for BERT - Wikipedia and BookCorpus""For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",True
ALBERT,gpuType,Google TPU v3,,True
ALBERT,releaseDate,2019-09-26,,True
CODEFUSION (Python),flops,7920000000000000000,V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP,True
CODEFUSION (Python),numParams,75000000.0,,True
CODEFUSION (Python),numTokens,4390400.0,"Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens",True
CODEFUSION (Python),trainingTimeDays,11.0,"""The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.""",True
CODEFUSION (Python),gpuType,NVIDIA Tesla V100 SXM2 32 GB,,True
CODEFUSION (Python),releaseDate,2023-10-26,,True
Convolutional Pose Machines,releaseDate,2016-01-30,,True
InternImage,numParams,1080000000.0,,True
InternImage,numTokens,427000000.0,427M images,True
InternImage,releaseDate,2022-11-10,,True
HSO,flops,345000000000000000000,,True
HSO,numParams,345000000.0,,True
HSO,releaseDate,2021-12-16,,True
Chinese - English translation,releaseDate,2018-03-01,,True
AWD-LSTM+WT+Cache+IOG (WT2),flops,3310000000000000,,True
AWD-LSTM+WT+Cache+IOG (WT2),numParams,53000000.0,,True
AWD-LSTM+WT+Cache+IOG (WT2),releaseDate,2017-09-26,,True
Qarasu-14B,numParams,14000000000.0,,True
Qarasu-14B,numTokens,5000000000.0,7B tokens or ~5B words,True
Qarasu-14B,gpuType,NVIDIA A100 SXM4 40 GB,,True
Qarasu-14B,releaseDate,2023-12-29,,True
Greedy layer-wise DNN training,releaseDate,2006-12-04,,True
DiffDock,flops,72000000000000000000,"""We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).""4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",True
DiffDock,numParams,20240000.0,,True
DiffDock,trainingTimeDays,432.0,18 days,True
DiffDock,gpuType,NVIDIA RTX A6000,,True
DiffDock,releaseDate,2022-10-04,,True
DARK,flops,9700000000000000000,"""Rounding up to the nearest day, if we were to re-perform DARK from nothing to having a trained DARK3 it would take 12 days when parallelized across ten V100 GPUS. Of that time, model training constitutes just over 3 days and only requires 1 GPU""3 * 24 * 3600 * 125 teraFLOP/s * 0.3 (utilization) = 9.7e18",True
DARK,gpuType,NVIDIA V100,,True
DARK,releaseDate,2022-01-28,,True
System 11,flops,12930000000,"Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.Number of passes (Section 2.1):* ""Nearly 1,050 face examples were gathered from face databases [...]""* ""Fifteen face examples are generated for the training set from each original image""Training loop:1. ""initial set of nonface images by generating 1,000 random images""2. Train (presumably on whole set)3. Run + collect false positives4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""""A typical training run selects approximately 8,000 nonface images ""Selecting 8,000 nonface images implies 8000/250 = 32 loops.Assuming compute is 3 * N * D, we have* Loop 1: D = 15*1050 + 1000* Loop 2: D = 15*1050 + 1000 + 250* So on.Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.Hence compute = 3 * 6452 * 668e3 = 1.3e10.",True
System 11,numParams,6452.0,,True
System 11,numTokens,9050.0,"""A typical trainingrun selects approximately 8000 non-face images from the146,212,178 subimages that are available at all locationsand scales in the training scenery images.""""Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from eachoriginal image [...]""""Create an initial set of non-face images by generating1000 images with random pixel intensities""",True
System 11,releaseDate,1996-06-18,,True
Temporal Convolutional Attention-based Network(TCAN) (PTB),releaseDate,2020-02-28,,True
Minerva (540B),flops,2.7415e+24,Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hoursMinerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hoursSo fine-tuning added 8.5% more compute.Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOPhttps://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24,True
Minerva (540B),numParams,540350000000.0,,True
Minerva (540B),numTokens,613875000000.0,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM",True
Minerva (540B),costDollars,3267257.74977208,,True
Minerva (540B),trainingTimeDays,696.0,,True
Minerva (540B),gpuCount,1024.0,,True
Minerva (540B),gpuType,Google TPU v4,,True
Minerva (540B),releaseDate,2022-06-29,,True
ChatRhino,numParams,100000000000.0,,True
ChatRhino,releaseDate,2023-07-13,,True
DANet,releaseDate,2019-04-21,,True
SparseOPT-13B,numParams,13000000000.0,,True
SparseOPT-13B,releaseDate,2023-01-02,,True
Pythia-1b,numParams,1000000000.0,,True
Pythia-1b,releaseDate,2023-04-03,,True
TOME,numParams,220000000.0,,True
TOME,releaseDate,2021-10-12,,True
Inflection-1,flops,1.0001e+24,"<= 2.5e24They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",True
Inflection-1,gpuType,NVIDIA H100 SXM5,,True
Inflection-1,releaseDate,2023-06-23,,True
bRSM + cache,flops,213000000000000,,True
bRSM + cache,numParams,2550000.0,,True
bRSM + cache,releaseDate,2019-12-02,,True
PLUG,flops,3.5997696e+22,128 Nvidia A100 for 35 days,True
PLUG,numParams,27000000000.0,,True
PLUG,trainingTimeDays,840.0,35 days,True
PLUG,gpuCount,128.0,,True
PLUG,gpuType,NVIDIA A100,,True
PLUG,releaseDate,2021-04-19,,True
"LSTM (Hebbian, Cache, MbPA)",flops,24000000000000000000,,True
"LSTM (Hebbian, Cache, MbPA)",numParams,45199999.99999999,,True
"LSTM (Hebbian, Cache, MbPA)",trainingTimeDays,144.0,6 days,True
"LSTM (Hebbian, Cache, MbPA)",gpuCount,8.0,,True
"LSTM (Hebbian, Cache, MbPA)",gpuType,NVIDIA P100,,True
"LSTM (Hebbian, Cache, MbPA)",releaseDate,2018-03-27,,True
λ-WASP,numTokens,792.0,"""Table 1 summarizes the results at the end of the learning curves (792 training examples for λWASP, WASP and SCISSOR, 600 for Z&C)""",True
λ-WASP,releaseDate,2007-06-01,,True
GPipe (Amoeba),numParams,557000000.0,,True
GPipe (Amoeba),numTokens,1281167.0,Table 4,True
GPipe (Amoeba),releaseDate,2018-11-16,,True
GRU + p-tHSM (pretrain via Brown) (WT103),numParams,206000000.0,,True
GRU + p-tHSM (pretrain via Brown) (WT103),releaseDate,2017-08-19,,True
Segment Anything Model,flops,7.8e+21,"""SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of traininglarge scale models. The environmental impact of training the released SAM model is approximately 6963 kWh""68*256 A100-hours = 17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)= 7.82e21max A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)",True
Segment Anything Model,numParams,636000000.0,,True
Segment Anything Model,numTokens,1100000000.0,"""SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.""segmentation mask is a map that identifies segments in an image",True
Segment Anything Model,trainingTimeDays,68.0,"""SAM was trained on 256 A100 GPUS for 68 hours""",True
Segment Anything Model,gpuCount,256.0,,True
Segment Anything Model,gpuType,NVIDIA A100,,True
Segment Anything Model,releaseDate,2023-04-05,,True
DETR,flops,400000000000000000000,"""Training the baseline model for 300 epochs on 16 V100 GPUstakes 3 days, with 4 images per GPU (hence a total batch size of 64). For thelonger schedule used to compare with Faster R-CNN we train for 500 epochswith learning rate drop after 400 epochs. This schedule adds 1.5 AP comparedto the shorter schedule.""48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20",True
DETR,numParams,60000000.0,,True
DETR,numTokens,123000.0,,True
DETR,gpuType,NVIDIA V100,,True
DETR,releaseDate,2020-05-26,,True
Dropout (ImageNet),flops,273196800000000000,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takesroughly four days with dropout and two days without.""1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",True
Dropout (ImageNet),numTokens,1000000.0,"In 2010, a subset of 1000 classeswith roughly 1000 examples per class was the basis of an object recognition competition...",True
Dropout (ImageNet),trainingTimeDays,96.0,4 days with dropout; 2 days without dropout,True
Dropout (ImageNet),gpuType,NVIDIA GeForce GTX 580,,True
Dropout (ImageNet),releaseDate,2012-06-03,,True
LBL,flops,501999999999999.94,,True
LBL,numParams,2000000.0,,True
LBL,releaseDate,2012-06-27,,True
MS-CNN,releaseDate,2016-09-17,,True
Inception-ResNet-V2,numParams,56000000.0,,True
Inception-ResNet-V2,releaseDate,2016-02-23,,True
Variational RHN + WT,releaseDate,2016-07-12,,True
ViT-G/14 (LiT),numParams,3005000000.0,,True
ViT-G/14 (LiT),numTokens,4000000000.0,"Largest dataset is ""4 billion image and alt-text pairs"". This is rounded down slightly; the other datasets are much smaller.",True
ViT-G/14 (LiT),gpuType,Google TPU v3,,True
ViT-G/14 (LiT),releaseDate,2021-11-15,,True
WD+LR+M,releaseDate,2021-10-20,,True
ESM2-8M,flops,48000000000000000000,"""All language models were trained for 500K updates, except the 15B language model"" ""All models used 2 million tokens as batch size except the 15B model""[Supplementary Materials]Hence: 1000B training tokens (500k steps, 2M tokens/batch)Estimate: 8M*2*1000B + 8M*4*1000B",True
ESM2-8M,numParams,8000000.0,,True
ESM2-8M,releaseDate,2022-07-21,,True
MoE-1.1T,flops,2.227e+22,Reported directly in paper. Authors calculate FLOPs analytically in appendix G,True
MoE-1.1T,numParams,1100000000000.0,,True
MoE-1.1T,numTokens,84000000000.0,"112B tokens, or 84B words at 0.75 English words/token. ""We pretrain our models on a union of six Englishlanguage datasets, including the five datasets used to pretrain RoBERTa (Liu et al., 2019) and theEnglish subset of CC100, totalling 112B tokens""",True
MoE-1.1T,gpuType,NVIDIA A100,,True
MoE-1.1T,releaseDate,2021-12-20,,True
TinyLlama-1.1B (1T token checkpoint),flops,7.24598784e+21,"6ND approximation: 6*1.1B * 1T = 6600000000000000000000Based on reported GPU-time and utilization:flops = (num gpu) * (peak flops) * (time in seconds) * (reported utilization rate) = (16) * (312 * 10**12) * (30 * 24 * 3600) * (0.56) = 7245987840000001048576source: https://github.com/jzhang38/TinyLlama""Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization""and Releases Schedule from the same link",True
TinyLlama-1.1B (1T token checkpoint),numParams,1100000000.0,,True
TinyLlama-1.1B (1T token checkpoint),numTokens,750000000000.0,1T tokens checkpoint so around 0.75T words,True
TinyLlama-1.1B (1T token checkpoint),trainingTimeDays,720.0,1T checkpoint was released after 1 monthsource: https://github.com/jzhang38/TinyLlama,True
TinyLlama-1.1B (1T token checkpoint),gpuCount,16.0,,True
TinyLlama-1.1B (1T token checkpoint),gpuType,NVIDIA A100 SXM4 40 GB,,True
TinyLlama-1.1B (1T token checkpoint),gpuUtilization,0.56,,True
TinyLlama-1.1B (1T token checkpoint),releaseDate,2023-10-01,,True
AWD-LSTM-DRILL + dynamic evaluation† (WT2),flops,424000000000000000,,True
AWD-LSTM-DRILL + dynamic evaluation† (WT2),numParams,34000000.0,,True
AWD-LSTM-DRILL + dynamic evaluation† (WT2),releaseDate,2019-05-14,,True
Poro34B (700B token checkpoint),flops,1.53e+23,"6ND = 6*0.75T*34B= 153000000000000000000000""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",True
Poro34B (700B token checkpoint),numParams,34000000000.0,,True
Poro34B (700B token checkpoint),numTokens,750000000000.0,"1T tokens, assuming 0.75 word per token""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",True
Poro34B (700B token checkpoint),gpuCount,512.0,,True
Poro34B (700B token checkpoint),gpuType,AMD Instinct MI250X,,True
Poro34B (700B token checkpoint),releaseDate,2023-12-14,,True
SqueezeNet,numParams,1200000.0,,True
SqueezeNet,releaseDate,2016-02-24,,True
BIG-G 137B,flops,5.6e+23,"""BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswaniet al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDAarchitectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of webdocuments, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8trillion BPE tokens using a 32k-token SentencePiece vocabulary""Appendix:""We use a pre-training batch size of 262k tokens for all models...""2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)681B * 137B * 6 = 5.6e23",True
BIG-G 137B,numParams,137000000000.0,,True
BIG-G 137B,releaseDate,2022-06-09,,True
Fairseq + UID: variance,releaseDate,2021-05-15,,True
YouTube Video Recommendation System,numTokens,10000000000.0,"""We currently handle millions of usersand tens of billions of activity events with a total footprintof several terabytes of data""If 10M users each watch 1000 videos, that's 10B visualizations, which matches their ""activity events"" count.",True
YouTube Video Recommendation System,releaseDate,2010-09-26,,True
Grover-Mega,numParams,1500000000.0,,True
Grover-Mega,releaseDate,2019-05-29,,True
Temporal Convolutional Attention-based Network(TCAN) (WT2),numParams,33000000.0,,True
Temporal Convolutional Attention-based Network(TCAN) (WT2),releaseDate,2020-02-28,,True
Hopfield network,numParams,9900.0,,True
Hopfield network,releaseDate,1982-04-01,,True
CT-MoS (PTB),releaseDate,2020-12-25,,True
RL mapping instructions (troubleshooting),numParams,133140.0,,True
RL mapping instructions (troubleshooting),numTokens,1327.36,"Shown at beginning of section 7Total number of documents is 128, average number of actions per document is 10.37",True
RL mapping instructions (troubleshooting),releaseDate,2009-08-02,,True
Neural Architecture Search with base 8 and shared embeddings,flops,10500000000000000,,True
Neural Architecture Search with base 8 and shared embeddings,numParams,54000000.0,,True
Neural Architecture Search with base 8 and shared embeddings,releaseDate,2016-11-05,,True
Hanabi 4 player,flops,4300000000000000000,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,True
Hanabi 4 player,numParams,764000.0,,True
Hanabi 4 player,costDollars,0.33577481094615,,True
Hanabi 4 player,releaseDate,2019-02-01,,True
PNAS-net,numParams,86000000.0,,True
PNAS-net,releaseDate,2017-12-02,,True
Vicuna-13B,numParams,13000000000.0,,True
Vicuna-13B,costDollars,259.0,"$300 in 2020, adjusted for inflation using BLS.gov inflation calculator",True
Vicuna-13B,releaseDate,2023-03-30,,True
GPT-2 (fine-tuned with HYDRA),flops,19200000000000000,,True
GPT-2 (fine-tuned with HYDRA),numParams,1540000000.0,,True
GPT-2 (fine-tuned with HYDRA),releaseDate,2021-10-16,,True
Binarized Neural Network (MNIST),numParams,37000000.0,,True
Binarized Neural Network (MNIST),numTokens,60000.0,"60k training images, 10k test in MNIST",True
Binarized Neural Network (MNIST),releaseDate,2016-03-17,,True
MOSS-Moon-003,flops,6.67e+22,"6.67e22 including pre-training for CodeGen:""The base language model of MOSS-003, which was initialized with CodeGen and further pre-trained on 100B Chinese tokens and 20B English tokens. The model has seen 700B tokens during pre-training and consumed ~6.67x10^22 FLOPs in total.""",True
MOSS-Moon-003,numParams,16000000000.0,,True
MOSS-Moon-003,releaseDate,2023-04-19,,True
Feedforward NN,flops,350000000000000,Roughly two times the number of parameters for ops per forward pass. So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14,True
Feedforward NN,numParams,7082000.0,,True
Feedforward NN,costDollars,0.0129257750226527,,True
Feedforward NN,releaseDate,2010-05-13,,True
ProgressiveGAN,releaseDate,2017-10-27,,True
Rita-XLarge,flops,3.4e+21,"""The models were trained for a total training time of over 25 thousand Nvidia-V100 GPU hours""125 teraFLOP/s (uncertain which V100 model, tensor performance varies from 112-130tFLOP/s) * 25000 * 3600 * 0.3 (utilization) = 3.4e+21""",True
Rita-XLarge,numParams,1200000000.0,,True
Rita-XLarge,gpuType,NVIDIA V100,,True
Rita-XLarge,releaseDate,2022-07-14,,True
Sparse all-MLP,flops,60770304000000000000,112 hours on 32 V100 GPUsassumed 0.33 util rate32*112*60*60*0.3*1.57E+13,True
Sparse all-MLP,numParams,9400000000.0,,True
Sparse all-MLP,numTokens,75000000000.0,100B tokens (Table 2) so 75B words.,True
Sparse all-MLP,costDollars,320.0,,True
Sparse all-MLP,trainingTimeDays,112.0,,True
Sparse all-MLP,releaseDate,2022-04-14,,True
Alleviated TOI 10 (PTB),releaseDate,2019-09-18,,True
LSTM-MemoryAug (WT2),releaseDate,2020-09-29,,True
OmniVec,releaseDate,2023-11-07,,True
TransformerXL + FWL,numParams,257000000.0,,True
TransformerXL + FWL,releaseDate,2022-12-05,,True
OmegaPLM,flops,1.38018816e+22,"""OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days."" ""Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.""Assume 0.4 utilization.Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.4 * = 1.38e2",True
OmegaPLM,numParams,670000000.0,,True
OmegaPLM,gpuType,NVIDIA A100 SXM4 80 GB,,True
OmegaPLM,releaseDate,2022-07-22,,True
DnCNN,releaseDate,2017-02-01,,True
GLM-2B,releaseDate,2021-03-18,,True
RNN Baseline,releaseDate,2019-07-14,,True
Named Entity Recognition model,flops,96940800000000000,8 hours of training for NERGeForce GTX TITAN X GPU0.33 utilization rate,True
Named Entity Recognition model,numTokens,204567.0,Table 2. 204567 tokens,True
Named Entity Recognition model,costDollars,0.625835848597977,,True
Named Entity Recognition model,trainingTimeDays,8.0,"""the model training requires about 12 hours for POS tagging and 8hours for NER""",True
Named Entity Recognition model,gpuCount,1.0,,True
Named Entity Recognition model,gpuType,NVIDIA GeForce GTX TITAN X,,True
Named Entity Recognition model,releaseDate,2016-05-29,,True
YOLO,numParams,271684800.0,,True
YOLO,releaseDate,2015-06-08,,True
PeptideBERT,flops,7.6e+21,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming from Table 1 we have 244 minutes11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",True
PeptideBERT,trainingTimeDays,4.067,244 minues from Table 1,True
PeptideBERT,gpuCount,1.0,,True
PeptideBERT,gpuType,NVIDIA Geforce GTX1080 Ti,,True
PeptideBERT,releaseDate,2023-08-28,,True
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),flops,309000000000000000,,True
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),numParams,33000000.0,,True
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),releaseDate,2017-08-07,,True
Maxout Networks,releaseDate,2013-02-18,,True
Tensor-Transformer(1core)+PN (PTB),releaseDate,2020-03-17,,True
BLIP-2 (Q-Former),flops,1.20000000001e+21,https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33,True
BLIP-2 (Q-Former),numParams,1480000000.0,,True
BLIP-2 (Q-Former),trainingTimeDays,200.0,"""For example, usinga single 16-A100(40G) machine, our largest model withViT-g and FlanT5-XXL requires less than 6 days for the firststage and less than 3 days for the second stage.""9 days = 216 hours",True
BLIP-2 (Q-Former),gpuCount,16.0,,True
BLIP-2 (Q-Former),gpuType,NVIDIA A100 SXM4 40 GB,,True
BLIP-2 (Q-Former),releaseDate,2023-01-30,,True
BLOOMZ-176B,numParams,176000000000.0,,True
BLOOMZ-176B,numTokens,20000000000.0,"per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",True
BLOOMZ-176B,releaseDate,2022-11-03,,True
InternLM2-20b,numParams,20000000000.0,,True
InternLM2-20b,releaseDate,2024-01-12,,True
Persimmon-8B,numParams,9300000000.0,,True
Persimmon-8B,numTokens,552750000000.0,737B tokens = 552750M words,True
Persimmon-8B,releaseDate,2023-09-07,,True
"Segatron-XL large, M=384 + HCP",flops,26500000000000000000,,True
"Segatron-XL large, M=384 + HCP",numParams,257000000.0,,True
"Segatron-XL large, M=384 + HCP",releaseDate,2022-03-21,,True
Hiero,numParams,120000000.0,,True
Hiero,numTokens,171400000.0,[WORDS]155M words dataset for the language model plus (7.2+9.2)M words for the translation model?,True
Hiero,releaseDate,2005-06-01,,True
RNN-WER,numParams,26500000.0,,True
RNN-WER,numTokens,1100000.0,"dataset is 81 hoursAt 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)that's 81*228*60 = 1,108,080another source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/",True
RNN-WER,releaseDate,2014-06-22,,True
ConSERT,flops,280000000000000000000,Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)Foundation model is BeRT with 2.8e+20 FLOP.So total compute is 2.8e+20.,True
ConSERT,numParams,345000000.0,,True
ConSERT,trainingTimeDays,0.1,,True
ConSERT,gpuType,NVIDIA Tesla V100S PCIe 32 GB,,True
ConSERT,releaseDate,2021-05-25,,True
MCDNN (MNIST),flops,3726979200000000,"Num of multiply-adds per forward pass2 FLOPs/mult-add3 (fp+bp FLOPs / fp FLOPs)800 epochs60.000 training size35 networks""Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed""",True
MCDNN (MNIST),numParams,1994300.0,,True
MCDNN (MNIST),numTokens,60000.0,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",True
MCDNN (MNIST),costDollars,0.0833259334877972,,True
MCDNN (MNIST),releaseDate,2012-02-13,,True
Network in Network,releaseDate,2013-12-16,,True
AudioLM,numParams,1500000000.0,,True
AudioLM,numTokens,820800000.0,60k hours of English speech13680*60000 = 820800000 wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce,True
AudioLM,gpuType,Google TPU v4,,True
AudioLM,releaseDate,2023-07-26,,True
NÜWA,flops,4.8384e+21,"From AI Tracker:""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". Info sheet from NVIDIA (https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) gives single precision TensorFloat 32 performance of 156 TFLOPs/s. So we get 64 x 14 x 156 = 140,000 TFLOPs/s x days.""Multiply by seconds/day and 30% utilization",True
NÜWA,numParams,870000000.0,,True
NÜWA,costDollars,10446.8368703773,,True
NÜWA,releaseDate,2021-11-24,,True
QRNN,flops,360000000000000000,,True
QRNN,numParams,135000000.0,,True
QRNN,releaseDate,2018-02-01,,True
BLOOM-1.7B,numParams,1700000000.0,,True
BLOOM-1.7B,releaseDate,2022-07-05,,True
GPU DBNs,flops,1000000000000000,https://www.getguesstimate.com/models/19602,True
GPU DBNs,numParams,100000000.0,,True
GPU DBNs,numTokens,1000000.0,Table 2 shows the running time for processing 1 millionexamples for RBMs of varying size,True
GPU DBNs,costDollars,0.0567799020351146,,True
GPU DBNs,releaseDate,2009-06-15,,True
ByT5-XXL,flops,8.1e+22,"""Like mT5, we set our sequencelength to 1024 (bytes rather than tokens), and trainfor 1 million steps over batches of 2^20 tokens.""12.9 billion * 1 million * 2^20 * 6 = ~8.1e22",True
ByT5-XXL,numParams,12900000000.0,,True
ByT5-XXL,batchSize,1048576.0,,True
ByT5-XXL,gpuCount,64.0,,True
ByT5-XXL,gpuType,Google TPU v3,,True
ByT5-XXL,releaseDate,2021-05-28,,True
ByT5-XXL,batchSize,1048576.0,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens""",True
NetTalk (dictionary),flops,27664065000,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word,True
NetTalk (dictionary),numParams,18629.0,,True
NetTalk (dictionary),numTokens,1000.0,"""A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus""",True
NetTalk (dictionary),releaseDate,1987-06-06,,True
H-LSTM+wg+rcp+rcg+wp,numParams,800000.0,,True
H-LSTM+wg+rcp+rcg+wp,releaseDate,2019-01-30,,True
Tranception,flops,7.24e+21,Trained using 64 A100 GPUs for two weeks.64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)= 7.24e21,True
Tranception,numParams,700000000.0,,True
Tranception,trainingTimeDays,336.0,2 weeks,True
Tranception,gpuCount,64.0,,True
Tranception,gpuType,NVIDIA A100,,True
Tranception,releaseDate,2022-05-27,,True
3D city reconstruction,releaseDate,2009-09-29,,True
ESM2-15B,flops,7.35000000001e+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOPfrom Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B “connections” x 6. : 7.8e22 FLOPfrom the paper's Supplementary Materials: ""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""60 days x 512 V100s x an imputed 30% utilization"": 1e23 FLOPGeometric mean: 7.35e22",True
ESM2-15B,numParams,15000000000.0,,True
ESM2-15B,trainingTimeDays,1440.0,,True
ESM2-15B,gpuCount,512.0,,True
ESM2-15B,gpuType,NVIDIA V100,,True
ESM2-15B,releaseDate,2022-07-21,,True
S4,flops,600000000000000000000,,True
S4,numParams,249000000.00000003,,True
S4,releaseDate,2021-10-31,,True
GPT-2-Small+Pixelfly,releaseDate,2021-11-30,,True
LSTM(medium)+Sememe+cell,releaseDate,2019-10-20,,True
MoCo,numParams,375000000.0,,True
MoCo,releaseDate,2019-11-13,,True
Time-delay neural networks,releaseDate,1989-03-03,,True
HyenaDNA,flops,4490000000000000000,"8 Nvidia A100 (80GB) GPUs, ~300 minutes (figure 3.2)Assuming 40% utilizationEstimate: 78 TFLOP/s * 8 GPUs * (300*60) s * 0.4 = 4.49e18 FLOPs",True
HyenaDNA,numParams,1600000.0,,True
HyenaDNA,gpuCount,8.0,,True
HyenaDNA,gpuType,NVIDIA A100,,True
HyenaDNA,releaseDate,2023-06-27,,True
Cube-Space AutoEncoder,flops,106608960000000000,"(1) * (4113 * 10**9) * (24 * 3600) * (0.3) = 106608960000000000(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = ""For each domain, we searched for 100 iterations (≈15min/iter, 24 hours total) on a Tesla K80""4.113 TFLOPS from https://www.techpowerup.com/gpu-specs/tesla-k80.c2616",True
Cube-Space AutoEncoder,numTokens,50000.0,"""Finally, we tested Mandrill 15-puzzle, a significantly morechallenging 4x4 variant of the sliding tile puzzle (Figure 1).We trained the network with more hyperparameter tuning it-erations (300) and a larger training set (50000). We gener-ated l = 14, 21 instances (20 each) and ran the system (Ta-ble 2, bottom right). """,True
Cube-Space AutoEncoder,trainingTimeDays,24.0,"""For each domain, we searched for 100 iterations (≈15min/iter, 24 hours total) on a Tesla K80""",True
Cube-Space AutoEncoder,gpuCount,1.0,,True
Cube-Space AutoEncoder,gpuType,NVIDIA Tesla K80,,True
Cube-Space AutoEncoder,releaseDate,2020-04-27,,True
Claude 2,flops,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,True
Claude 2,releaseDate,2023-07-11,,True
StableLM-3B-4E1T,flops,6.21e+22,"""StableLM-3B-4E1T was trained on the Stability AI cluster across 256 NVIDIA A100 40GB GPUs (AWS P4d instances). Training began on August 23, 2023, and took approximately 30 days to complete.""256 * 30 * 24* 3600 * 312 trillion * 0.3 utilization (assumption) = 6.21e22",True
StableLM-3B-4E1T,numParams,2795443200.0,,True
StableLM-3B-4E1T,numTokens,750000000000.0,Trained on 1T tokens (~750B words),True
StableLM-3B-4E1T,batchSize,4194304.0,,True
StableLM-3B-4E1T,trainingTimeDays,720.0,approximately 30 days,True
StableLM-3B-4E1T,gpuType,NVIDIA A100,,True
StableLM-3B-4E1T,releaseDate,2023-09-29,,True
StableLM-3B-4E1T,batchSize,4194304.0,"""The batch size is set to 1024 (4,194,304 tokens).""",True
2-layer skip-LSTM + dropout tuning (WT2),numParams,5400000.0,,True
2-layer skip-LSTM + dropout tuning (WT2),releaseDate,2018-05-23,,True
DIABETES,numParams,429409.0,,True
DIABETES,releaseDate,1991-06-24,,True
BLOOM-560M,numParams,560000000.0,,True
BLOOM-560M,releaseDate,2022-07-05,,True
Go-explore,releaseDate,2020-04-27,,True
Integer Transformer,numParams,247000000.00000003,,True
Integer Transformer,releaseDate,2020-09-17,,True
LMSI-Palm,numParams,540000000000.0,,True
LMSI-Palm,releaseDate,2022-10-20,,True
NAS+ESS (23M),releaseDate,2020-05-06,,True
BLSTM for handwriting (2),numParams,100881.0,,True
BLSTM for handwriting (2),releaseDate,2007-12-03,,True
Guanaco-65B,numParams,65000000000.0,,True
Guanaco-65B,trainingTimeDays,24.0,24 hours,True
Guanaco-65B,releaseDate,2023-05-23,,True
HRA,releaseDate,2017-06-13,,True
VGG16,flops,9253440000000000000,"2.5 weeks * 4 Titan Black GPUs * 0.30 utilizationSection 3.3: ""On a system equipped withfour NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""",True
VGG16,numParams,138000000.0,,True
VGG16,numTokens,1300000.0,"""In this section, we present the image classification results achieved by the describedConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",True
VGG16,costDollars,82.800136143535,,True
VGG16,gpuType,NVIDIA GTX Titan Black,,True
VGG16,releaseDate,2014-09-04,,True
Advantage Learning,releaseDate,2015-12-15,,True
EXAONE 2.0,numParams,300000000000.0,,True
EXAONE 2.0,releaseDate,2023-07-19,,True
Cascaded LNet-ANet,releaseDate,2014-11-28,,True
Primer,flops,7.1e+21,From the email they claim to have use 72K TPUv4 hours for trainingThus: 72000 h * 0.1 * 275e12 FLOP/s 3600s/h = 7.1e21 FLOP,True
Primer,numParams,1900000000.0,,True
Primer,numTokens,173284750600.0,"In GB - TODO convert to words""Dataset size: 806.92 GiB""https://www.tensorflow.org/datasets/catalog/c4This was the largest dataset that the authors used ""These benefits are robust and hold across model sizes (20Mto 1.9B parameters), across compute scales (10 to 105accelerator hours), across datasets (LM1B,C4, PG19 [22])""802.92 GiB ~ 866.42 GB1 GB ~ 200M words",True
Primer,costDollars,9690.72466888913,,True
Primer,releaseDate,2022-01-24,,True
Cognitron,releaseDate,1975-09-01,,True
Qwen-7B,flops,1.01e+23,2.4T tokens per Table 17b*2.4T*6 = 1.01e23,True
Qwen-7B,numParams,7000000000.0,,True
Qwen-7B,batchSize,4000000.0,,True
Qwen-7B,releaseDate,2023-09-28,,True
Qwen-7B,batchSize,4000000.0,Table 1,True
RedPajama-INCITE-7B-Base,flops,4.1e+22,Trained over 1.001 trillion tokens.6.9b * 1 trillion * 6 = 4.1e22,True
RedPajama-INCITE-7B-Base,numParams,6900000000.0,,True
RedPajama-INCITE-7B-Base,numTokens,900000000000.0,"1.2 trillion, or 900b words at 0.75 words/token",True
RedPajama-INCITE-7B-Base,batchSize,4000000.0,,True
RedPajama-INCITE-7B-Base,gpuType,NVIDIA V100,,True
RedPajama-INCITE-7B-Base,releaseDate,2023-06-06,,True
RedPajama-INCITE-7B-Base,batchSize,4000000.0,"""global batch size 4M tokens""",True
GOAT,flops,7.8e+22,"[Final calculation](8 TPUs)(4.20e14 FLOP/s)(0.1 utilisation rate)(32 agents)(7.3e6 s/agent) = 7.8e22 FLOPs==========================NOTES BELOW[Hardware]- ""Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.""- TPUv3 (half precision): 4.2e14 FLOP/s- Number of TPUs: 8- Utilisation rate: 0.1[Timesteps]- Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.- 3.9e11 / 5e4 = 8e6 s → ~93 days- 100 million steps is equivalent to 30 minutes of wall-clock time in our setup. (pg 29, fig 27)- 1e8 steps → 0.5h- 3.9e11 steps → 1950h → 7.0e6 s → ~82 days- Both of these seem like overestimates, because:“Finally, on the largest timescale (days), generational training iteratively improves population performance by bootstrapping off previous generations, whilst also iteratively updating the validation normalised percentile metric itself.” (pg 16)- Suggests that the above is an overestimate of the number of days needed, else they would have said (months) or (weeks)?- Final choice (guesstimate): 85 days = 7.3e6 s[Population size]- 8 agents? (pg 21) → this is describing the case where they’re not using PBT, so ignore this number- The original PBT paper uses 32 agents for one task https://arxiv.org/pdf/1711.09846.pdf (in general it uses between 10 and 80)- (Guesstimate) Average population size: 32",True
GOAT,numParams,3500000.0,,True
GOAT,numTokens,390000000000.0,Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.,True
GOAT,costDollars,122418.966962561,,True
GOAT,gpuType,Google TPU v3,,True
GOAT,releaseDate,2021-07-27,,True
Xinghan Foundation Model,releaseDate,2023-10-25,,True
T5-3B,flops,865865406873600000000,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodbHowever, this seems dubiously high.""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOPhttps://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5",True
T5-3B,numParams,2800000000.0,,True
T5-3B,numTokens,25500000000.0,"""This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but alsocomprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""750GB * 200M word/GB = 1.5e11""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""""Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.""The fraction is 25.5 billion / 150 billion = 0.17 epochs.",True
T5-3B,gpuType,Google TPU v3,,True
T5-3B,releaseDate,2019-10-23,,True
CPM-2,numParams,11000000000.0,,True
CPM-2,numTokens,444100000000.0,"""We pre-train our model on WuDaoCorpus (Yuan et al., 2021), which contains 2.3TB cleaned Chinese data as well as 300GB cleaned English data.""2300*167 million + 300*200 million = 444,100,000,000 (444 billion)https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",True
CPM-2,releaseDate,2021-06-24,,True
CodeWhisperer,releaseDate,2022-06-24,,True
Pluribus,flops,66000000000000000,"Trained in 8 days on a 64 core CPUhttps://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core. https://epochai.org/blog/estimating-training-compute8 days, 64 cores, 5e9 FLOP/s, 30% utilization",True
Pluribus,releaseDate,2019-07-11,,True
KwaiYii,numParams,13000000000.0,,True
KwaiYii,releaseDate,2023-08-16,,True
Swin Transformer V2,flops,1.1e+21,"trained on ""<0.5k"" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)A core is 123/2 teraflops500 core-days= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization~= 1.1e21",True
Swin Transformer V2,numParams,3000000000.0,,True
Swin Transformer V2,gpuType,NVIDIA A100 SXM4 40 GB,,True
Swin Transformer V2,releaseDate,2021-11-18,,True
ResNeXt-101 Billion-scale,numParams,193000000.0,,True
ResNeXt-101 Billion-scale,releaseDate,2019-05-02,,True
SNARC,numParams,40.0,,True
SNARC,releaseDate,1952-01-08,,True
MMLSTM,flops,2320000000000000000,,True
MMLSTM,numParams,75000000.0,,True
MMLSTM,releaseDate,2019-12-05,,True
ESM2-150M,flops,1.1e+21,from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 1.1e21 FLOP,True
ESM2-150M,numParams,150000000.0,,True
ESM2-150M,releaseDate,2022-07-21,,True
Ferret (13B),numParams,13000000000.0,,True
Ferret (13B),trainingTimeDays,120.0,"""The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""",True
Ferret (13B),gpuCount,8.0,,True
Ferret (13B),gpuType,NVIDIA A100,,True
Ferret (13B),releaseDate,2023-10-11,,True
LiMoE,flops,1.8e+22,"Section 3.2: ""The model contains 5.6B parameters in total, but only applies 675M parameters per token""From Section A.3, ""batch size 21502 with resolution 288 and text sequence length16"". ""The model was trained for 700k steps pre-cooldown. There was one cooldown of length 125k stepsfrom the final step, and 3 of length 40k steps starting from step 650k"". Patch size 14 for images.Assume C = 6*N*D. C = 6*675e6*21.5e3*1e6*(16+(288/14)**2)/2 = 1.8e22This is broadly consistent with ViT-H/14's compute",True
LiMoE,numParams,5600000000.0,,True
LiMoE,numTokens,7600000000.0,"Section 3: ""Training data. By default, all models are trained on paired image-text data used in [16], consisting of 3.6B images and alt-texts scraped from the web. For large LIMoE-H/14 experiment, we also co-train with JFT-4B [17]. """,True
LiMoE,releaseDate,2022-06-06,,True
BigChaos 2008,numTokens,100480507.0,"""Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.""",True
BigChaos 2008,releaseDate,2008-11-25,,True
PolyCoder,flops,1.1e+21,"""We use GPT-NeoX toolkit 11 totrain the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The walltime used to train the largest 2.7B model is about 6 weeks""8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21",True
PolyCoder,numParams,2700000000.0,,True
PolyCoder,trainingTimeDays,1000.0,6 weeks,True
PolyCoder,gpuType,NVIDIA Quadro RTX 8000,,True
PolyCoder,releaseDate,2022-02-26,,True
WizardCoder-15.5B,numParams,15500000000.0,,True
WizardCoder-15.5B,releaseDate,2023-06-14,,True
FTW,flops,7.26e+21,We assume that most operations happen in the visual embedding.2* 84^2*84^2 * 32 * 3 / 1^2 = 9.5 *10^9new image size: 76 x 76 x 32ignore ReLU/additions becaue probably very little influence 2 * 76^2 * 76^2 * 10* 64 = 4 *10^10new image size: 72 x 72 x 642 * 72^2 *72^2 * 64 * 64 * 3=  6.6 * 10^11new image size: 69 x 69 x 642 * 69^2 *69^2 * 64 * 64 * 3=  5.5 * 10^11new image size: 66 x 66 x 64Linear layer: 2* ( 66*66*64)*256 = 1.4*10^8Total aprox: 1.21e+12 FLOP/forward pass,True
FTW,numParams,126001330.0,,True
FTW,costDollars,21045.016545808,,True
FTW,releaseDate,2018-07-03,,True
Code Llama-70B,flops,1.230000000001e+24,Finetune compute for 70B model: 1T tokens of code * ,True
Code Llama-70B,numParams,70000000000.0,,True
Code Llama-70B,numTokens,3000000000000.0,Llama 70B training dataset was 2 trillion tokens. Code Llama finetuning dataset was 1 trillion tokens of code.,True
Code Llama-70B,batchSize,4000000.0,,True
Code Llama-70B,trainingTimeDays,6480.0,Assuming Code Llama 70B training continued on same hardware as Llama 2 70B.,True
Code Llama-70B,gpuCount,400.0,,True
Code Llama-70B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Code Llama-70B,gpuUtilization,0.435,,True
Code Llama-70B,releaseDate,2024-01-29,,True
Code Llama-70B,batchSize,4000000.0,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. ""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""Subsequent fine-tuning batch sizes are 500k-1M. ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",True
S-Norm,numTokens,1500000000.0,"""530k question-document training pairs""average question length of 14 words and document length of 2895 words, per https://www.cs.utexas.edu/~eunsol/files/papers/acl17jcwz.pdf530,000 * 2900 = ~1,500,000,000",True
S-Norm,releaseDate,2017-10-29,,True
Dropout-LSTM+Noise(Laplace),releaseDate,2018-05-03,,True
AlexNet,flops,470000000000000000,1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) = 470 PF = 0.0054 pfs-daysSource: https://openai.com/blog/ai-and-compute/,True
AlexNet,numParams,60000000.0,,True
AlexNet,numTokens,1200000.0,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""",True
AlexNet,costDollars,7.99991372136391,,True
AlexNet,gpuType,NVIDIA GeForce GTX 580,,True
AlexNet,releaseDate,2012-09-30,,True
LSTM+GraB,releaseDate,2022-05-22,,True
OPT-2.7B (finetuned on WT2),numParams,2700000000.0,,True
OPT-2.7B (finetuned on WT2),releaseDate,2022-06-21,,True
Yuan 1.0,flops,3.5380000000001e+23,Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOPhttps://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day,True
Yuan 1.0,numParams,245730000000.0,,True
Yuan 1.0,numTokens,1000000000000.0,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.",True
Yuan 1.0,batchSize,6881280.0,,True
Yuan 1.0,costDollars,606364.74789733,,True
Yuan 1.0,gpuCount,2128.0,,True
Yuan 1.0,gpuUtilization,0.45,,True
Yuan 1.0,releaseDate,2021-10-12,,True
Yuan 1.0,batchSize,6881280.0,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",True
AlphaGo Zero,flops,3.41e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-secondsTotal compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOPSo similar to the Cotra and Davidson estimate (within a factor of 2).",True
AlphaGo Zero,numParams,46400244.0,,True
AlphaGo Zero,numTokens,5800000000.0,"""Over the course of training, 29 million games of self-play were generated""Approx 200 moves per Go game on averagehttps://homepages.cwi.nl/~aeb/go/misc/gostat.htmlThus 200 * 29e6 = 5.8e9",True
AlphaGo Zero,costDollars,1544149.41763173,,True
AlphaGo Zero,trainingTimeDays,480.0,,True
AlphaGo Zero,gpuType,Google TPU v1,,True
AlphaGo Zero,releaseDate,2017-10-18,,True
Hybrid H3-125M,numParams,125000000.0,,True
Hybrid H3-125M,releaseDate,2022-12-28,,True
Seq2Seq LSTM,flops,56000000000000000000,384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOPTimes 5 independent models in ensemble => 5.6E+19 FLOPIf we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP,True
Seq2Seq LSTM,numParams,1920000000.0,,True
Seq2Seq LSTM,numTokens,652000000.0,"[WORDS]""We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”subset from [29].""",True
Seq2Seq LSTM,costDollars,79.5959948476017,,True
Seq2Seq LSTM,releaseDate,2014-09-10,,True
DrLIM,numParams,37097.0,,True
DrLIM,numTokens,217470.0,"""The dataset was split into 660 training images and a 312test images. The result of training on all 10989 similar pairsand 206481 dissimilar pairs is a 3-dimensional manifold inthe shape of a cylinder (see figure 8).""206481 + 10989 = 217470",True
DrLIM,releaseDate,2006-06-17,,True
xTrimoPGLM -100B,flops,6.0001e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date,xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""6 * 100 billion params * 1T tokens = 6e238*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24",True
xTrimoPGLM -100B,numParams,1820000.0,,True
xTrimoPGLM -100B,trainingTimeDays,3912.0,163 days,True
xTrimoPGLM -100B,gpuCount,768.0,,True
xTrimoPGLM -100B,gpuType,NVIDIA A100 SXM4 40 GB,,True
xTrimoPGLM -100B,releaseDate,2023-07-06,,True
SARA-RT-2,numParams,5000000000.0,,True
SARA-RT-2,releaseDate,2023-12-04,,True
MEB,numParams,135000000000.0,,True
MEB,releaseDate,2021-09-04,,True
Zi Yue,releaseDate,2023-07-28,,True
OtterHD-8B,numParams,8000000000.0,,True
OtterHD-8B,trainingTimeDays,3.0,3 hours,True
OtterHD-8B,gpuCount,8.0,,True
OtterHD-8B,gpuType,NVIDIA A100,,True
OtterHD-8B,releaseDate,2023-11-07,,True
Hybrid H3-2.7B,flops,849000000000000000000,,True
Hybrid H3-2.7B,numParams,2700000000.0,,True
Hybrid H3-2.7B,releaseDate,2022-12-28,,True
Symmetric Residual Encoder-Decoder Net,releaseDate,2016-03-30,,True
Diplodocus,releaseDate,2022-10-11,,True
SENet (ImageNet),numParams,28100000.0,,True
SENet (ImageNet),releaseDate,2017-09-05,,True
ChatGPT (gpt-3.5-turbo),numParams,20000000000.0,,True
ChatGPT (gpt-3.5-turbo),releaseDate,2022-11-30,,True
Pythia-1.4b,numParams,1400000000.0,,True
Pythia-1.4b,releaseDate,2023-04-03,,True
MoE,flops,93939056640000000000,12 days 64 NVIDIA K40 GPUS (see hardware data sheet for performance)0.33 util rate ,True
MoE,numParams,8700000000.0,,True
MoE,numTokens,100000000000.0,"[WORDS]""We constructed a similar training set consisting of shuffled unique sentences from Google’s internalnews corpus, totalling roughly 100 billion words""",True
MoE,costDollars,8484.354244363,,True
MoE,trainingTimeDays,288.0,12 days,True
MoE,gpuCount,64.0,,True
MoE,gpuType,NVIDIA Tesla K40t,,True
MoE,releaseDate,2017-01-23,,True
Dropout (2014),releaseDate,2014-06-01,,True
Skywork-13B,flops,2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". ""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",True
Skywork-13B,numParams,13000000000.0,,True
Skywork-13B,numTokens,2780000000000.0,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",True
Skywork-13B,batchSize,16000000.0,,True
Skywork-13B,trainingTimeDays,940.0,39 days,True
Skywork-13B,gpuCount,512.0,,True
Skywork-13B,gpuType,NVIDIA A800,,True
Skywork-13B,gpuUtilization,0.46,,True
Skywork-13B,releaseDate,2023-10-30,,True
Skywork-13B,batchSize,16000000.0,Table 3,True
Weblab-10B,flops,3.6e+22,"6ND = 10B*600B * 6 = 3.6e22"" The model was trained on around 600B tokens from a mixture of the following corpora.""See also: https://weblab.t.u-tokyo.ac.jp/en/100%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%BA%E3%83%BB%E6%97%A5%E8%8B%B12%E3%83%B6%E5%9B%BD%E8%AA%9E%E5%AF%BE%E5%BF%9C%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1/",True
Weblab-10B,numParams,10000000000.0,,True
Weblab-10B,numTokens,600000000000.0,"600B tokens , assuming 1 word per token",True
Weblab-10B,releaseDate,2023-08-04,,True
data2vec (language),numParams,705134592.0,,True
data2vec (language),numTokens,3300000000.0,"Section 5.3: ""weadopt the same training setup as BERT (Devlin et al., 2019)by pre-training on the Books Corpus (Zhu et al., 2015) andEnglish Wikipedia data over 1M updates and a batch sizeof 256 sequences.""",True
data2vec (language),releaseDate,2022-01-20,,True
Feedback Transformer,flops,44100000000000000000,,True
Feedback Transformer,numParams,126000000.0,,True
Feedback Transformer,releaseDate,2020-02-21,,True
Mid-level Features,releaseDate,2010-06-13,,True
MAGNeT,numParams,1500000000.0,,True
MAGNeT,releaseDate,2024-01-09,,True
LLaMA-13B (LoRA finetuned),numParams,13000000000.0,,True
LLaMA-13B (LoRA finetuned),releaseDate,2023-05-23,,True
PanGu-Σ,flops,4.67e+23,"It has sparse architecture, so we can't use C=6ND.""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOPhttps://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",True
PanGu-Σ,numParams,1085000000000.0,,True
PanGu-Σ,numTokens,246750000000.0,329B tokens ~= 247B words,True
PanGu-Σ,batchSize,524288.0,,True
PanGu-Σ,trainingTimeDays,2400.0,We develop PanGu-Σ model under the framework of MindSpore 5and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.,True
PanGu-Σ,gpuCount,512.0,,True
PanGu-Σ,gpuType,Huawei Ascend 910,,True
PanGu-Σ,releaseDate,2023-03-20,,True
PanGu-Σ,batchSize,524288.0,"""We train PanGu-Σ with global batch size of 512 with sequence length of 1024 for each sample""",True
CT-MoS + DynamicEval (PTB),releaseDate,2020-12-25,,True
RNS-RNN,flops,3150000000000000,,True
RNS-RNN,numParams,5660000.0,,True
RNS-RNN,releaseDate,2021-09-05,,True
ATLAS,flops,38257920000000000000,"flops = (8) * (123 * 10**12) * (36 * 3600) * (0.3)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from Appendix A.2: ""Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""so 36h for T5""Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for BART models.""from https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_chiptpu chip have peak flops 123 teraflopsso 8 chips have peak flops 123 * 8",True
ATLAS,numParams,11000000000.0,,True
ATLAS,trainingTimeDays,36.0,"Appendix A.2: Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.",True
ATLAS,gpuType,Google TPU v3,,True
ATLAS,releaseDate,2020-05-02,,True
MnasNet-A3,flops,1.5e+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPUAssuming a 33% utilization rate:4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOPHowever, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",True
MnasNet-A3,numParams,5200000.0,,True
MnasNet-A3,numTokens,1280000.0,"""In this paper, we directly perform our architecture search on the ImageNet training set but with fewer training steps (5 epochs). As a common practice, we reserve randomly selected 50K images from the training set as the fixed validation set. """,True
MnasNet-A3,costDollars,4330.99721777198,,True
MnasNet-A3,trainingTimeDays,108.0,,True
MnasNet-A3,gpuCount,256.0,,True
MnasNet-A3,gpuType,Google TPU v3,,True
MnasNet-A3,releaseDate,2019-05-29,,True
LaMDA,flops,3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days= 3.55E+23""From https://arxiv.org/pdf/2201.08239.pdf p.18",True
LaMDA,numParams,137000000000.0,,True
LaMDA,numTokens,1560000000000.0,"""and are pre-trained on 1.56T words of public dialog data and web text""",True
LaMDA,batchSize,256000.0,,True
LaMDA,costDollars,484957.204278073,,True
LaMDA,trainingTimeDays,1385.0,57.7 days * 24,True
LaMDA,gpuCount,1024.0,,True
LaMDA,gpuType,Google TPU v3,,True
LaMDA,gpuUtilization,0.565,,True
LaMDA,releaseDate,2022-02-10,,True
LaMDA,batchSize,256000.0,"""All models were trained with 256K tokens per batch""",True
Consistency Decoder,releaseDate,2023-11-06,,True
BTLM-3B,flops,9.8e+21,2.6b params * 627b tokens * 6 = 9.8e21,True
BTLM-3B,numParams,2600000000.0,,True
BTLM-3B,numTokens,470000000000.0,"627B tokens, equivalent to 470B english words",True
BTLM-3B,batchSize,3932160.0,,True
BTLM-3B,gpuType,Cerebras CS-2,,True
BTLM-3B,releaseDate,2023-09-20,,True
BTLM-3B,batchSize,3932160.0,,True
Galactica,flops,3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",True
Galactica,numParams,120000000000.0,,True
Galactica,batchSize,2000000.0,,True
Galactica,gpuCount,128.0,,True
Galactica,gpuType,NVIDIA A100 SXM4 80 GB,,True
Galactica,releaseDate,2022-11-16,,True
Galactica,batchSize,2000000.0,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)",True
"Mogrifier (d2, MC) + dynamic eval",releaseDate,2019-09-04,,True
SVD in recommender systems,releaseDate,2000-07-14,,True
RL mapping instructions (games),numParams,80940.0,,True
RL mapping instructions (games),numTokens,293.0,"Shown at beginning of section 7Total number of documents is 50, average number of actions per document is 5.86source: https://en.wikipedia.org/wiki/Netflix_Prize",True
RL mapping instructions (games),releaseDate,2009-08-01,,True
NLM,flops,7360000000000000000,,True
NLM,numParams,515000000.00000006,,True
NLM,releaseDate,2021-09-09,,True
HyperCLOVA X,releaseDate,2023-08-24,,True
PAR Transformer Large,releaseDate,2020-09-09,,True
Sparse Wide GPT-3 Small,flops,88400000000000000000,,True
Sparse Wide GPT-3 Small,numParams,1300000000.0,,True
Sparse Wide GPT-3 Small,releaseDate,2023-03-21,,True
Optimized Single-layer Net,releaseDate,2011-04-11,,True
StripedHyena-Hessian-7B,numParams,7000000000.0,,True
StripedHyena-Hessian-7B,releaseDate,2023-11-27,,True
Pythia-70m,numParams,70000000.0,,True
Pythia-70m,releaseDate,2023-04-03,,True
YuLan-Chat-2 (13B),numParams,13000000000.0,,True
YuLan-Chat-2 (13B),releaseDate,2023-08-02,,True
ViT-Huge/14,flops,4.262e+21,from Table 6,True
ViT-Huge/14,numParams,632000000.0,,True
ViT-Huge/14,numTokens,1280000.0,,True
ViT-Huge/14,gpuType,Google TPU v3,,True
ViT-Huge/14,gpuUtilization,0.32,,True
ViT-Huge/14,releaseDate,2020-10-22,,True
AWD-LSTM-DOC (fin) (37M),releaseDate,2018-08-30,,True
LRCN,numParams,142552000.0,,True
LRCN,numTokens,40000.0,,True
LRCN,releaseDate,2014-11-07,,True
DiT-XL/2 + CADS,numParams,675000000.0,,True
DiT-XL/2 + CADS,releaseDate,2023-10-26,,True
2-layer-LSTM+Deep-Gradient-Compression,flops,1340000000000000,,True
2-layer-LSTM+Deep-Gradient-Compression,numParams,6020000.0,,True
2-layer-LSTM+Deep-Gradient-Compression,releaseDate,2017-12-05,,True
DLRM-2021,flops,300000000000000000000,Figure 1https://arxiv.org/abs/2104.05158,True
DLRM-2021,numParams,1000000000000.0,,True
DLRM-2021,costDollars,1094.91786178092,https://bdtechtalks.com/2020/02/03/google-meena-chatbot-ai-language-model/,True
DLRM-2021,releaseDate,2020-07-01,,True
CRF-RNN,releaseDate,2015-02-11,,True
Cloob,numTokens,15000000.0,"[Image-text pairs]""To be comparable to the CLIP results, we use the same subset of 15 million samples from the YFCC100M dataset""",True
Cloob,releaseDate,2021-10-21,,True
RHN+HSG(depth=40),releaseDate,2018-05-23,,True
OntoProtein,numParams,420000000.0,,True
OntoProtein,releaseDate,2022-01-23,,True
CD-GraB (WT2),releaseDate,2023-02-02,,True
RSM,releaseDate,2019-05-28,,True
GANs,flops,518400000000000000,"From https://openai.com/blog/ai-and-compute/ Appendix""Less than 0.006 pfs-days""(86400*10^15*0.006)Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",True
GANs,numTokens,60000.0,"""We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].""MNIST has 60k images https://en.wikipedia.org/wiki/MNIST_databaseTFD seems to have 2925 examples (?)https://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdfCIFAR-10 has 60k imageshttps://www.cs.toronto.edu/~kriz/cifar.html",True
GANs,costDollars,6.08698841694225,,True
GANs,releaseDate,2014-06-10,,True
Deep RNN,releaseDate,2013-12-11,,True
WeNet (PTB),releaseDate,2019-04-08,,True
Hybrid H3-355M,numParams,355000000.0,,True
Hybrid H3-355M,releaseDate,2022-12-28,,True
Dropout (CIFAR),flops,4268700000000000,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p171.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",True
Dropout (CIFAR),trainingTimeDays,1.5,90 minutes,True
Dropout (CIFAR),gpuType,NVIDIA GeForce GTX 580,,True
Dropout (CIFAR),releaseDate,2012-06-03,,True
PaLI,flops,5.1e+22,"""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days""275 teraFLOP/s * 1024 * 7 * 24 * 3600 * 0.3 (utilization assumption) = 5.1e22",True
PaLI,numParams,16900000000.0,,True
PaLI,trainingTimeDays,168.0,7 days,True
PaLI,gpuCount,1024.0,,True
PaLI,gpuType,Google TPU v4,,True
PaLI,releaseDate,2022-09-14,,True
ResNeXt-50,numParams,25000000.0,,True
ResNeXt-50,releaseDate,2016-11-16,,True
ZymCTRL,flops,5.05e+21,"""We trained for 179,000 steps on 48 NVIDIA A100s 80GB for about 15,000 GPU hours""15000  * 3600 * 312 teraFLOPS * 0.3 (utilization assumption) = 5.05e21",True
ZymCTRL,numParams,738000000.0,,True
ZymCTRL,gpuType,NVIDIA A100,,True
ZymCTRL,releaseDate,2022-12-01,,True
OPT-175B,flops,4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""",True
OPT-175B,numParams,175000000000.0,,True
OPT-175B,numTokens,135000000000.0,"""The training data contains 180B tokens corresponding to 800 GB of data""1 token ~ 0.75 words",True
OPT-175B,batchSize,2000000.0,,True
OPT-175B,costDollars,1654082.50447642,,True
OPT-175B,trainingTimeDays,793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hourshttps://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",True
OPT-175B,gpuCount,1024.0,,True
OPT-175B,gpuType,NVIDIA A100 SXM4 80 GB,,True
OPT-175B,gpuUtilization,0.47115,,True
OPT-175B,releaseDate,2022-05-02,,True
OPT-175B,batchSize,2000000.0,Table 1,True
MADLAD-400 10B,flops,1.605e+22,6ND = 10.7B * 250B = 1.6e22'MADLAD-400-10B-MT is a multilingual machine translation model based on the T5 architecture that was trained on 250 billion tokens covering over 450 languages using publicly available data. '10.7B  params from appendix A.8,True
MADLAD-400 10B,numParams,10700000000.0,,True
MADLAD-400 10B,numTokens,3000000000000.0,"Assuming 1 word per token we have 3T words 'We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages.'",True
MADLAD-400 10B,releaseDate,2023-09-09,,True
Grown to Prune Two-layer stacked LSTM,releaseDate,2020-07-30,,True
ReLU (LFW),releaseDate,2010-06-15,,True
Stanley (DARPA Grand Challenge 2),releaseDate,2006-01-01,,True
NTM,releaseDate,2014-12-10,,True
Relational Memory Core,releaseDate,2018-06-05,,True
Selfish-RNN (AWD-LSTM-MoS),releaseDate,2021-01-22,,True
Ankh_base,flops,2.6e+21,Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf,True
Ankh_base,numParams,450000000.0,,True
Ankh_base,gpuType,Google TPU v4,,True
Ankh_base,releaseDate,2023-01-16,,True
ViT-G (model soup),flops,3.4e+21,"This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon. Fine-tuning compute is likely minor in comparision.",True
ViT-G (model soup),numParams,1843000000.0,,True
ViT-G (model soup),releaseDate,2022-03-10,,True
RNN + char3-MS-vec,releaseDate,2019-07-16,,True
Transformer-XL+AdamP,numParams,257000000.0,,True
Transformer-XL+AdamP,releaseDate,2020-06-15,,True
TRIMELMext (247M),flops,31200000000000000000,,True
TRIMELMext (247M),numParams,247000000.00000003,,True
TRIMELMext (247M),releaseDate,2022-05-25,,True
Gemini Nano-2,numParams,3250000000.0,,True
Gemini Nano-2,gpuType,Google TPU v5e,,True
Gemini Nano-2,releaseDate,2023-12-19,,True
RNNsearch-50*,flops,1555200000000000000,From https://openai.com/blog/ai-and-compute/ Appendix.0.018 pfs-days(86400*10^15*0.018)252 hours in a Quadro K-6000 GPU,True
RNNsearch-50*,numTokens,348000000.0,"[WORDS]""WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), newscommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size ofthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",True
RNNsearch-50*,costDollars,81.4758557901812,,True
RNNsearch-50*,releaseDate,2014-09-01,,True
Image-to-image cGAN,releaseDate,2016-11-21,,True
TripletRes,releaseDate,2019-08-13,,True
Sequence-based pattern recognition,releaseDate,1955-03-01,,True
Polyglot-Ko-12.8B,flops,1.28e+22,trained for 167 billion tokens167b * 12.8b * 6 = 1.28e22,True
Polyglot-Ko-12.8B,numParams,12898631680.0,,True
Polyglot-Ko-12.8B,numTokens,96000000000.0,"863 GB of Korean language data after processing~111m Korean words per GB, so ~95,793,000,000 or ~96B wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",True
Polyglot-Ko-12.8B,batchSize,554817.0,,True
Polyglot-Ko-12.8B,gpuType,NVIDIA A100,,True
Polyglot-Ko-12.8B,releaseDate,2023-06-04,,True
Polyglot-Ko-12.8B,batchSize,554817.0,"from HuggingFace: ""Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework.""from the paper: ""The overall batch size was maintained through the use of gradient accumulation steps (GAS). The model was trained for a total of 301,000 steps.""GAS is a technique to train larger batches if you have limited memory. I don't think this text says anything in particular about whether the batch sizes changed over the course of training? 167B / 301k = 554,817",True
SPALM + RelationLM,numParams,124000000.0,,True
SPALM + RelationLM,releaseDate,2022-01-24,,True
TCN (13M),releaseDate,2018-02-15,,True
Semantic Taxonomy Induction,numParams,100.0,,True
Semantic Taxonomy Induction,numTokens,850750.0,"[Classification task]The labeled training set isconstructed by labeling the collected feature vectors as positive “known hypernym” or negative“known non-hypernym” examples using WordNet2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairswere labeled as negative training examples.800,828 + 49,922 = 850750",True
Semantic Taxonomy Induction,releaseDate,2006-07-07,,True
Recursive sentiment autoencoder,releaseDate,2011-07-01,,True
ProGen2-xlarge,flops,1.35e+22,"Estimate 1:""350,000 steps x 1m batch size x 6.4 B “connections” x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)Steps and batches from Table 1. FLOP estimate: 1.3e22Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdfFLOP estimate: 1.4e22Geometric mean = 1.35e22 FLOP",True
ProGen2-xlarge,numParams,6400000000.0,,True
ProGen2-xlarge,gpuType,Google TPU v3,,True
ProGen2-xlarge,releaseDate,2022-06-27,,True
Theseus,flops,40,"The ""training"" consists on the mouse running around and checking each wall.",True
Theseus,numParams,40.0,,True
Theseus,numTokens,40.0,Each wall Theseus bumps into is a datapoint,True
Theseus,releaseDate,1950-07-02,,True
T5-11B,flops,3.3e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdfTable 4, 4.05e22update: 3.3e22 per FLAN paper from Google https://arxiv.org/pdf/2210.11416.pdf",True
T5-11B,numParams,11000000000.0,,True
T5-11B,numTokens,150000000000.0,"""This produces a collection of text that is not onlyorders of magnitude larger than most data sets used for pre-training (about 750 GB) but alsocomprises reasonably clean and natural English text. We dub this data set the “ColossalClean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""750GB * 200M word/GB = 1.5e11",True
T5-11B,batchSize,65536.0,,True
T5-11B,costDollars,105686.200097103,,True
T5-11B,trainingTimeDays,481.9,4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hourshttps://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29,True
T5-11B,gpuCount,512.0,,True
T5-11B,gpuType,Google TPU v3,,True
T5-11B,gpuUtilization,0.3707,,True
T5-11B,releaseDate,2019-10-23,,True
T5-11B,batchSize,65536.0,"""We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we “pack” multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens""",True
Nemotron-3-8B,flops,1.8e+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k""This model was trained on a dataset containing 3.8 Trillion tokens of text""8 billion * 3.8 trillion * 6 = 1.8e23Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23",True
Nemotron-3-8B,numParams,8000000000.0,,True
Nemotron-3-8B,trainingTimeDays,456.0,19 days,True
Nemotron-3-8B,gpuCount,1024.0,,True
Nemotron-3-8B,gpuType,NVIDIA A100,,True
Nemotron-3-8B,gpuUtilization,0.34,,True
Nemotron-3-8B,releaseDate,2023-11-15,,True
fastText,releaseDate,2016-07-06,,True
RetinaNet-R101,flops,2065392000000000000,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""NVIDIA M40 GPU35*60**2*0.3*8*6.83E+12 = 2.07e18",True
RetinaNet-R101,numParams,53000000.0,,True
RetinaNet-R101,numTokens,135000.0,trainval135k split,True
RetinaNet-R101,trainingTimeDays,35.0,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""NVIDIA M40 GPU35*60**2*0.3*8*6.83E+12 = 2.07e18",True
RetinaNet-R101,gpuCount,8.0,,True
RetinaNet-R101,gpuType,NVIDIA M40,,True
RetinaNet-R101,releaseDate,2017-08-07,,True
BEIT-3,numParams,1900000000.0,,True
BEIT-3,releaseDate,2022-08-22,,True
L_UL-seq,numParams,247000000.00000003,,True
L_UL-seq,releaseDate,2019-08-12,,True
Distilled Grandmaster,flops,1.035671832e+22,"10356718320000000065536 FLOP""Board states 𝑠 are encoded as FEN strings which we convert to fixed-length strings of 77 characters where the ASCII-code of each character is one token."" so 77 tokens for board + 1 token for action ""For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates""so input is 78 tokens for each action-valuenumber of tokens = 1194960000000.0The model is dense transformer"" We train for 10 millionsteps, which corresponds to 2.67 epochs for a batchsize of 4096 with 15.32B data points "", but in appendix A.2 there is mention of 5.35 epochsI have used higher value from 5.35 and 2.67,Probably final they trained model for 5.35 epochs and used checkpoint from 2.67 as final model.aproximation 6ND for 5.35 epochs = 6*270e6*1194960000000.0 * 5.35 =  10356718320000000065536",True
Distilled Grandmaster,numParams,270000000.0,,True
Distilled Grandmaster,numTokens,15320000000.0,"15.32BTraining is supervised. I count each action-value (board state, action and numeric evaluation of state from Stockfish 16) as 1 data point.""For our largest training dataset, based on 10M games, this results in 15.32B action-value estimates""",True
Distilled Grandmaster,releaseDate,2024-02-07,,True
LLaMA-13B,flops,4.55e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22from paper, Llama-7B took 135,168 GPU hours using A100s312 trillion * 135,168 * 3600 * 0.3 = 4.55e22 FLOP",True
LLaMA-13B,numParams,13000000000.0,,True
LLaMA-13B,batchSize,4000000.0,,True
LLaMA-13B,gpuType,NVIDIA A100,,True
LLaMA-13B,releaseDate,2023-02-27,,True
LLaMA-13B,batchSize,4000000.0,,True
AWD-LSTM+Behaviorial-Gating,releaseDate,2019-08-31,,True
DOC + Finetune∗ + Partial Shuffle (WT2),numParams,67300000.0,,True
DOC + Finetune∗ + Partial Shuffle (WT2),releaseDate,2019-03-11,,True
Transformer+Recurrent Windows of Context,flops,117000000000000000000,,True
Transformer+Recurrent Windows of Context,numParams,124000000.0,,True
Transformer+Recurrent Windows of Context,releaseDate,2020-08-16,,True
Megatron-LM (2.5B),releaseDate,2019-09-17,,True
Nanbeige-16B,flops,2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.16 billion * 2.5 trillion * 6 = 2.4e23",True
Nanbeige-16B,numParams,16000000000.0,,True
Nanbeige-16B,releaseDate,2023-11-01,,True
retrieval-quality-kNN-LMs,numParams,247000000.00000003,,True
retrieval-quality-kNN-LMs,releaseDate,2022-10-28,,True
OPT-125M (finetuned),numParams,125000000.0,,True
OPT-125M (finetuned),releaseDate,2022-06-21,,True
TRIMELMext (7M),numParams,7000000.0,,True
TRIMELMext (7M),releaseDate,2022-05-25,,True
RefineNet,releaseDate,2016-11-20,,True
BaGuaLu,numParams,173900000000000.0,,True
BaGuaLu,numTokens,13200000000.0,"17.5B tokens (in English, this is approximately 13.1B words, but the conversion may be different in Chinese) and 60.5M images.",True
BaGuaLu,releaseDate,2022-03-28,,True
Mono3D++,flops,4856060160000000000,"(4) * (6.691 * 10**12) * (168* 3600) * (0.3) = (num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) = training time - about one week = 168 hours from""It takes about one week to train the 2D bounding box net-work, and two hours for the orientation/3D scale networkon KITTI with 4 TITAN-X GPUs. The landmark detector istrained on Pascal3D. The training process for the monocu-lar depth estimation network is unsupervised using KITTIstereo-pairs, which takes around 5 to 12 hours dependingon the amount of data available. ""gpu flops - FP32 (float) 6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632",True
Mono3D++,numTokens,7481.0,"""We evaluate our method on the KITTI object detection benchmark. This dataset contains 7, 481 training images """,True
Mono3D++,trainingTimeDays,168.0,"""about one week"" from section 3.4 ",True
Mono3D++,gpuCount,4.0,,True
Mono3D++,gpuType,NVIDIA GTX Titan X,,True
Mono3D++,releaseDate,2019-01-11,,True
XVERSE-65B-2,flops,1.24800000000001e+24,C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP,True
XVERSE-65B-2,numParams,65000000000.0,,True
XVERSE-65B-2,numTokens,2720000000000.0,"Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.Assume 0.85 words per token on average for the mix of languages.",True
XVERSE-65B-2,trainingTimeDays,4096.0,November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours.,True
XVERSE-65B-2,releaseDate,2023-12-08,,True
LLaMA-7B,flops,4.02e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOPfrom paper, Llama-7B took 82,432 GPU hours using A100s312 trillion * 82,432 * 3600 * 0.3 = 2.78e22 FLOP",True
LLaMA-7B,numParams,6700000000.0,,True
LLaMA-7B,numTokens,750000000000.0,1 trillion tokens * 0.75 words/token = 750 billion words,True
LLaMA-7B,batchSize,4000000.0,,True
LLaMA-7B,gpuType,NVIDIA A100,,True
LLaMA-7B,gpuUtilization,0.43,,True
LLaMA-7B,releaseDate,2023-02-24,,True
LLaMA-7B,batchSize,4000000.0,,True
Deconvolutional Network,releaseDate,2010-06-13,,True
Llama 2-13B,flops,1.6e+23,13 billion * 2 trillion * 6 = 1.6e23,True
Llama 2-13B,numParams,13000000000.0,,True
Llama 2-13B,numTokens,1500000000000.0,2 trillion tokens ~= 1.5 trillion words,True
Llama 2-13B,batchSize,4000000.0,,True
Llama 2-13B,gpuType,NVIDIA A100 SXM4 80 GB,,True
Llama 2-13B,releaseDate,2023-07-18,,True
Llama 2-13B,batchSize,4000000.0,,True
Wu Dao - Wen Hui,flops,116121600000000000000,64 Nvidia V100 GPUs for 2.5 days64 GPUs * 2.8e13 FLOP/s /GPU * 2.5*24*60*60s* 0.3 [utilization rate],True
Wu Dao - Wen Hui,numParams,11300000000.0,,True
Wu Dao - Wen Hui,releaseDate,2021-03-01,,True
SmooCT,flops,69000000000000000,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""Assume an Intel i7 so 400e9 FLOP/s.6.9e16 = 400e9*60*60*48",True
SmooCT,numTokens,12000000000.0,"""Each three-player agentwas trained for about 12 billion episodes""An episode seems to be a round of betting.",True
SmooCT,trainingTimeDays,48.0,,True
SmooCT,releaseDate,2014-07-01,,True
Mitosis,flops,137000000000000000,"""Training each network requires one day of computation with an optimized GPUimplementation""Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get3600*24*1.58E+12 = 1.37E+17 FLOP",True
Mitosis,numParams,37230.0,,True
Mitosis,numTokens,1000000.0,"The dataset is built in two stages. First a classifier is trained on small sample, and used to curate a more representative larger dataset.The final dataset has 1M instances""We build the actual training set, composed by 1 million instances, which includesall mitosis pixels (6.6% of the training instances). The remaining 95.4% is sampledfrom non-mitosis pixels by assigning to each pixel p a weight D(p).""",True
Mitosis,costDollars,2.00443851959482,,True
Mitosis,releaseDate,2013-09-22,,True
ENAS,flops,20099999999999996,,True
ENAS,numParams,24000000.0,,True
ENAS,releaseDate,2018-02-09,,True
GLM-130B,flops,3.778e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 30% utilization = 3.778*10^23 FLOPhttps://www.wolframalpha.com/input?i=312+teraflops+*+96+*+8+*+2+months+*+30%25utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""",True
GLM-130B,numParams,130000000000.0,,True
GLM-130B,trainingTimeDays,1440.0,see compute notes,True
GLM-130B,gpuCount,768.0,,True
GLM-130B,gpuType,NVIDIA A100 SXM4 40 GB,,True
GLM-130B,gpuUtilization,0.433,,True
GLM-130B,releaseDate,2022-08-04,,True
LSTM + dynamic eval,numParams,50000000.0,,True
LSTM + dynamic eval,releaseDate,2017-09-21,,True
GLM-10B,flops,3.79e+22,,True
GLM-10B,numParams,10000000000.0,,True
GLM-10B,batchSize,524288.0,,True
GLM-10B,releaseDate,2021-03-18,,True
GLM-10B,batchSize,524288.0,"""The models are trained on 64 V100 GPUs for 200K steps withbatch size of 1024 and maximum sequence lengthof 512""",True
AFP+FPI (PTB),flops,227000000000000,,True
AFP+FPI (PTB),numParams,2040000.0,,True
AFP+FPI (PTB),releaseDate,2021-06-04,,True
Adversarial + AWD-LSTM-MoS + partial shuffled,releaseDate,2019-06-10,,True
Jurassic-1-Jumbo,flops,3.7e+23,see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,True
Jurassic-1-Jumbo,numParams,178000000000.0,,True
Jurassic-1-Jumbo,numTokens,225000000000.0,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""1 token ~ 0.75 words",True
Jurassic-1-Jumbo,batchSize,3200000.0,,True
Jurassic-1-Jumbo,costDollars,805277.008758257,,True
Jurassic-1-Jumbo,releaseDate,2021-08-11,,True
Jurassic-1-Jumbo,batchSize,3200000.0,"""Namely, we used a base learning rate of 1.2 × 10−4 and 0.6 × 10−4 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.""",True
mT0-13B,numParams,13000000000.0,,True
mT0-13B,numTokens,20000000000.0,"per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB if approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",True
mT0-13B,releaseDate,2022-11-03,,True
Alleviated TOI 10 (WT2),releaseDate,2019-09-18,,True
Word-Independent-SRNN+KN5,numParams,5320000.0,,True
Word-Independent-SRNN+KN5,releaseDate,2017-03-23,,True
AR-LDM,flops,510000000000000000000,8 NVIDIA A100 GPUs for 8 days,True
AR-LDM,numParams,1500000000.0,,True
AR-LDM,trainingTimeDays,194.0,8 NVIDIA A100 GPUs for 8 days,True
AR-LDM,gpuType,NVIDIA A100,,True
AR-LDM,releaseDate,2022-11-20,,True
Zoneout + Variational LSTM (WT2),flops,16800000000000000,,True
Zoneout + Variational LSTM (WT2),numParams,21000000.0,,True
Zoneout + Variational LSTM (WT2),releaseDate,2016-09-26,,True
mPLUG-Owl2,numParams,7120000000.0,,True
mPLUG-Owl2,numTokens,400000000.0,,True
mPLUG-Owl2,releaseDate,2023-11-07,,True
StableLM-2-1.6B,flops,1.92e+22,6 * 1.6B * 2T = 19200000000000000000000,True
StableLM-2-1.6B,numParams,1600000000.0,,True
StableLM-2-1.6B,numTokens,2000000000000.0,"""model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.""assuming 1 word per token ",True
StableLM-2-1.6B,gpuCount,512.0,,True
StableLM-2-1.6B,gpuType,NVIDIA A100 SXM4 40 GB,,True
StableLM-2-1.6B,releaseDate,2024-01-18,,True
FinGPT-13B,numParams,13000000000.0,,True
FinGPT-13B,releaseDate,2023-10-07,,True
Show-1,gpuType,NVIDIA A100,,True
Show-1,releaseDate,2023-09-27,,True
dense-IndRNN+dynamic eval,numParams,44100000.0,,True
dense-IndRNN+dynamic eval,releaseDate,2019-10-11,,True
DALL-E mega,flops,228527308800000000000,flops = (128) * (1230 * 10**9) * (1344 * 3600) * (0.3) = 2.3e20(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact,True
DALL-E mega,trainingTimeDays,1344.0,from https://huggingface.co/dalle-mini/dalle-mega#environmental-impact,True
DALL-E mega,gpuCount,128.0,,True
DALL-E mega,gpuType,Google TPU v3,,True
DALL-E mega,releaseDate,2022-06-28,,True
Culturome,releaseDate,2010-12-16,,True
Retrieval-Augmented Generator,numParams,626000000.0,,True
Retrieval-Augmented Generator,gpuType,NVIDIA Tesla V100 PCIe 32 GB,,True
Retrieval-Augmented Generator,releaseDate,2020-05-22,,True
BellKor 2007,numTokens,100480507.0,"The training data set consists of 100,480,507ratings",True
BellKor 2007,releaseDate,2009-09-21,,True
Tacotron 2,numTokens,340000.0,"""We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker.""13,680 words/hour * 24.6 = 336528 words",True
Tacotron 2,releaseDate,2017-12-19,,True
GPT-Neo-2.7B (finetuned on PTB),releaseDate,2021-03-21,,True
Stacked hourglass network,releaseDate,2016-09-17,,True
PointNet,releaseDate,2016-12-02,,True
SantaCoder,flops,2.1e+21,Each training run takes 3.1 days to complete on 96 Tesla V100 GPUs for a total of 1.05 × 10^21 FLOPs. The final model described in Section 6.2 uses twice the amount of compute.,True
SantaCoder,numParams,1100000000.0,,True
SantaCoder,trainingTimeDays,150.0,"Their initial training runs took 3.1 days. The final training run was run for twice as many iterations with ""all other hyper-parameters the same"" and used twice as much compute as this. So likely 6 days or ~150 hours, but they don't explicitly say whether they used the same hardware.",True
SantaCoder,gpuType,NVIDIA V100,,True
SantaCoder,releaseDate,2023-01-09,,True
Jurassic-X,numParams,7000000000.0,,True
Jurassic-X,releaseDate,2022-05-03,,True
Zip CNN,flops,43372117520,"Its a deep CNN so we assume a backward-forward ratio of 2:1""The network was trained for 23passes through the training set (167,693 pattern presentations).""",True
Zip CNN,numParams,9760.0,,True
Zip CNN,numTokens,7291.0,"The digits were writtenby many different people, using a great variety of sizes, writing styles,and instruments, with widely varying amounts of care; 7291 examplesare used for training the network and 2007 are used for testing the generalization performance",True
Zip CNN,releaseDate,1989-12-01,,True
Word2Vec (large),flops,38880000000000000,"From https://openai.com/blog/ai-and-compute/ Appendix.""less than 0.00045 pfs days""(86400*10^15*0.00045)",True
Word2Vec (large),numParams,692000000.0,,True
Word2Vec (large),numTokens,33000000000.0,"""For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K""",True
Word2Vec (large),costDollars,0.548133695223449,,True
Word2Vec (large),releaseDate,2013-10-16,,True
"Mogrifier (d2, MoS2, MC) + dynamic eval",numParams,35000000.0,,True
"Mogrifier (d2, MoS2, MC) + dynamic eval",releaseDate,2019-09-04,,True
Learning deep architectures,releaseDate,2009-11-15,,True
GLM-10B-bidirectional,releaseDate,2021-03-18,,True
GNoME for crystal discovery,numParams,16240000.0,,True
GNoME for crystal discovery,gpuCount,4.0,,True
GNoME for crystal discovery,gpuType,Google TPU v4,,True
GNoME for crystal discovery,releaseDate,2023-11-29,,True
1-layer-LSTM,numParams,86500000.0,,True
1-layer-LSTM,releaseDate,2020-07-13,,True
OPT-IML (175B),numParams,175000000000.0,,True
OPT-IML (175B),gpuType,NVIDIA A100,,True
OPT-IML (175B),releaseDate,2022-12-22,,True
DeepFace,releaseDate,2014-06-23,,True
ESM2-650M,flops,7.560000000001e+21,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 4.4e21 FLOPfrom the paper's Supplementary Materials: ""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""8 days x 512 V100s x an imputed 30% utilization"": 1.3e22 FLOPGeometric mean: 7.56e21 FLOP",True
ESM2-650M,numParams,650000000.0,,True
ESM2-650M,trainingTimeDays,192.0,,True
ESM2-650M,gpuCount,512.0,,True
ESM2-650M,gpuType,NVIDIA V100,,True
ESM2-650M,releaseDate,2022-07-21,,True
NAS+ESS (156M),flops,2890000000000000000,,True
NAS+ESS (156M),numParams,156000000.0,,True
NAS+ESS (156M),releaseDate,2020-05-06,,True
GPT-2-Medium+Pixelfly,flops,834000000000000000000,,True
GPT-2-Medium+Pixelfly,numParams,203000000.0,,True
GPT-2-Medium+Pixelfly,releaseDate,2021-11-30,,True
Wu Dao 2.0,numParams,1750000000000.0,,True
Wu Dao 2.0,releaseDate,2021-05-31,,True
Samuel Neural Checkers,flops,428400000,"""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""",True
Samuel Neural Checkers,numParams,16.0,,True
Samuel Neural Checkers,numTokens,53000.0,"Based on number of board positionsAt the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much largernumber of positions by means of the culling techniquesdescribed. While this is still far from the number whichwould tax the listing and searching procedures used inthe program, rough estimates, based on the frequencywith which the saved boards are utilized during normalplay (these figures being tabulated automatically), indicate that a library tape containing at least 20 times thepresent number of board positions would be needed toimprove the midgame play significantly. At the presentrate of acquisition of new positions this would requirean inordinate amount of play and, consequently, ofmachine time.",True
Samuel Neural Checkers,releaseDate,1959-07-01,,True
TigerBot-70B,flops,1.02e+24,~1.02e24Tigerobo did ~2.1e23 additional pre-training. We estimated Llama 2 was trained on 8.1e23 FLOP.,True
TigerBot-70B,numParams,70000000000.0,,True
TigerBot-70B,batchSize,4000000.0,,True
TigerBot-70B,releaseDate,2023-09-06,,True
TigerBot-70B,batchSize,4000000.0,"from paper:""We pretrained TigerBot models using a global batch size (GBS) of 4M tokens, while fine-tuned models with a GBS as small as 100–400k tokens""It's also based on pretrained Llama 2, which also used a batch size of 4M",True
Two-stream ConvNets for action recognition,releaseDate,2014-06-09,,True
SparseOPT-175B,flops,1.58e+23,,True
SparseOPT-175B,numParams,87500000000.0,,True
SparseOPT-175B,releaseDate,2023-01-02,,True
MuZero VP9,releaseDate,2022-02-14,,True
Luminous Sparse,numParams,2600000000.0,,True
Luminous Sparse,releaseDate,2022-11-16,,True
Gradient Boosting Machine,releaseDate,2001-10-01,,True
AWD-LSTM-DRILL + dynamic evaluation† (PTB),releaseDate,2019-05-14,,True
M4-50B,numParams,50000000000.0,,True
M4-50B,releaseDate,2019-10-11,,True
SPALM + kNN,releaseDate,2021-04-26,,True
LIRA,numParams,100000.0,,True
LIRA,numTokens,10000.0,,True
LIRA,releaseDate,2004-07-30,,True
Agile Soccer Robot,trainingTimeDays,240.0,"14+158+68 hours:""Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-playtook 68 hours (see Appendix B for details)""",True
Agile Soccer Robot,releaseDate,2023-04-26,,True
STeLLA,releaseDate,1963-06-01,,True
Monarch-GPT-2-Small,releaseDate,2022-04-01,,True
Firefly,releaseDate,2023-03-21,,True
2-layer skip-LSTM + dropout tuning (PTB),releaseDate,2018-05-23,,True
PyramidNet,numParams,26000000.0,,True
PyramidNet,releaseDate,2017-09-06,,True
CICERO,releaseDate,2022-11-22,,True
Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),numParams,33000000.0,,True
Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),releaseDate,2020-09-29,,True
CodeGen2.5,flops,5.9e+22,"7B parameters, trained on 1.4T tokens7 billion * 1.4 trillion * 6 = 5.9e22",True
CodeGen2.5,numParams,7000000000.0,,True
CodeGen2.5,releaseDate,2023-07-06,,True
Ankh_large,flops,6.5e+21,Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf,True
Ankh_large,numParams,1150000000.0,,True
Ankh_large,gpuType,Google TPU v4,,True
Ankh_large,releaseDate,2023-01-16,,True
Rotation,numParams,86000000.0,,True
Rotation,releaseDate,2018-03-21,,True
wave2vec 2.0 LARGE,flops,1.9e+21,"From surveying the authors:We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)Assume 40% utilization based on default for non-Language domain (https://epochai.org/blog/estimating-training-compute)64 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h~= 1.9E+21 FLOP",True
wave2vec 2.0 LARGE,numParams,317000000.0,,True
wave2vec 2.0 LARGE,numTokens,727776000.0,"pg 4, section 4.1""As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.""53.2k h * 13,680 words/h = 727776000 words",True
wave2vec 2.0 LARGE,costDollars,1569.38226855265,,True
wave2vec 2.0 LARGE,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
wave2vec 2.0 LARGE,releaseDate,2020-10-22,,True
Xception,flops,436000000000000000000,60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20,True
Xception,numParams,22855952.0,,True
Xception,numTokens,350000000.0,"""JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k""",True
Xception,costDollars,1961.33825417794,,True
Xception,trainingTimeDays,720.0,"""while the JFT experiments took over one month each.""",True
Xception,gpuCount,60.0,,True
Xception,gpuType,NVIDIA Tesla K80,,True
Xception,releaseDate,2016-10-07,,True
LDA,releaseDate,2003-02-02,,True
OPT-13B,numParams,13000000000.0,,True
OPT-13B,releaseDate,2022-06-21,,True
E-SPA,numParams,243000000.00000003,,True
E-SPA,releaseDate,2023-02-20,,True
GPT-Neo-2.7B,flops,6.48e+21,,True
GPT-Neo-2.7B,numParams,2700000000.0,,True
GPT-Neo-2.7B,releaseDate,2021-03-21,,True
ASE+ACE,numParams,324.0,,True
ASE+ACE,releaseDate,1983-09-01,,True
UDSMProt,flops,890000000000000000,"170k sequences, each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.170k * 1024 * 30 * 28.3 * 6 = 8.9e17.",True
UDSMProt,numParams,28303800.0,,True
UDSMProt,releaseDate,2019-09-04,,True
6-Layer-Tensor-Transformer+AdaHessian,flops,1580000000000000000,,True
6-Layer-Tensor-Transformer+AdaHessian,numParams,85300000.0,,True
6-Layer-Tensor-Transformer+AdaHessian,releaseDate,2020-06-01,,True
GPT3-6.7B + muP,flops,1.28e+22,,True
GPT3-6.7B + muP,numParams,6700000000.0,,True
GPT3-6.7B + muP,releaseDate,2022-03-07,,True
PAGnol-XL,flops,259200000000000000000,"They report their compute directly.From section 8: ""About 62k GPU-hours on the Jean Zay HPC Cluster."" Jean Zay uses both A100 and V100 GPUs, and maybe other stuff as well?Note they explicitly call out V100 in their Appendix A.https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/",True
PAGnol-XL,numParams,1500000000.0,,True
PAGnol-XL,numTokens,24000000000.0,Section 4.1: 32G tokens => 32e9*0.75 = 24e9 words,True
PAGnol-XL,gpuType,NVIDIA Tesla V100 SXM2,,True
PAGnol-XL,releaseDate,2021-10-16,,True
PointNet++,releaseDate,2017-06-07,,True
genCNN + dyn eval,flops,73000000000000000,,True
genCNN + dyn eval,numParams,8000000.0,,True
genCNN + dyn eval,releaseDate,2015-03-17,,True
DeepStack,flops,14463360000000000000,"The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack’s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.Calculation:6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",True
DeepStack,numParams,2500000.0,,True
DeepStack,numTokens,10000000.0,"""The turn network was trained by solving 10 million randomly generated poker turngames. These turn games used randomly generated ranges, public cards, and a random potsize (10).""",True
DeepStack,costDollars,0.0008494643652848,,True
DeepStack,trainingTimeDays,218.0,from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs,True
DeepStack,gpuCount,20.0,,True
DeepStack,releaseDate,2017-01-06,,True
"Segatron XL base, M=384",releaseDate,2020-04-30,,True
BERT-RBP,flops,140000000000000000000,"See DNABert entry:""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""Assuming FP16 and 30% utilizationCalculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",True
BERT-RBP,numParams,110000000.0,,True
BERT-RBP,releaseDate,2022-04-07,,True
CapsNet (MultiMNIST),numParams,11360000.0,,True
CapsNet (MultiMNIST),releaseDate,2017-10-26,,True
Scatterbrain,releaseDate,2021-10-28,,True
Fuyu-8B,numParams,8000000000.0,,True
Fuyu-8B,releaseDate,2023-10-17,,True
Incoder-6.7B,flops,3.00001e+21,"per table 5, required 3 zettaflop (3e21) to train.also, ""INCODER-6.7B was trained on 248 V100 GPUs for 24 days""hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.",True
Incoder-6.7B,numParams,6700000000.0,,True
Incoder-6.7B,trainingTimeDays,576.0,24,True
Incoder-6.7B,gpuType,NVIDIA V100,,True
Incoder-6.7B,releaseDate,2023-04-09,,True
Byte-mLSTM+emb+WN+VD,numParams,46000000.0,,True
Byte-mLSTM+emb+WN+VD,releaseDate,2016-09-26,,True
Shortformer,flops,3040000000000000000,,True
Shortformer,numParams,24000000.0,,True
Shortformer,releaseDate,2020-12-31,,True
Falcon-40B,flops,2.4e+23,C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch),True
Falcon-40B,numParams,40000000000.0,,True
Falcon-40B,numTokens,750000000000.0,1000B tokens ~= 750B words,True
Falcon-40B,batchSize,2359296.0,,True
Falcon-40B,trainingTimeDays,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""""Training started in December 2022 and took two months.""",True
Falcon-40B,gpuCount,384.0,,True
Falcon-40B,gpuType,NVIDIA A100,,True
Falcon-40B,gpuUtilization,0.3864,,True
Falcon-40B,releaseDate,2023-03-15,,True
Falcon-40B,batchSize,2359296.0,"Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.""All Falcon models are pretrained with a 2,048 sequence length""https://arxiv.org/pdf/2311.16867.pdf",True
Imagen,flops,1.46e+22,"256 TPU v4 chips for 64x64, for 4 days128 TPU v4 chips for 64->256, for 2 days128 TPU v4 chips for 256->1024, for 2 days256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP",True
Imagen,numParams,3000000000.0,,True
Imagen,numTokens,860000000.0,"""We train on a combination of internal datasets, with ≈ 460Mimage-text pairs, and the publicly available Laion dataset [61], with ≈ 400M image-text pairs.""",True
Imagen,trainingTimeDays,96.0,4 days,True
Imagen,gpuCount,256.0,,True
Imagen,gpuType,Google TPU v4,,True
Imagen,releaseDate,2022-05-23,,True
ResNet-RS,flops,1.763328e+22,"(350) * (128000000000) * (1312 * 10**5) * 3 = 17633280000000000000000(epochs) * (inference FLOP) * (dataset size) * (constant to account for backpropagation)from 4.2 ""Our training method closely matches that of EfficientNet, where we train for 350 epochs, but with a few small differences""350 epochs from description of Table 8 in appendix C",True
ResNet-RS,numParams,192000000.0,,True
ResNet-RS,numTokens,131200000.0,"1.2M + 130M = 131.2M ""In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfficientNet-B5 when co-trained on ImageNet and an additional 130M pseudo-labeled images.""""We train ResNets-RS on the combination of 1.2M labeled ImageNet images and 130M pseudo-labeled images, in a similar fashion to Noisy Studen"" ""We use the same dataset of 130M images pseudo-labeled as Noisy Student""",True
ResNet-RS,gpuType,Google TPU v3,,True
ResNet-RS,releaseDate,2021-03-13,,True
InternLM-XComposer,flops,3.4504704e+21,flops = (128) * (312 * 10**12) * (80 * 3600) * (0.3) = 3.45e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)from section A.1 we have 128xA100 used for 80 hours,True
InternLM-XComposer,numParams,7000000000.0,,True
InternLM-XComposer,numTokens,64900000000.0,from appendix A.1  55.6B English tokens and 22.1B Chineese tokens and 1.1B images so 0.75*55.6e9+22.1e9+1.1e9 = 64900000000.0,True
InternLM-XComposer,trainingTimeDays,80.0,from appendix A.1,True
InternLM-XComposer,gpuCount,128.0,,True
InternLM-XComposer,gpuType,NVIDIA A100,,True
InternLM-XComposer,releaseDate,2023-09-26,,True
Thumbs Up?,numTokens,2053.0,yielding a corpus of 752 negative and1301 positive reviews,True
Thumbs Up?,releaseDate,2002-05-28,,True
Stable Diffusion (LDM-KL-8-G),flops,5.0000000000000004e+22,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""[1] https://twitter.com/EMostaque/status/1563870674111832066",True
Stable Diffusion (LDM-KL-8-G),numParams,1450000000.0,,True
Stable Diffusion (LDM-KL-8-G),numTokens,400000000.0,,True
Stable Diffusion (LDM-KL-8-G),trainingTimeDays,585.9375,total chip-hours divided by number of GPUs150k/256,True
Stable Diffusion (LDM-KL-8-G),gpuCount,256.0,,True
Stable Diffusion (LDM-KL-8-G),gpuType,NVIDIA A100,,True
Stable Diffusion (LDM-KL-8-G),releaseDate,2022-04-13,,True
SPN-4+KN5,flops,44000000000000000,,True
SPN-4+KN5,numParams,5000000.0,,True
SPN-4+KN5,releaseDate,2014-01-01,,True
InstructGPT,numParams,175000000000.0,,True
InstructGPT,numTokens,374000033207.0,Table 6 - describes **number of prompts**26584 + 6623 = 33207This is added to GPT-3 dataset size.,True
InstructGPT,releaseDate,2022-01-27,,True
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),flops,474000000000000000,,True
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),numParams,38000000.0,,True
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),releaseDate,2017-08-29,,True
Spectrally Normalized GAN,releaseDate,2018-02-16,,True
CLIP (ViT L/14@336px),flops,1.05e+22,https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing,True
CLIP (ViT L/14@336px),numParams,370000000.0,,True
CLIP (ViT L/14@336px),numTokens,400000000.0,,True
CLIP (ViT L/14@336px),costDollars,40146.9882653005,https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html,True
CLIP (ViT L/14@336px),trainingTimeDays,288.0,"“The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs”",True
CLIP (ViT L/14@336px),gpuCount,256.0,,True
CLIP (ViT L/14@336px),gpuType,NVIDIA V100,,True
CLIP (ViT L/14@336px),releaseDate,2021-01-05,,True
DeepLabV3,releaseDate,2017-06-17,,True
Cutout-regularized net,releaseDate,2017-08-15,,True
KataGo,flops,23200000000000000000,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",True
KataGo,numParams,2500000.0,,True
KataGo,numTokens,241000000.0,241 million training samples across 4.2 million games,True
KataGo,trainingTimeDays,456.0,27 processors for 19 days,True
KataGo,gpuType,NVIDIA Tesla V100 DGXS 16 GB,,True
KataGo,releaseDate,2019-02-27,,True
RNN + char2-MS-vec,releaseDate,2019-07-15,,True
RNN+LSA+KN5+cache (model combination w/ linear extrapolation),releaseDate,2012-12-01,,True
FixRes ResNeXt-101 WSL,numParams,829000000.0,,True
FixRes ResNeXt-101 WSL,numTokens,940000000.0,"""Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)""",True
FixRes ResNeXt-101 WSL,releaseDate,2019-06-14,,True
LSTM (WT2),releaseDate,2016-12-13,,True
Innervator,flops,120000000,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,True
Innervator,numParams,10.0,,True
Innervator,numTokens,4.0,,True
Innervator,releaseDate,1989-01-01,,True
Character-enriched word2vec,releaseDate,2016-07-15,,True
Zi Yue 2.0,releaseDate,2023-07-28,,True
Anthropic LM 175B,numParams,175000000000.0,,True
Anthropic LM 175B,releaseDate,2023-02-15,,True
Switch,flops,8.22e+22,Table 4https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,True
Switch,numParams,1600000000000.0,,True
Switch,numTokens,432000000000.0,"""In our protocol we pre-train with 220 (1,048,576) tokensper batch for 550k steps amounting to 576B total tokens.""1 token ~ 0.75 words",True
Switch,costDollars,149825.603595328,,True
Switch,trainingTimeDays,648.0,see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,True
Switch,gpuCount,1024.0,,True
Switch,gpuType,Google TPU v3,,True
Switch,gpuUtilization,0.28,,True
Switch,releaseDate,2021-01-11,,True
"AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)",releaseDate,2018-09-18,,True
MPT-7B,flops,4.2000000000000004e+22,,True
MPT-7B,numParams,7000000000.0,,True
MPT-7B,releaseDate,2023-05-05,,True
Back-propagation,flops,124416000,"We assume that the number of mult-adds per pass is equal to the number of parameters.""We trained the network for 1500 sweeps""There are 12*12 possible pairs of people, so we assume that is the dataset size",True
Back-propagation,numParams,144.0,,True
Back-propagation,numTokens,144.0,"There are 12*12 possible pairs of people, so we assume that is the dataset size",True
Back-propagation,releaseDate,1986-10-01,,True
Swift,flops,53370000000000000,"Policies are trained for a total of 1 × 108 environment interactions, which takes 50 min on a workstation (i9 12900K, RTX 3090, 32 GB RAM DDR5). Fine-tuning is performed for 2 × 107 environment interactions.35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",True
Swift,numParams,21124.0,,True
Swift,trainingTimeDays,0.833,"50 minutes (training details, page 8)",True
Swift,gpuType,NVIDIA GeForce RTX 3090,,True
Swift,releaseDate,2023-08-30,,True
DeciLM 6B,numParams,5700000000.0,,True
DeciLM 6B,releaseDate,2023-09-13,,True
LLaVA,flops,48522240000000000000,"8*312e12*(10+8)*3600*0.3 = 4.852224e+19num gpus * peak flops * time *assumed utilization rate ""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time, 8 A100,",True
LLaVA,numParams,13000000000.0,,True
LLaVA,trainingTimeDays,10.0,"""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuningon Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.""",True
LLaVA,gpuCount,8.0,,True
LLaVA,gpuType,NVIDIA A100,,True
LLaVA,releaseDate,2023-04-17,,True
BLSTM for handwriting (1),releaseDate,2007-09-23,,True
True-Regularization+Finetune,numParams,7000000.0,,True
True-Regularization+Finetune,releaseDate,2019-04-08,,True
GPT-Neo-125M,releaseDate,2021-03-21,,True
ESM1-85M,flops,56000000000000000000,"Information: 128 NVIDIA V100 GPUs [Pre-training details]840k steps [See Table S2: Hyperparameters]131,072 tokens per batch [""We trained with 131,072 tokens per batch (128 gpus x 1024 tokens)."" - Pre-training details]Estimate:  840e3 updates * 3 * 131072 tokens/update * 2 * 85.1e6 parameters = 5.6e19 FLOP",True
ESM1-85M,numParams,85100000.0,,True
ESM1-85M,gpuCount,128.0,,True
ESM1-85M,gpuType,NVIDIA V100,,True
ESM1-85M,releaseDate,2020-08-31,,True
Transformer,flops,7424524800000000000,"""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.htmlNVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performancesource: https://www.nvidia.com/en-gb/data-center/tesla-p100/We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" articlesource: https://openai.com/blog/ai-and-compute/",True
Transformer,numParams,213000000.0,,True
Transformer,numTokens,1400000000.0,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary ""In total, this is 40.5 million sentence-pairs. Assuming each sentence pair is 15-20 words in each language, this is 1.2-1.6 billion words.",True
Transformer,costDollars,111.166108817145,,True
Transformer,trainingTimeDays,84.0,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on thebottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps(3.5 days).",True
Transformer,gpuCount,8.0,,True
Transformer,gpuType,NVIDIA P100,,True
Transformer,releaseDate,2017-06-12,,True
Deep LSTM for video classification,releaseDate,2015-05-01,,True
GShard (dense),flops,1.28e+22,"""The 600B parameters model that achieved the best translation quality was trained with 2048 TPU v3 cores for 4 days, a total cost of 22 TPU v3core-years""Assume 30% utilization. 2 TPU v3 cores = 1 TPU v3 chip.TPU v3 performance is 123 teraFLOPS per chip2048 TPU cores * (1 chip / 2 cores) * 123 TFLOPS/chip * 0.30 = 1.28e22 FLOPhttps://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+22+years+*+0.30Meanwhile their best dense model was trained on 235.5 TPU v3 core-years or 1.3702e23 FLOPhttps://www.wolframalpha.com/input?i=123+teraFLOPS+%2F+2+*+235.5+years+*+0.30Effective model FLOPs utilization could have been lower since this model has very high training compute compared to parameter count (2.3B). (Compare to Chinchilla-optimal?)",True
GShard (dense),numParams,600000000000.0,,True
GShard (dense),numTokens,260000000000.0,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""Each example is a sentence pair. Assuming 20 words per sentence, that is 13*20 billion words.",True
GShard (dense),batchSize,4000000.0,,True
GShard (dense),trainingTimeDays,1008.0,6 weeks = 1008 hours,True
GShard (dense),gpuCount,1024.0,,True
GShard (dense),gpuType,Google TPU v3,,True
GShard (dense),releaseDate,2020-06-30,,True
GShard (dense),batchSize,4000000.0,"Table 3, bolded row is best model",True
DeBERTa,flops,6.00000000001e+21,"From section 5.1.1: ""We use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K batch size and 1M steps takes about 20 days."" This specifically refers to the largest models referred to in the paper, and smaller models are described elsewhere, but I'm assuming the large models are what we care about here. Apparently there are multiple types of GPUs referred to as V100s. I'm guessing these are NVIDIA Tesla SMX2s.",True
DeBERTa,numParams,1500000000.0,,True
DeBERTa,numTokens,15600000000.0,""" DeBERTa is pretrained on 78G training data""1GB ~ 200M words",True
DeBERTa,trainingTimeDays,240.0,20 days,True
DeBERTa,gpuCount,96.0,,True
DeBERTa,gpuType,NVIDIA V100,,True
DeBERTa,releaseDate,2021-06-10,,True
Eve,releaseDate,2021-10-27,,True
BERT-Large,flops,285000000000000000000,more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing,True
BERT-Large,numParams,340000000.0,,True
BERT-Large,numTokens,3300000000.0,"""For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",True
BERT-Large,costDollars,999.934574249643,,True
BERT-Large,trainingTimeDays,96.0,"from appendix A.2: ""Training of BERTLARGE was performedon 16 Cloud TPUs (64 TPU chips total). Each pre-training took 4 days to complete.""",True
BERT-Large,gpuCount,64.0,,True
BERT-Large,gpuType,Google TPU v2,,True
BERT-Large,gpuUtilization,0.29,,True
BERT-Large,releaseDate,2018-10-11,,True
Compress-LSTM (4.6M),releaseDate,2019-02-06,,True
NLLB,flops,1.751113728e+22,"Section 8.8:"" To train NLLB-200, a cumulativeof 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""See also Table 48Section 8.2.4 states they use FP16NVIDIA Datasheet states 312TFLOPS for FP16https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdfAssuming 0.3 utilization:312e12*3600*51968*0.3Also:""Our final model is a Transformerencoder-decoder model in which we replace the Feed Forward Network (FFN) layer inevery 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoderlayers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described inSection 6.1.1. We share the embedding weights of the encoder input embedding, decoderinput embedding and decoder output embedding layers. We use an overall dropout of 0.3,attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parametersand FLOPs similar to that of a 3.3B dense model.""",True
NLLB,numParams,54500000000.0,,True
NLLB,numTokens,360000000000.0,"[WORDS]Section 8.2.2: ""As we prepare to train on the final 202 language dataset comprising of over 18B sentencepairs and 2440 language directions""18B sentences * 20 words/sentence",True
NLLB,costDollars,39175.6382639152,,True
NLLB,gpuType,NVIDIA A100 SXM4 80 GB,,True
NLLB,releaseDate,2022-07-06,,True
GPT-4 Turbo,releaseDate,2023-11-06,,True
Tensorized Transformer (core-2),releaseDate,2019-06-24,,True
Fast R-CNN,releaseDate,2015-04-30,,True
VALL-E,flops,10100000000000000000,"""The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustictokens per GPU for 800k steps""353M * 800k * 6k * 6 = 1.01e1916 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?",True
VALL-E,numParams,353000000.0,,True
VALL-E,numTokens,820800000.0,"60k hours~13,680 words/hour * 60,000 = 820800000 wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",True
VALL-E,gpuType,NVIDIA V100,,True
VALL-E,releaseDate,2023-01-05,,True
LTM,releaseDate,2019-04-18,,True
Wu Dao - Wen Lan,flops,7.1995392e+21,128 Nvidia A100 GPUs for 7 days128 GPUs * 3.1e14 FLOP/s /GPU * 7*24*60*60s* 0.3 [utilization rate],True
Wu Dao - Wen Lan,numParams,1000000000.0,,True
Wu Dao - Wen Lan,releaseDate,2021-03-01,,True
GPT-3.5 (text-davinci-003),flops,2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,True
GPT-3.5 (text-davinci-003),gpuType,NVIDIA A100 SXM4 40 GB,,True
GPT-3.5 (text-davinci-003),releaseDate,2022-11-28,,True
SRN-Encoded Grammatical Structures,numTokens,177805.0,4 training sets of 10k sentences each. Total number of words calculated by multiplying 10k and the avg. number of words per sentence in the training set.,True
SRN-Encoded Grammatical Structures,releaseDate,1991-09-01,,True
OPT-6.7B,numParams,6700000000.0,,True
OPT-6.7B,releaseDate,2022-06-21,,True
Stable Beluga 1,numParams,65200000000.0,,True
Stable Beluga 1,releaseDate,2023-07-21,,True
LSTM-Large+Behaviorial-Gating,numParams,67000000.0,,True
LSTM-Large+Behaviorial-Gating,releaseDate,2019-08-31,,True
HOGWILD!,releaseDate,2011-11-11,,True
NASv3 (CIFAR-10),flops,2.2e+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass = 1.9e6 PF = 22 pfs-dayssource: https://openai.com/blog/ai-and-compute/",True
NASv3 (CIFAR-10),numParams,37400000.0,,True
NASv3 (CIFAR-10),costDollars,13069.3458085824,,True
NASv3 (CIFAR-10),gpuCount,800.0,,True
NASv3 (CIFAR-10),releaseDate,2016-11-05,,True
Adaptive Subgrad,releaseDate,2011-10-03,,True
GAN-Advancer,releaseDate,2016-12-05,,True
AWD-LSTM + dynamic eval (WT2),releaseDate,2017-09-21,,True
YaLM,flops,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""",True
YaLM,numParams,100000000000.0,,True
YaLM,numTokens,300000000000.0,1.7TB of data 300B tokens – from github https://github.com/yandex/YaLM-100BI've assumed that 1 token correspond to 1 word in russian language.,True
YaLM,trainingTimeDays,1560.0,65 days,True
YaLM,gpuCount,800.0,,True
YaLM,gpuType,NVIDIA A100,,True
YaLM,releaseDate,2022-06-23,,True
PAPA,releaseDate,1961-09-01,,True
RaptorX-Contact,releaseDate,2019-05-02,,True
Unified-IO,flops,3.5e+21,"1M steps, batch size 1024. Sequence length may be 128-256:""We use a maximum of 256 and 128 text tokens for inputs and outputs respectively, and a maximumlength of 576 (i.e. 24 × 24 patch encoding from a 384 × 384 image) for image inputs and 256 (i.e.16 × 16 latent codes from a 256 × 256 image) for image outputs6 * 1 million * 1024 * 128 * 2.9 billion = 2.3e216 * 1 million * 1024 * 256 * 2.9 billion = 4.6e21average is 3.5e21No hardware details.",True
Unified-IO,numParams,2925000000.0,,True
Unified-IO,gpuType,Google TPU v4,,True
Unified-IO,releaseDate,2022-06-17,,True
CPM-Large,flops,1.8e+21,source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/tree/main/akrodb,True
CPM-Large,numParams,2600000000.0,,True
CPM-Large,numTokens,16700000000.0,"""language model, with 2.6 billion parameters and 100GB Chinese training data.""We use the conversion factor 1GB ~ 167M words",True
CPM-Large,costDollars,6569.50717068554,"https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9.",True
CPM-Large,trainingTimeDays,336.0,"""It takes two weeks to train our largest model using 64 NVIDIA V100.""",True
CPM-Large,gpuCount,64.0,,True
CPM-Large,gpuType,NVIDIA V100,,True
CPM-Large,releaseDate,2020-12-01,,True
DeepNash,releaseDate,2022-12-01,,True
Llama-2-Chinese 13B,numParams,13000000000.0,,True
Llama-2-Chinese 13B,releaseDate,2023-06-25,,True
OPT-66B,flops,1.100000000001e+23,"OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 ∗ 2e6 ∗ 66e9 ∗ 6 = 1.1e23 FLOP",True
OPT-66B,numParams,66000000000.0,,True
OPT-66B,releaseDate,2022-06-21,,True
RBM Image Classifier,numParams,80000000.0,,True
RBM Image Classifier,numTokens,2000000.0,,True
RBM Image Classifier,releaseDate,2009-04-08,,True
Spatial Pyramid Matching,releaseDate,2006-06-17,,True
Multi-scale Dilated CNN,releaseDate,2015-11-23,,True
WeNet (Penn Treebank),numParams,23000000.0,,True
WeNet (Penn Treebank),releaseDate,2019-04-08,,True
TF-LM-discourse LSTM (WT2),releaseDate,2018-05-01,,True
RoseTTAFold All-Atom (RFAA),releaseDate,2023-10-09,,True
Flan-PaLM 540B,numParams,540000000000.0,,True
Flan-PaLM 540B,trainingTimeDays,37.0,"""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4TPU chips for 37 hours)""",True
Flan-PaLM 540B,gpuCount,512.0,,True
Flan-PaLM 540B,gpuType,Google TPU v4,,True
Flan-PaLM 540B,gpuUtilization,0.3,,True
Flan-PaLM 540B,releaseDate,2022-10-20,,True
ADAM (CIFAR-10),flops,60480000000000000,From https://openai.com/blog/ai-and-compute/ Appendixless than 0.0007 pfs-days (86400*10^15*0.0007),True
ADAM (CIFAR-10),costDollars,0.604230356497444,,True
ADAM (CIFAR-10),releaseDate,2014-12-22,,True
GloVe (6B),numParams,120000000.0,,True
GloVe (6B),numTokens,6000000000.0,"""We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]""",True
GloVe (6B),releaseDate,2014-01-01,,True
Granite 13B,flops,2.44e+23,"Estimate using hardware:""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.Granite.13b.v2 was trained on the same infrastructure for anadditional 1152 hours with 120 TFLOPS, bringing the total to2208 hours""Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.256 * 2208 * 3600 * 120 TFLOPS = 2.44e23Using 6ND:""The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.""""The granite.13b.v1 base model is trained for 300K iterations,with a batch size of 4M tokens, for a total of 1.25 trillion5 tokens. The granite.13b.v2 base model continued pre-trainingon top of the granite.13b.v1 checkpoint for an additional 300Kiterations and a total of 2.5 trillion tokens.""2.5T * 13B * 6 = 1.95e23",True
Granite 13B,numParams,13000000000.0,,True
Granite 13B,numTokens,1875000000000.0,"2.5T tokens, 1.875T words at 0.75 words/token",True
Granite 13B,trainingTimeDays,2208.0,"""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for anadditional 1152 hours with 120 TFLOPS, bringing the total to2208 hours""",True
Granite 13B,gpuType,NVIDIA A100,,True
Granite 13B,releaseDate,2023-11-30,,True
WeLM,flops,6.2084579328e+21,">>> num_gpu = 128 >>> flops = 7797 * 10**10 >>> time = 24 * 24 * 3600 >>> utilization = 0.3 >>> num_gpu * flops * time * utilization 6.2084579328e+21 >>> int(num_gpu * flops * time * utilization) 6208457932800000000000 from citations:""The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 days”, ""All models are trained with FP16 mixed precision.""from https://www.techpowerup.com/gpu-specs/a100-sxm4-40-gb.c3506 A100 have  77.97 TFLOPS for FP16 (half), ",True
WeLM,numParams,10000000000.0,,True
WeLM,numTokens,262000000000.0,"from the paper ""After all the above filtering process, our corpus contains 262B tokens”Data is mostly in Chinese language - we assume that 1 token correspond to 1 word",True
WeLM,trainingTimeDays,576.0,"""The largest model is trained on 128 A100-SXM4-40GB GPUs in about 24 days”, ",True
WeLM,gpuCount,128.0,,True
WeLM,gpuType,NVIDIA A100 SXM4 40 GB,,True
WeLM,releaseDate,2023-05-16,,True
MADALINE III,releaseDate,1990-09-01,,True
OPT-350M,numParams,350000000.0,,True
OPT-350M,releaseDate,2022-06-21,,True
SimpleNet,numParams,5480000.0,,True
SimpleNet,gpuType,NVIDIA GeForce GTX 980,,True
SimpleNet,releaseDate,2016-08-22,,True
RNN for speech,flops,226690156032.01834,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
RNN for speech,numParams,7512.0,,True
RNN for speech,numTokens,14096.0,"The data base was divided into two parts: a training set and an open test set. These two sets consisted of 28 191 and 7051 syllables,respectively.Of the top 10,000 Chinese words, 15% have 1 syllable, 78% have 2 syllables, and 7% have more than two syllables. Assuming 2 syllables per word, the training set is around 14100 words.",True
RNN for speech,releaseDate,1998-05-15,,True
Transformer-XL-ptb,releaseDate,2019-01-09,,True
MultiBand Diffusion,flops,26000000000000000000,"""It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.""125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number)4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19",True
MultiBand Diffusion,trainingTimeDays,48.0,around 2 days,True
MultiBand Diffusion,gpuType,NVIDIA V100,,True
MultiBand Diffusion,releaseDate,2023-11-08,,True
Wide & Deep,releaseDate,2016-06-24,,True
Mistral 7B,numParams,7000000000.0,,True
Mistral 7B,releaseDate,2023-10-10,,True
UniPi,gpuType,Google TPU v4,,True
UniPi,releaseDate,2023-01-31,,True
CLIP (ResNet-50),numParams,88600000.0,,True
CLIP (ResNet-50),numTokens,400000000.0,,True
CLIP (ResNet-50),releaseDate,2021-01-05,,True
ISS,flops,3400000000000000,,True
ISS,numParams,11100000.000000002,,True
ISS,releaseDate,2017-09-15,,True
Adversarial Joint Adaptation Network (ResNet),numParams,60000000.0,,True
Adversarial Joint Adaptation Network (ResNet),numTokens,4652.0,,True
Adversarial Joint Adaptation Network (ResNet),releaseDate,2017-08-17,,True
Flamingo,flops,2.7e+23,1536 TPU v4 chips for 15 days. Assuming 50% utilization:C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.50 = 2.7*10^23 FLOPAll training and evaluationwas performed on TPUv4 instances. The largest model containing 80 billion parameters is trained onQUSV chips for 15 days and sharded across 16 devices.All trained parameters and optimizer accumulators are storedand updated in float32; all activations and gradients are computed in bfloat16 after downcastingof parameters from float32 to bfloat16,True
Flamingo,numParams,80000000000.0,,True
Flamingo,trainingTimeDays,360.0,1536 TPU v4 chips for 15 days,True
Flamingo,gpuCount,1536.0,,True
Flamingo,gpuType,Google TPU v4,,True
Flamingo,releaseDate,2022-04-29,,True
CodeT5-base,flops,1.56e+21,"""We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21",True
CodeT5-base,numParams,220000000.0,,True
CodeT5-base,trainingTimeDays,288.0,"""The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""",True
CodeT5-base,gpuType,NVIDIA A100,,True
CodeT5-base,releaseDate,2021-11-01,,True
PanGu-α,flops,5.83e+22,source: https://lair.lighton.ai/akronomicon/archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Huawei/PanGu-%CE%B1.json,True
PanGu-α,numParams,207000000000.0,,True
PanGu-α,numTokens,200000000000.0,"""The composition of our corpus and the processing steps adopted to each data source is shown in Table 3.2.Based on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and 13B) and large (200B) models, respectively""1 TB = 1000 GB1 GB ~ 200M words",True
PanGu-α,costDollars,97802.0583215743,,True
PanGu-α,releaseDate,2021-04-25,,True
GRU + p-tHSM (pretrain via Brown) (PTB),releaseDate,2017-08-19,,True
ST-MoE,numParams,269000000000.0,,True
ST-MoE,numTokens,1342000000000.0,"1790B tokens, or 1342B words at 0.75 words/token",True
ST-MoE,releaseDate,2022-02-17,,True
Engin-Base (NE),releaseDate,2021-12-11,,True
LSTM+Noise(Beta),flops,127000000000000000,,True
LSTM+Noise(Beta),numParams,51000000.0,,True
LSTM+Noise(Beta),releaseDate,2018-05-03,,True
Differentiable neural computer,releaseDate,2016-10-12,,True
AlphaGo Lee,flops,1.9e+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked belowAlphaGo Fan: https://www.nature.com/articles/nature16961AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",True
AlphaGo Lee,numTokens,29400000.0,"We trained the policy network pσ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",True
AlphaGo Lee,costDollars,14041.8044022234,,True
AlphaGo Lee,releaseDate,2016-01-27,,True
Mask R-CNN,releaseDate,2017-03-30,,True
Dropout-LSTM+Noise(Bernoulli) (WT2),flops,127000000000000000,,True
Dropout-LSTM+Noise(Bernoulli) (WT2),numParams,51000000.0,,True
Dropout-LSTM+Noise(Bernoulli) (WT2),releaseDate,2018-05-03,,True
EMDR,flops,1.24e+21,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete,while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we alsoperform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.""1 week + 25+25+8 hours * 16 A100s= ~230 * 16 A100-hours= 230 * 16 * 3600 * 312 trillion * 0.3 = 1.24e21",True
EMDR,numParams,440000000.0,,True
EMDR,trainingTimeDays,230.0,,True
EMDR,gpuType,NVIDIA A100,,True
EMDR,releaseDate,2021-06-09,,True
Discriminator Guidance,flops,215700000010000000000,481 hours * 312 TFLOPS (A100) * 40% utilization,True
Discriminator Guidance,trainingTimeDays,481.0,Table 6,True
Discriminator Guidance,gpuType,NVIDIA A100 PCIe,,True
Discriminator Guidance,releaseDate,2022-11-28,,True
MusicGen,numParams,3300000000.0,,True
MusicGen,releaseDate,2023-06-08,,True
Transformer-XL Large,flops,10900000000000000000,,True
Transformer-XL Large,numParams,257000000.0,,True
Transformer-XL Large,releaseDate,2019-01-09,,True
Claude Instant,releaseDate,2023-08-09,,True
Probabilistic modeling for object recognition,numTokens,120472.0,"Section 5.1: ""We formed training sets from 991 faces images and 1,552non-face images.""""For each face image we generated120 synthetic variations""",True
Probabilistic modeling for object recognition,releaseDate,1998-06-23,,True
MT-DNN,numParams,330000000.0,,True
MT-DNN,releaseDate,2019-01-31,,True
"DEQ-Transformer (Medium, Adaptive Embedding)",flops,816000000000000000,,True
"DEQ-Transformer (Medium, Adaptive Embedding)",numParams,110000000.0,,True
"DEQ-Transformer (Medium, Adaptive Embedding)",releaseDate,2019-09-03,,True
Random Decision Forests,numTokens,60000.0,The images are from the 1992 NIST (National Institute of Standards and Technology) Competition,True
Random Decision Forests,releaseDate,1995-08-14,,True
Meena,flops,1.12e+23,https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdfTable 4,True
Meena,numParams,2600000000.0,,True
Meena,numTokens,40000000000.0,"""The final Meena dataset contains 341GB of text(40B words)""Converting from GB to words yields 6.8e10, which is in the same OOM",True
Meena,batchSize,82655.0,,True
Meena,costDollars,263099.940265426,,True
Meena,trainingTimeDays,720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",True
Meena,gpuCount,1024.0,,True
Meena,gpuType,Google TPU v3,,True
Meena,gpuUtilization,0.3439,,True
Meena,releaseDate,2020-01-28,,True
Meena,batchSize,82655.0,"61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",True
ImageBind,numParams,932000000.0,,True
ImageBind,gpuType,"NVIDIA V100,NVIDIA A100",,True
ImageBind,releaseDate,2023-05-09,,True
Decaying Fast Weights Transformer,flops,79000000000000000000,"Pre-trained model is Transformer (Adaptive Input Embeddings) which was 7.3e19. This is from 8 * 67 V100-hours. fine-tuning:""Training was performed on a single NVIDIA A40 GPU for 35 hours""35h, 1 GPU, 149.7e12, 30% = 5.7e18 FLOP""5.7e18 + 7.3e19 is 7.9e19",True
Decaying Fast Weights Transformer,numParams,242000000.0,,True
Decaying Fast Weights Transformer,releaseDate,2022-10-09,,True
Baichuan2-13B,flops,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.13b * 2.6t * 6 = 2.03e23",True
Baichuan2-13B,numParams,13000000000.0,,True
Baichuan2-13B,numTokens,2300000000000.0,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words1.3T English tokens * (0.75 words/token) = 0.975T English wordstotal: 2.275T, or ~2.3T",True
Baichuan2-13B,releaseDate,2023-09-06,,True
Pythia-12b,flops,2.16e+22,,True
Pythia-12b,numParams,12000000000.0,,True
Pythia-12b,batchSize,2097152.0,,True
Pythia-12b,releaseDate,2023-04-03,,True
Pythia-12b,batchSize,2097152.0,"""The most notable divergence from standard training procedures is that we use a much larger batch size than what is standard for training small language models... we use a batch size of 1024 samples with a sequence length of 2048 (2,097,152 tokens) for all models""",True
TF-LM-discourse LSTM (PTB),releaseDate,2018-05-01,,True
"GPT-2 (1.5B, Curriculum Learning 45K)",flops,600000000000000000000,,True
"GPT-2 (1.5B, Curriculum Learning 45K)",numParams,1500000000.0,,True
"GPT-2 (1.5B, Curriculum Learning 45K)",releaseDate,2021-08-13,,True
Samsung Gauss Image,releaseDate,2023-11-08,,True
Samuel Neural Checkers II,numParams,40.0,,True
Samuel Neural Checkers II,releaseDate,1967-11-01,,True
Verbatim Memory Transformer (117M),releaseDate,2022-10-24,,True
BlueLM 13B,flops,1.0920000000001e+23,C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOPhttps://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion(assuming 1 epoch),True
BlueLM 13B,numParams,7000000000000.0,,True
BlueLM 13B,numTokens,1950000000000.0,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub",True
BlueLM 13B,releaseDate,2023-10-31,,True
GPT2+CoreLM+Fine-Tuning,flops,31700000000000000,,True
GPT2+CoreLM+Fine-Tuning,numParams,132000000.0,,True
GPT2+CoreLM+Fine-Tuning,releaseDate,2021-11-04,,True
Transformer-XL + RMT,numParams,247000000.00000003,,True
Transformer-XL + RMT,releaseDate,2022-07-14,,True
S + I-Attention (3),releaseDate,2018-06-26,,True
LSTM-Medium+Behaviorial-Gating,releaseDate,2019-08-31,,True
PaLI-X,numParams,55000000000.0,,True
PaLI-X,numTokens,1400000000.0,"1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data",True
PaLI-X,releaseDate,2023-05-29,,True
Statistical Shape Constellations,releaseDate,2003-01-01,,True
SRU++ Base,releaseDate,2021-02-24,,True
TransformerXL+RelationLM,flops,3.2e+21,,True
TransformerXL+RelationLM,numParams,124000000.0,,True
TransformerXL+RelationLM,releaseDate,2022-01-24,,True
OPT-2.7B,numParams,2700000000.0,,True
OPT-2.7B,releaseDate,2022-06-21,,True
EfficientDet,numParams,77000000.0,,True
EfficientDet,releaseDate,2020-07-27,,True
AlphaX-1,flops,7600000000000000000,,True
AlphaX-1,numParams,579000000.0,,True
AlphaX-1,costDollars,24.1016174472636,,True
AlphaX-1,gpuType,NVIDIA Geforce GTX1080 Ti,,True
AlphaX-1,releaseDate,2019-10-02,,True
PNASNet-5,flops,66290400000000000000,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.(From Imagenet paper-data, Besiroglu et al., forthcoming) ",True
PNASNet-5,numTokens,1280000.0,,True
PNASNet-5,costDollars,991.481511071839,,True
PNASNet-5,releaseDate,2017-12-02,,True
ProteinGAN,flops,4300000000000000000,"2.5 million steps, batch size 64, 210 hours on NVIDIA Tesla P100 system",True
ProteinGAN,numParams,60000000.0,,True
ProteinGAN,trainingTimeDays,210.0,,True
ProteinGAN,gpuCount,1.0,,True
ProteinGAN,gpuType,NVIDIA P100,,True
ProteinGAN,releaseDate,2021-03-04,,True
ChatGLM3,flops,1.09200000000001e+24,Highly speculative.Assume 1 epoch on 1.4T tokens.6 FLOP/token/param * 1.4T tokens * 130B paramshttps://www.wolframalpha.com/input?i=6*130+billion*1.4+trillion,True
ChatGLM3,numParams,130000000000.0,,True
ChatGLM3,numTokens,1050000000000.0,"The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.Sources:https://chatglm.cn/https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.mdhttps://www.zhipuai.cn/en/news/76",True
ChatGLM3,releaseDate,2023-10-27,,True
Optimized Multi-Scale Edge Detection,releaseDate,1986-11-01,,True
CURL,numParams,907264.0,,True
CURL,releaseDate,2020-04-08,,True
ObjectNet,flops,19400000000000000000,"3-5 days of training (say, 4.5), 50 teraFLOP/second at 50% utilization rate (reported) = 1.94E19",True
ObjectNet,numParams,38000000.0,,True
ObjectNet,numTokens,50000.0,"In total, 95,824 images were collected from 5,982 workers out of which 50,000 images were retainedafter validation and included in the dataset",True
ObjectNet,costDollars,50.7853597195102,,True
ObjectNet,releaseDate,2019-09-06,,True
AmoebaNet-A (F=190),numParams,87000000.0,,True
AmoebaNet-A (F=190),releaseDate,2018-02-05,,True
OPT-125M (finetuned on PTB),numParams,125000000.0,,True
OPT-125M (finetuned on PTB),releaseDate,2022-06-21,,True
GRUs,releaseDate,2014-06-03,,True
Multi-cell LSTM,flops,2009999999999999.8,,True
Multi-cell LSTM,numParams,7200000.0,,True
Multi-cell LSTM,releaseDate,2018-11-15,,True
Cohere Embed,releaseDate,2023-11-02,,True
Peephole LSTM,numParams,17.0,,True
Peephole LSTM,numTokens,64970000.0,See Table 2,True
Peephole LSTM,releaseDate,2000-07-26,,True
OpenAI TI7 DOTA 1v1,flops,604609522259200200000,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,True
OpenAI TI7 DOTA 1v1,costDollars,2873.98659242643,,True
OpenAI TI7 DOTA 1v1,releaseDate,2017-08-11,,True
MSA Transformer,flops,5.49e+21,"Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp",True
MSA Transformer,numParams,100000000.0,,True
MSA Transformer,numTokens,26000000.0,"""Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.""",True
MSA Transformer,gpuCount,32.0,,True
MSA Transformer,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
MSA Transformer,releaseDate,2021-02-13,,True
Fusion in Encoder,flops,130000000000000000000,"""The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.""2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20",True
Fusion in Encoder,numParams,330000000.0,,True
Fusion in Encoder,trainingTimeDays,48.0,2 days,True
Fusion in Encoder,gpuType,NVIDIA A100 SXM4 80 GB,,True
Fusion in Encoder,releaseDate,2022-11-18,,True
PhraseCond,numTokens,4000000.0,size of SQuAD 1.1,True
PhraseCond,releaseDate,2017-10-28,,True
ALBERT-xxlarge,flops,2.39e+21,32 hours of training512 TPU V3s0.33 utilization rate,True
ALBERT-xxlarge,numParams,235000000.0,,True
ALBERT-xxlarge,numTokens,3300000000.0,"Pretraining same as for BERT - Wikipedia and BookCorpus""For the pre-training corpus weuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)""",True
ALBERT-xxlarge,costDollars,5924.431294748,,True
ALBERT-xxlarge,trainingTimeDays,32.0,,True
ALBERT-xxlarge,gpuCount,512.0,,True
ALBERT-xxlarge,gpuType,Google TPU v3,,True
ALBERT-xxlarge,releaseDate,2020-02-09,,True
iGPT-XL,flops,3.3e+22,Taken from herehttps://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening,True
iGPT-XL,numParams,6801000000.0,,True
iGPT-XL,numTokens,9600000.0,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""https://image-net.org/challenges/LSVRC/2012/""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""",True
iGPT-XL,costDollars,120440.964795901,,True
iGPT-XL,gpuType,NVIDIA Tesla V100 DGXS 32 GB,,True
iGPT-XL,releaseDate,2020-06-17,,True
LLaVA + LVIS-INSTRUCT4V,numParams,13000000000.0,,True
LLaVA + LVIS-INSTRUCT4V,releaseDate,2023-11-13,,True
YOLOX-X,numParams,99100000.0,,True
YOLOX-X,numTokens,2500000.0,"2.5 million image-label pairs, per Coco paper https://arxiv.org/abs/1405.0312",True
YOLOX-X,gpuType,NVIDIA V100,,True
YOLOX-X,releaseDate,2021-08-06,,True
TinyLlama-1.1B (3T token checkpoint),flops,2.173796352e+22,"6ND approximation: 6*1.1B * 3T = 19800000000000000000000Extrapolation from the 1T checkpoint:flops = (16) * (312 * 10**12) * (3 * 30 * 24 * 3600) * (0.56) = 7245987840000001048576(num gpu) * (peak flops) * (time in seconds) * (reported utilization rate)source: https://github.com/jzhang38/TinyLlama""Thanks to those optimizations, we achieve a throughput of 24k tokens per second per A100-40G GPU, which translates to 56% model flops utilization""and Releases Schedule from the same link",True
TinyLlama-1.1B (3T token checkpoint),numParams,1100000000.0,,True
TinyLlama-1.1B (3T token checkpoint),numTokens,750000000000.0,1T tokens checkpoint so around 0.75T words,True
TinyLlama-1.1B (3T token checkpoint),trainingTimeDays,2160.0,1T checkpoint was released after 1 month. Assume the 3T checkpoint took 3 months.source: https://github.com/jzhang38/TinyLlama,True
TinyLlama-1.1B (3T token checkpoint),gpuCount,16.0,,True
TinyLlama-1.1B (3T token checkpoint),gpuType,NVIDIA A100 SXM4 40 GB,,True
TinyLlama-1.1B (3T token checkpoint),gpuUtilization,0.56,,True
TinyLlama-1.1B (3T token checkpoint),releaseDate,2023-10-01,,True
BatchNorm,releaseDate,2015-06-15,,True
DBN for NLP,numParams,1021535.0,,True
DBN for NLP,numTokens,178000.0,The training data has 27K automatically transcribed utterances amounting to 178K words.,True
DBN for NLP,releaseDate,2014-02-11,,True
BAAI/bge-reranker-large,numParams,560000000.0,,True
BAAI/bge-reranker-large,releaseDate,2023-09-14,,True
PaLM 2,flops,7.34e+24,"Compute Requirements ""Not reported.""Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP.",True
PaLM 2,numParams,340000000000.0,,True
PaLM 2,numTokens,2700000000000.0,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",True
PaLM 2,gpuType,Google TPU v4,,True
PaLM 2,releaseDate,2023-05-10,,True
Local Transformer,releaseDate,2020-03-12,,True
Gopher (7.1B),releaseDate,2021-12-08,,True
IMPALA,flops,168000000000000000000,"Source: Ajeya Cotra and Tom Davidson, https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",True
IMPALA,numParams,1600000.0,,True
IMPALA,numTokens,240000000000.0,"From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the ""dataset size""",True
IMPALA,trainingTimeDays,100.0,Maximum training time for IMPALA is 100 hours according to Figure 6. This seems to refer to the 1 GPU model. The 8 GPU model looks to have been trained about 1/8 as long.,True
IMPALA,gpuCount,1.0,,True
IMPALA,gpuType,NVIDIA P100,,True
IMPALA,releaseDate,2018-02-05,,True
Cerebras-GPT-13B,flops,2.3e+22,"2.3e22, per table 2",True
Cerebras-GPT-13B,numParams,13000000000.0,,True
Cerebras-GPT-13B,numTokens,278000000000.0,"371B tokens, or 278B words",True
Cerebras-GPT-13B,batchSize,2210000.0,,True
Cerebras-GPT-13B,gpuType,Cerebras CS-2,,True
Cerebras-GPT-13B,releaseDate,2023-04-06,,True
Cerebras-GPT-13B,batchSize,2210000.0,"""For the 13B parameter model, we train with a batch size of 720 sequences of length 2048 tokens for the first 84B tokens. At that point, we observed the gap between validation and train loss growing, indicating that the gradient noise was growing, so we increased the batch size to 1080 sequences for the rest of training.""batch sizes ramp from 1.47M to 2.21M",True
TSN,releaseDate,2016-09-17,,True
Photo-Geometric Autoencoder,releaseDate,2019-11-25,,True
Deep Boltzmann Machines,releaseDate,2009-04-16,,True
GBERT-Large,flops,2.24446464e+21,flops = (64) * (123* 10**12) * (11 * 24 * 3600) * (0.3) = 2.24e21(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1it was trained for 11 days from Table 2,True
GBERT-Large,numParams,335000000.0,,True
GBERT-Large,numTokens,27287800000.0,163.4GB from Table 1 in the paperassuming 167M words per GB (German Language) we have 163.4 * 167M = 27287800000.0,True
GBERT-Large,trainingTimeDays,264.0,11 days from Table 2,True
GBERT-Large,gpuCount,64.0,,True
GBERT-Large,gpuType,Google TPU v3,,True
GBERT-Large,releaseDate,2020-10-21,,True
BERT-Large-CAS (PTB+WT2+WT103),flops,521000000000000000000,,True
BERT-Large-CAS (PTB+WT2+WT103),numParams,395000000.0,,True
BERT-Large-CAS (PTB+WT2+WT103),releaseDate,2019-04-20,,True
CNN Best Practices,numTokens,50000.0,,True
CNN Best Practices,releaseDate,2003-08-06,,True
ERNIE-Doc (247M),flops,29100000000000000000,,True
ERNIE-Doc (247M),numParams,247000000.00000003,,True
ERNIE-Doc (247M),releaseDate,2020-12-31,,True
Midjourney V1,releaseDate,2022-02-15,,True
LaMemo,numParams,151000000.0,,True
LaMemo,releaseDate,2022-04-15,,True
DeciCoder-6B,numParams,6000000000.0,,True
DeciCoder-6B,releaseDate,2024-01-15,,True
ELMo,numParams,94000000.0,,True
ELMo,releaseDate,2018-02-01,,True
tsuzumi 7B,numParams,7000000000.0,,True
tsuzumi 7B,releaseDate,2023-12-01,,True
VD-LSTM+REAL Small,releaseDate,2016-11-04,,True
DLRM-12T,numParams,12000000000000.0,,True
DLRM-12T,gpuType,NVIDIA A100,,True
DLRM-12T,releaseDate,2021-04-12,,True
FLAN 137B,flops,4.896e+22,"From section 2.4: ""60 hours on a TPUv3 with 128 cores."" I assume that ""128 cores"" = 128 TPUv3s. Which took less than 2% of total time (see environmental considerations section)",True
FLAN 137B,numParams,137000000000.0,,True
FLAN 137B,numTokens,1870000000000.0,"""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""2.49e12 tokens ~= 1.87e12 words",True
FLAN 137B,trainingTimeDays,60.0,,True
FLAN 137B,gpuCount,64.0,,True
FLAN 137B,gpuType,Google TPU v3,,True
FLAN 137B,releaseDate,2021-09-03,,True
