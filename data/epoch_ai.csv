model_name,attribute,value,citation
Claude,releaseDate,2023-03-14,
Japanese-LM-3.6B,numParams,3600000000.0,
Japanese-LM-3.6B,numTokens,72000000000.0,"650GB per huggingfaceour guide says 111M Japanese words per GB, which would be ~72B wordshttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0"
Japanese-LM-3.6B,gpuType,NVIDIA A100,
Japanese-LM-3.6B,releaseDate,2023-08-14,
GPT-SW3,flops,1.3e+21,"From section 4: ""Training was performed on GPU resources from the Berzelius Superpod, which is currently the fastest supercomputer in Sweden, equipped with 60 Nvidia DGXA100 servers, each of which consists of 8 Nvidia A100GPUs with 320 GB Total GPU memory. Our trainingprocess took 2.5 days utilizing 16 of the DGX A100servers (in total 128 GPUs).""2.5*24*60**2 * 128 * 1.56E+14 * 0.3 = 1.3e21"
GPT-SW3,numParams,3500000000.0,
GPT-SW3,numTokens,16700000000.0,"100GB Swedish corpus, assume Swedish has similar 167M words per GB as German.100*167e6 = 1.67e10"
GPT-SW3,trainingTimeDays,60.0,"""Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs)."""
GPT-SW3,gpuType,NVIDIA A100,
GPT-SW3,releaseDate,2022-06-25,
LLaMA-33B,flops,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP
LLaMA-33B,numParams,32500000000.0,
LLaMA-33B,batchSize,4000000.0,
LLaMA-33B,releaseDate,2023-02-27,
LLaMA-33B,batchSize,4000000.0,
LLaMA-65B,flops,5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOPCompared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29"
LLaMA-65B,numParams,65200000000.0,
LLaMA-65B,numTokens,1050000000000.0,1.4 trillion tokens * 0.75 words/token = 1.05 trillion words
LLaMA-65B,batchSize,4000000.0,
LLaMA-65B,costDollars,1179384.75,"1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.$1391674 / 1.18 = $1179385 in 2020 USD."
LLaMA-65B,trainingTimeDays,500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days."""
LLaMA-65B,gpuCount,2048.0,
LLaMA-65B,gpuType,NVIDIA A100,
LLaMA-65B,gpuUtilization,0.4746,
LLaMA-65B,releaseDate,2023-02-24,
LLaMA-65B,batchSize,4000000.0,
Falcon 180B,flops,3.76e+24,"43,500 petaflop-days per Table 1 of the paper43500 * 1e15 * 24 * 3600 = 3.76e24C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP"
Falcon 180B,numParams,180000000000.0,
Falcon 180B,numTokens,2625000000000.0,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words
Falcon 180B,batchSize,4194304.0,
Falcon 180B,trainingTimeDays,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that."
Falcon 180B,gpuCount,4096.0,
Falcon 180B,gpuType,NVIDIA A100 SXM4 40 GB,
Falcon 180B,gpuUtilization,0.1876,
Falcon 180B,releaseDate,2023-09-06,
Falcon 180B,batchSize,4194304.0,"from paper (https://arxiv.org/pdf/2311.16867.pdf):Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.""All Falcon models are pretrained with a 2,048 sequence length""2048*2048 = 4194304"
BloombergGPT,flops,2.36e+23,"2.36e23 per Table 4(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)"
BloombergGPT,numParams,50558868480.0,
BloombergGPT,numTokens,532000000000.0,"708.9 billion tokens. At 0.75 English words per token, that's 532B words"
BloombergGPT,batchSize,4200000.0,
BloombergGPT,trainingTimeDays,1270.0,"""~53 days"""
BloombergGPT,gpuCount,512.0,
BloombergGPT,gpuType,NVIDIA A100,
BloombergGPT,gpuUtilization,0.32,
BloombergGPT,releaseDate,2023-03-30,
BloombergGPT,batchSize,4200000.0,"""in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training."""
StableLM-Base-Alpha-7B,flops,4.5e+22,"""StableLM-Base-Alpha-7B-v2 is pre-trained using a multi-stage context length extension schedule following similar work (Nijkamp et al. 2023); first pre-training at a context length of 2048 for 1 trillion tokens, then fine-tuning at a context length of 4096 for another 100B tokens""6890209280 params * 1.1 trillion tokens * 6 = 4.5e22alternatively: ""StableLM-Base-Alpha-7B-v2 was trained on the Stability AI cluster - occupying 384 NVIDIA A100 40GB GPUs across AWS P4d instances. Training took approximately 16.33 days to complete across both stages.""312 teraflops * 384 * 16.33 * 24 * 3600 * 0.3 = 5.07e22"
StableLM-Base-Alpha-7B,numParams,6890209280.0,
StableLM-Base-Alpha-7B,numTokens,750000000000.0,1 trillion tokens
StableLM-Base-Alpha-7B,trainingTimeDays,392.0,16.33 days
StableLM-Base-Alpha-7B,gpuType,NVIDIA A100 SXM4 40 GB,
StableLM-Base-Alpha-7B,releaseDate,2023-08-05,
QMoE: compressed 1T model,numParams,1600000000000.0,
QMoE: compressed 1T model,releaseDate,2023-10-25,
WizardLM-7B,flops,4.02e+22,"""We use pre-trained LLaMA 7B [4] to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 ×10−5, a maximum number of tokens 2048, and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs""Llama-7b was ~4e22. 8*70 V100-hours is ~2e20, so fine-tuning was <1% of base training."
WizardLM-7B,numParams,6700000000.0,
WizardLM-7B,trainingTimeDays,70.0,
WizardLM-7B,gpuCount,8.0,
WizardLM-7B,gpuType,NVIDIA V100,
WizardLM-7B,releaseDate,2023-04-24,
RetNet,flops,4.02e+21,C = 6ND = 6 * 6.7 billion * 100 billion
RetNet,numParams,6700000000.0,
RetNet,numTokens,75000000000.0,
RetNet,batchSize,4000000.0,
RetNet,releaseDate,2023-07-17,
RetNet,batchSize,4000000.0,4M
Gopher (280B),flops,6.31e+23,Table A266.31E+08 Train PFLOPs
Gopher (280B),numParams,280000000000.0,
Gopher (280B),numTokens,225000000000.0,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""1 token ~ 0.75 words"
Gopher (280B),batchSize,6000000.0,
Gopher (280B),costDollars,891638.804314709,
Gopher (280B),trainingTimeDays,920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e"""
Gopher (280B),gpuCount,4096.0,
Gopher (280B),gpuType,Google TPU v3,
Gopher (280B),gpuUtilization,0.378,
Gopher (280B),releaseDate,2021-12-08,
Gopher (280B),batchSize,6000000.0,"Table 1. ""Furthermore, we increase Gopher’s batch size from three to six million tokens per batch during training"""
OPT-30B,numParams,30000000000.0,
OPT-30B,releaseDate,2022-06-21,
BLUUMI,numParams,176000000000.0,
BLUUMI,numTokens,38000000000.0,38B tokens
BLUUMI,gpuType,AMD Instinct MI250X,
BLUUMI,releaseDate,2023-11-03,
IDEFICS,flops,1.1593580544e+23,"flops = 512 * 312e12 * 28*24*3600 * 0.3(num gpus) * (peak perforemence) * (time in seconds) * (assumed utilization rate)""The IDEFICS models were trained on an AWS SageMaker cluster with 8x80GB A100 GPUs nodes and EFA network.    IDEFICS-80B took ~28 days of training on 64 nodes (512 GPUs)."""
IDEFICS,numParams,80000000000.0,
IDEFICS,releaseDate,2023-08-22,
OPT-2.7B (finetuned on PTB),numParams,2700000000.0,
OPT-2.7B (finetuned on PTB),releaseDate,2022-06-21,
DeepNet,numParams,3200000000.0,
DeepNet,numTokens,12000000000.0,""" The final data consists of 102 languages, 1932 directions, and12B sentence pairs."""
DeepNet,releaseDate,2022-03-01,
Llama 2-70B,flops,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP."
Llama 2-70B,numParams,70000000000.0,
Llama 2-70B,numTokens,1500000000000.0,2 trillion tokens ~= 1.5 trillion words
Llama 2-70B,batchSize,4000000.0,
Llama 2-70B,costDollars,1620000.0,A100 cost in 2023: $1.10/hourTraining time: 1720320 A100 GPU-hoursInflation adjustment: $1.000 2020 = $1.145 2023
Llama 2-70B,trainingTimeDays,2160.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods."
Llama 2-70B,gpuCount,1000.0,
Llama 2-70B,gpuType,NVIDIA A100 SXM4 80 GB,
Llama 2-70B,gpuUtilization,0.435,
Llama 2-70B,releaseDate,2023-07-18,
Llama 2-70B,batchSize,4000000.0,
Context-dependent RNN,releaseDate,2012-07-27,
InternLM,numParams,100000000000.0,
InternLM,numTokens,750000000000.0,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens"" equals approximately 750B words for English, but the tokenizer's conversion ratio may be different for Chinese."
InternLM,gpuType,NVIDIA A100 SXM4 80 GB,
InternLM,releaseDate,2023-07-06,
OPT-1.3B,numParams,1300000000.0,
OPT-1.3B,releaseDate,2022-06-21,
PLATO-XL,numParams,11000000000.0,
PLATO-XL,gpuType,NVIDIA Tesla V100 DGXS 32 GB,
PLATO-XL,releaseDate,2021-09-20,
AlexaTM 20B,flops,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper."
AlexaTM 20B,numParams,19750000000.0,
AlexaTM 20B,batchSize,2000000.0,
AlexaTM 20B,trainingTimeDays,2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs..."""
AlexaTM 20B,gpuCount,128.0,
AlexaTM 20B,gpuType,NVIDIA A100,
AlexaTM 20B,gpuUtilization,0.4935,
AlexaTM 20B,releaseDate,2022-08-02,
AlexaTM 20B,batchSize,2000000.0,"""We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens"""
OPT-1.3B (finetuned on PTB),numParams,1300000000.0,
OPT-1.3B (finetuned on PTB),releaseDate,2022-06-21,
T0-XXL,flops,1.792e+22,"From section B.1: ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device."" (512 cores for 270 hours)"
T0-XXL,numParams,11000000000.0,
T0-XXL,trainingTimeDays,270.0,"""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device."""
T0-XXL,gpuCount,256.0,
T0-XXL,gpuType,Google TPU v3,
T0-XXL,releaseDate,2021-10-15,
Megatron-Turing NLG 530B,flops,1.17e+24,https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx
Megatron-Turing NLG 530B,numParams,530000000000.0,
Megatron-Turing NLG 530B,numTokens,202500000000.0,"""Our training dataset consists of 339 billion tokens and wetrained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""1 token ~ 0.75 words"
Megatron-Turing NLG 530B,batchSize,3932160.0,
Megatron-Turing NLG 530B,costDollars,3046994.0871934,
Megatron-Turing NLG 530B,trainingTimeDays,770.0,"Total compute was 1.17*10^24 FLOP.They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours."
Megatron-Turing NLG 530B,gpuCount,4480.0,
Megatron-Turing NLG 530B,gpuType,NVIDIA A100 SXM4 80 GB,
Megatron-Turing NLG 530B,gpuUtilization,0.302,
Megatron-Turing NLG 530B,releaseDate,2021-10-11,
Megatron-Turing NLG 530B,batchSize,3932160.0,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" Final batch size is 1920 * 2048 = 3932160"
Llama 2-7B,flops,8.4e+22,"Trained on 2 trillion tokens per Table 1. C = 6ND = 6*7B*2T = 8.4e+22 FLOP.Also, 7B model was trained on 184320 GPU-hours312 trillion * 184320 * 3600 * 0.3 = 6.21e22"
Llama 2-7B,numParams,70000000000.0,
Llama 2-7B,numTokens,1500000000000.0,2 trillion tokens ~= 1.5 trillion words
Llama 2-7B,batchSize,4000000.0,
Llama 2-7B,gpuType,NVIDIA A100 SXM4 80 GB,
Llama 2-7B,releaseDate,2023-07-18,
Llama 2-7B,batchSize,4000000.0,
OPT-1.3B (finetuned),numParams,1300000000.0,
OPT-1.3B (finetuned),releaseDate,2022-06-21,
Chinchilla,flops,5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""We see the number of flops in table 3"
Chinchilla,numParams,70000000000.0,
Chinchilla,numTokens,1050000000000.0,Table 1 shows Chinchilla was training on 1.4 trillion tokens1 token ~ 0.75 words
Chinchilla,batchSize,3000000.0,
Chinchilla,costDollars,753491.57852839,
Chinchilla,gpuType,"Google TPU v4,Google TPU v3",
Chinchilla,releaseDate,2022-03-29,
Chinchilla,batchSize,3000000.0,"Table 1. ""1.5M → 3M"""
ALM 1.0,releaseDate,2022-11-28,
BLOOM-176B,flops,3.6e+23,https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32384 A100 GPUs * 116 days = 3.6e23 at 30% utilization
BLOOM-176B,numParams,176247271424.0,
BLOOM-176B,numTokens,262500000000.0,350B words ~= 262B tokens
BLOOM-176B,batchSize,4194304.0,
BLOOM-176B,trainingTimeDays,2808.0,117 days * 24 hours/day
BLOOM-176B,gpuCount,384.0,
BLOOM-176B,gpuType,NVIDIA A100 SXM4 80 GB,
BLOOM-176B,releaseDate,2022-11-08,
BLOOM-176B,batchSize,4194304.0,Table 3. 2048*2048
GPT-4,flops,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing"
GPT-4,numTokens,4900000000000.0,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget."
GPT-4,trainingTimeDays,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.
GPT-4,gpuCount,25000.0,
GPT-4,gpuType,NVIDIA A100 SXM4 40 GB,
GPT-4,gpuUtilization,0.34,
GPT-4,releaseDate,2023-03-15,
Transformer ELMo,numParams,56000000.0,
Transformer ELMo,releaseDate,2019-01-01,
JIANG,flops,4.03e+22,"""The training was conducted using 96 A100 80G GPUs, and the entire process took approximately 52 days.""312 teraflop/s * 96 * 52 * 24 * 3600 * 0.3 = 4e22"
JIANG,numTokens,467000000000.0,"467B tokens (inferred from Table 1).It's a mix of Chinese and English text, I'll use our standard 1:1 token:words ratio for Chinese."
JIANG,batchSize,6000000.0,
JIANG,trainingTimeDays,1200.0,52 days
JIANG,gpuType,NVIDIA A100 SXM4 80 GB,
JIANG,releaseDate,2023-08-01,
JIANG,batchSize,6000000.0,"""During the training process, we employed a large batch size of 6 million tokens to enhance the model’s stability"""
mT5-XXL,flops,7.8e+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""1 trillion tokens * 13 billion params * 6 = 7.8e22"
mT5-XXL,numParams,13000000000.0,
mT5-XXL,numTokens,750000000000.0,"""totaling 6.6B pages and 6.3T tokens""It's multilingual so we don't have a standard word:token ratio, but using the 0.75 for English that's ~5 trillion.Distribution by language is in Appendix A.The model was trained for 0.159 equivalent epochs of the full dataset, or 1 epoch on a subset of 1 trillion tokens."
mT5-XXL,batchSize,1048576.0,
mT5-XXL,releaseDate,2020-10-20,
mT5-XXL,batchSize,1048576.0,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total."""
RWKV-4 14B,flops,2.78e+22,"from HuggingFace page: https://huggingface.co/BlinkDL/rwkv-4-pile-14btrained for 331B tokens14 billion * 331 billion * 6 = 2.78e22paper notes that a forward pass is almost exactly 2x parameters (within 2%): ""Alternative approximations for FLOPs include doubling the parameters which yields similar results within 2% for 14B and a 30% discrepancy for 169M variant."" and that 6*params*tokens is a good approximation because it's not a transformer: ""FLOPs is for a forward pass for one token. It was calculated as 6(V D + 13D2L), which is thetwice (add and multiply) the number of parametersin linear layers. The backwards pass FLOPs can beapproximated as twice that of the forward pass. Sothe total is 6(V D + 13D2L) per token for training(3x fw FLOPs). It is noteworthy that FLOPs areindependent of the context length, unlike regulartransformers"""
RWKV-4 14B,numParams,14000000000.0,
RWKV-4 14B,batchSize,262144.0,
RWKV-4 14B,gpuType,NVIDIA A100 SXM4 80 GB,
RWKV-4 14B,releaseDate,2023-05-22,
RWKV-4 14B,batchSize,262144.0,"262144 (or 131072?)""To train the models mentioned, we... switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens"""
Jais,flops,3.08e+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP
Jais,numParams,13000000000.0,
Jais,numTokens,300000000000.0,395B tokens ~= 300B words
Jais,batchSize,3932160.0,
Jais,trainingTimeDays,600.0,2023 June 25 to July 18 = 25 days = 600 hours
Jais,releaseDate,2023-08-29,
Jais,batchSize,3932160.0,"""After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. """
ERNIE 3.5,releaseDate,2023-06-27,
Inflection-2,flops,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""(the second 1 is there because of airtable being wonky, it's not a real sig fig)"
Inflection-2,gpuCount,5000.0,
Inflection-2,gpuType,NVIDIA H100 SXM5,
Inflection-2,releaseDate,2023-11-22,
PaLM (540B),flops,2.5272e+24,See Table 20: https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.Equivalent to 6144 TPUv4 for 1368 hours.46.2% model FLOPs utilization
PaLM (540B),numParams,540350000000.0,
PaLM (540B),numTokens,585000000000.0,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""1 token ~ 0.75 words"
PaLM (540B),batchSize,4000000.0,
PaLM (540B),costDollars,3232806.53266529,"Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization."
PaLM (540B),trainingTimeDays,1368.0,6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.Equivalent to 6144 TPUv4 for 1368 hours.
PaLM (540B),gpuCount,6144.0,
PaLM (540B),gpuType,Google TPU v4,
PaLM (540B),gpuUtilization,0.462,
PaLM (540B),releaseDate,2022-04-04,
PaLM (540B),batchSize,4000000.0,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k"""
GPT-4V,releaseDate,2023-09-25,
Palmyra Large 20B,flops,9.6e+22,"""Palmyra-Large is a 20B parameters causal decoder-only model built by Writer and trained on +800B tokens of Palmyra-Index-Data enhanced with curated corpora.""I'm not sure if the 800B is how many tokens the model was trained on, or the size of the dataset. But the dataset linked on HuggingFace has 1T tokens, so 800B as tokens trained is more likely.20B*800B*6 = 9.6e22"
Palmyra Large 20B,numParams,20000000000.0,
Palmyra Large 20B,numTokens,750000000000.0,"1 trillion tokens, or 750B words: https://huggingface.co/datasets/Writer/palmyra-data-index"
Palmyra Large 20B,releaseDate,2023-03-01,
Inflection-1,flops,1.0001e+24,"<= 2.5e24They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)"
Inflection-1,gpuType,NVIDIA H100 SXM5,
Inflection-1,releaseDate,2023-06-23,
Persimmon-8B,numParams,9300000000.0,
Persimmon-8B,numTokens,552750000000.0,737B tokens = 552750M words
Persimmon-8B,releaseDate,2023-09-07,
ConSERT,flops,280000000000000000000,Fine-tuning was done using a single Nvidia V100 GPU for a few minutes -> 1.0E+15 to 5.0E+15 (2 to 10 min)Foundation model is BeRT with 2.8e+20 FLOP.So total compute is 2.8e+20.
ConSERT,numParams,345000000.0,
ConSERT,trainingTimeDays,0.1,
ConSERT,gpuType,NVIDIA Tesla V100S PCIe 32 GB,
ConSERT,releaseDate,2021-05-25,
ByT5-XXL,flops,8.1e+22,"""Like mT5, we set our sequencelength to 1024 (bytes rather than tokens), and trainfor 1 million steps over batches of 2^20 tokens.""12.9 billion * 1 million * 2^20 * 6 = ~8.1e22"
ByT5-XXL,numParams,12900000000.0,
ByT5-XXL,batchSize,1048576.0,
ByT5-XXL,gpuCount,64.0,
ByT5-XXL,gpuType,Google TPU v3,
ByT5-XXL,releaseDate,2021-05-28,
ByT5-XXL,batchSize,1048576.0,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens"""
Claude 2,flops,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY
Claude 2,releaseDate,2023-07-11,
EXAONE 2.0,numParams,300000000000.0,
EXAONE 2.0,releaseDate,2023-07-19,
OPT-2.7B (finetuned on WT2),numParams,2700000000.0,
OPT-2.7B (finetuned on WT2),releaseDate,2022-06-21,
Yuan 1.0,flops,3.5380000000001e+23,Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOPhttps://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
Yuan 1.0,numParams,245730000000.0,
Yuan 1.0,numTokens,1000000000000.0,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words."
Yuan 1.0,batchSize,6881280.0,
Yuan 1.0,costDollars,606364.74789733,
Yuan 1.0,gpuCount,2128.0,
Yuan 1.0,gpuUtilization,0.45,
Yuan 1.0,releaseDate,2021-10-12,
Yuan 1.0,batchSize,6881280.0,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280"
Skywork-13B,flops,2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". ""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode."
Skywork-13B,numParams,13000000000.0,
Skywork-13B,numTokens,2780000000000.0,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion."
Skywork-13B,batchSize,16000000.0,
Skywork-13B,trainingTimeDays,940.0,39 days
Skywork-13B,gpuCount,512.0,
Skywork-13B,gpuType,NVIDIA A800,
Skywork-13B,gpuUtilization,0.46,
Skywork-13B,releaseDate,2023-10-30,
Skywork-13B,batchSize,16000000.0,Table 3
LaMDA,flops,3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days= 3.55E+23""From https://arxiv.org/pdf/2201.08239.pdf p.18"
LaMDA,numParams,137000000000.0,
LaMDA,numTokens,1560000000000.0,"""and are pre-trained on 1.56T words of public dialog data and web text"""
LaMDA,batchSize,256000.0,
LaMDA,costDollars,484957.204278073,
LaMDA,trainingTimeDays,1385.0,57.7 days * 24
LaMDA,gpuCount,1024.0,
LaMDA,gpuType,Google TPU v3,
LaMDA,gpuUtilization,0.565,
LaMDA,releaseDate,2022-02-10,
LaMDA,batchSize,256000.0,"""All models were trained with 256K tokens per batch"""
Galactica,flops,3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23"
Galactica,numParams,120000000000.0,
Galactica,batchSize,2000000.0,
Galactica,gpuCount,128.0,
Galactica,gpuType,NVIDIA A100 SXM4 80 GB,
Galactica,releaseDate,2022-11-16,
Galactica,batchSize,2000000.0,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)"
OPT-175B,flops,4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute"""
OPT-175B,numParams,175000000000.0,
OPT-175B,numTokens,135000000000.0,"""The training data contains 180B tokens corresponding to 800 GB of data""1 token ~ 0.75 words"
OPT-175B,batchSize,2000000.0,
OPT-175B,costDollars,1654082.50447642,
OPT-175B,trainingTimeDays,793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hourshttps://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.)."""
OPT-175B,gpuCount,1024.0,
OPT-175B,gpuType,NVIDIA A100 SXM4 80 GB,
OPT-175B,gpuUtilization,0.47115,
OPT-175B,releaseDate,2022-05-02,
OPT-175B,batchSize,2000000.0,Table 1
LLaMA-13B,flops,4.55e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22from paper, Llama-7B took 135,168 GPU hours using A100s312 trillion * 135,168 * 3600 * 0.3 = 4.55e22 FLOP"
LLaMA-13B,numParams,13000000000.0,
LLaMA-13B,batchSize,4000000.0,
LLaMA-13B,gpuType,NVIDIA A100,
LLaMA-13B,releaseDate,2023-02-27,
LLaMA-13B,batchSize,4000000.0,
OPT-125M (finetuned),numParams,125000000.0,
OPT-125M (finetuned),releaseDate,2022-06-21,
LLaMA-7B,flops,4.02e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOPfrom paper, Llama-7B took 82,432 GPU hours using A100s312 trillion * 82,432 * 3600 * 0.3 = 2.78e22 FLOP"
LLaMA-7B,numParams,6700000000.0,
LLaMA-7B,numTokens,750000000000.0,1 trillion tokens * 0.75 words/token = 750 billion words
LLaMA-7B,batchSize,4000000.0,
LLaMA-7B,gpuType,NVIDIA A100,
LLaMA-7B,gpuUtilization,0.43,
LLaMA-7B,releaseDate,2023-02-24,
LLaMA-7B,batchSize,4000000.0,
Llama 2-13B,flops,1.6e+23,13 billion * 2 trillion * 6 = 1.6e23
Llama 2-13B,numParams,13000000000.0,
Llama 2-13B,numTokens,1500000000000.0,2 trillion tokens ~= 1.5 trillion words
Llama 2-13B,batchSize,4000000.0,
Llama 2-13B,gpuType,NVIDIA A100 SXM4 80 GB,
Llama 2-13B,releaseDate,2023-07-18,
Llama 2-13B,batchSize,4000000.0,
OPT-IML (175B),numParams,175000000000.0,
OPT-IML (175B),gpuType,NVIDIA A100,
OPT-IML (175B),releaseDate,2022-12-22,
OPT-13B,numParams,13000000000.0,
OPT-13B,releaseDate,2022-06-21,
PAGnol-XL,flops,259200000000000000000,"They report their compute directly.From section 8: ""About 62k GPU-hours on the Jean Zay HPC Cluster."" Jean Zay uses both A100 and V100 GPUs, and maybe other stuff as well?Note they explicitly call out V100 in their Appendix A.https://www.hpcwire.com/2021/11/17/frances-jean-zay-supercomputer-gets-ai-boost-from-hpe-nvidia/"
PAGnol-XL,numParams,1500000000.0,
PAGnol-XL,numTokens,24000000000.0,Section 4.1: 32G tokens => 32e9*0.75 = 24e9 words
PAGnol-XL,gpuType,NVIDIA Tesla V100 SXM2,
PAGnol-XL,releaseDate,2021-10-16,
Falcon-40B,flops,2.4e+23,C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)
Falcon-40B,numParams,40000000000.0,
Falcon-40B,numTokens,750000000000.0,1000B tokens ~= 750B words
Falcon-40B,batchSize,2359296.0,
Falcon-40B,trainingTimeDays,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""""Training started in December 2022 and took two months."""
Falcon-40B,gpuCount,384.0,
Falcon-40B,gpuType,NVIDIA A100,
Falcon-40B,gpuUtilization,0.3864,
Falcon-40B,releaseDate,2023-03-15,
Falcon-40B,batchSize,2359296.0,"Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.""All Falcon models are pretrained with a 2,048 sequence length""https://arxiv.org/pdf/2311.16867.pdf"
GPT-3.5 (text-davinci-003),flops,2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI
GPT-3.5 (text-davinci-003),gpuType,NVIDIA A100 SXM4 40 GB,
GPT-3.5 (text-davinci-003),releaseDate,2022-11-28,
OPT-6.7B,numParams,6700000000.0,
OPT-6.7B,releaseDate,2022-06-21,
YaLM,flops,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources."""
YaLM,numParams,100000000000.0,
YaLM,numTokens,300000000000.0,1.7TB of data 300B tokens – from github https://github.com/yandex/YaLM-100BI've assumed that 1 token correspond to 1 word in russian language.
YaLM,trainingTimeDays,1560.0,65 days
YaLM,gpuCount,800.0,
YaLM,gpuType,NVIDIA A100,
YaLM,releaseDate,2022-06-23,
OPT-66B,flops,1.100000000001e+23,"OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 ∗ 2e6 ∗ 66e9 ∗ 6 = 1.1e23 FLOP"
OPT-66B,numParams,66000000000.0,
OPT-66B,releaseDate,2022-06-21,
OPT-2.7B,numParams,2700000000.0,
OPT-2.7B,releaseDate,2022-06-21,
OPT-125M (finetuned on PTB),numParams,125000000.0,
OPT-125M (finetuned on PTB),releaseDate,2022-06-21,
PaLM 2,flops,7.34e+24,"Compute Requirements ""Not reported.""Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP."
PaLM 2,numParams,340000000000.0,
PaLM 2,numTokens,2700000000000.0,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html"
PaLM 2,gpuType,Google TPU v4,
PaLM 2,releaseDate,2023-05-10,
Cerebras-GPT-13B,flops,2.3e+22,"2.3e22, per table 2"
Cerebras-GPT-13B,numParams,13000000000.0,
Cerebras-GPT-13B,numTokens,278000000000.0,"371B tokens, or 278B words"
Cerebras-GPT-13B,batchSize,2210000.0,
Cerebras-GPT-13B,gpuType,Cerebras CS-2,
Cerebras-GPT-13B,releaseDate,2023-04-06,
Cerebras-GPT-13B,batchSize,2210000.0,"""For the 13B parameter model, we train with a batch size of 720 sequences of length 2048 tokens for the first 84B tokens. At that point, we observed the gap between validation and train loss growing, indicating that the gradient noise was growing, so we increased the batch size to 1080 sequences for the rest of training.""batch sizes ramp from 1.47M to 2.21M"
FLAN 137B,flops,4.896e+22,"From section 2.4: ""60 hours on a TPUv3 with 128 cores."" I assume that ""128 cores"" = 128 TPUv3s. Which took less than 2% of total time (see environmental considerations section)"
FLAN 137B,numParams,137000000000.0,
FLAN 137B,numTokens,1870000000000.0,"""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""2.49e12 tokens ~= 1.87e12 words"
FLAN 137B,trainingTimeDays,60.0,
FLAN 137B,gpuCount,64.0,
FLAN 137B,gpuType,Google TPU v3,
FLAN 137B,releaseDate,2021-09-03,
